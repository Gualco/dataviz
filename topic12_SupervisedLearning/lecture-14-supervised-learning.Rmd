---
title       : Data Analysis and Visualization
subtitle    : Classification
author      : Julien Gagneur, Jan Krumsiek
# job         : Professor Computational Biology
biglogo     : title.jpg
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax, bootstrap, quiz]            # {mathjax, quiz, bootstrap}
ext_widgets : {rCharts: ["libraries/highcharts", libraries/nvd3"]}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---
<!-- Center image on slide --> 
<script 
src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script> <script 
type='text/javascript'> $(function() { $("p:has(img)").addClass('centered'); });
</script>

<!-- setwd('./lectures/') -->

```{r global_options, include=FALSE, cache=F}
source("../config.R")
opts_chunk$set(
    echo=TRUE, warning=FALSE, message=FALSE, cache=F, 
    results="show",
    out.width="500px", out.height="400px", fig.height = 3, fig.width = 4, 
    dpi=200
)
# options(width=100)
imgprefix <- 'assets/img/lec13_plotting/'
figprefix <- 'assets/fig/lec13_'

mysize <- 15
mytheme <- theme(
    axis.title = element_text(size=mysize), 
    axis.text = element_text(size=mysize),
    legend.title = element_text(size=mysize),
    legend.text = element_text(size=mysize)
    )

options(knitr.package.unnamed.chunk.label="lecture-14")

library(data.table)
```

<!-- START LECTURE -->

```{r, cache=F, echo=F, message=FALSE, warning=FALSE} 
source("../config.R") 
```

--- 
## Motivation

* The last two weeks we worked with Linear and Logistic Regression models.
* We looked at them from a statistical point of few.
* Today we will take a machine learning angle. 


--- 
## Overview

* Alternative Models

* Random Forests for classification and prediction

* Benchmarking of models in ML

* Overfitting and Feature selection

---
## Notation

* We use the machine learning nomenclature:

  * $y$ denotes the _outcome_ (or response) we want to predict 
  * $x_1, \dots, x_p$ denote the _features_ that we will use to predict the outcome.

* Goal: Build an algorithm that takes feature values as input and returns a prediction for the outcome when we don't know the outcome.

* The machine learning approach is to _train_ an algorithm using a data set for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don't know the outcome.

---
## Regression models in ML

We have a series of features and an unknown outcome we want to predict:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
n <- 1
tmp <- data.frame(outcome=rep("?",n), 
                  feature_1 = paste0("$x_1$"),
                  feature_2 = paste0("$x_2$"),
                  feature_3 = paste0("$x_3$"),
                  feature_4 = paste0("$x_4$"),
                  feature_5 = paste0("$x_5$"))
tmp %>% knitr::kable(align="c")
```

To _build a model_ that provides a prediction for any set of observed values $x_1, x_2, \dots x_5$, we collect data for which we know the outcome:

```{r, echo=FALSE}
n <- 10
tmp <- data.frame(outcome = paste0("$y_{", 1:n,"}$"), 
                  feature_1 = paste0("$x_{",1:n,",1}$"),
                  feature_2 = paste0("$x_{",1:n,",2}$"),
                  feature_3 = paste0("$x_{",1:n,",3}$"),
                  feature_4 = paste0("$x_{",1:n,",4}$"),
                  feature_5 = paste0("$x_{",1:n,",5}$"))
tmp %>% knitr::kable()
```

---
## Classification

* A _classification_ is a prediction task with a categorical outcome.

* A _binary classification_ is a prediction task with a binary outcome.

* Here we will focus on binary classification.

* We denote $k=0,1$ the two classes.


---
## A univariate example: predicting sex given the height.
The `heights` dataset of the `dslabs` package :
```{r, echo=TRUE, out.width = "500px"}
heights <- as.data.table(heights)
heights[, y:=as.numeric(sex == "Female")]
heights
```

---
## Linear regression is not appropriate for classification
```{r, echo=TRUE, out.width = "500px"}
lm_fit0 <- lm(y~height, data=heights)
ggplot(heights, aes(height, y)) + geom_point() + geom_abline(intercept = lm_fit0$coef[1], slope = lm_fit0$coef[2])
```

* We need to step back and consider a different modeling approach for categorical responses.

---
## Linear regression predicts the expected values
* We denote the density of the normal distribution with mean $\mu$ and variance $\sigma^2$ as:
$$N (x|\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{2 \sigma^2})$$

> * We have introduced linear regression with the following model:
$$
\begin{array}
\\
y_i &= \beta_0 + \sum_{j=1}^p \beta_j x_{ij}  + \epsilon_i \\
p(\epsilon_i) &= N(\epsilon_i| 0, \sigma^2)
\end{array}
$$ 
> * This is equivalent to write:
$$
\begin{array}
\\
p(y_i | x_i) &= N(y_i | \mu_i, \sigma^2) \\
\mu_i &= \beta_0 + \sum_{j=1}^p \beta_j x_{ij} 
\end{array}
$$ 

> * Hence, linear regression models $\mu_i := E(y_i|x_{i1},...x_{ip})$, the _conditional_ _expectation_ of the outcome conditioned on the features, as a linear combination of the features. 

---
## Logistic regression predicts the expected values
* Logistic regression models the conditional expectation of the outcome conditioned on the features.

> * For a binary outcome $y \in \{0,1\}$, the expectation $\mu$ is the probability of class 1.

> * Modeling a probability with a linear function is not ideal because you can make predictions <0 or >1. Here is a linear fit to the proportion of women vs. height in the `heights` dataset of the `dslabs` package :
```{r, echo=FALSE, out.width = "500px"}
props <- heights %>% 
  mutate(height = round(height)) %>%
  group_by(height) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female"))

lm_fit <- lm(prop ~ height, data=props)

ggplot(props, aes(height, prop)) +
  geom_point() +
  xlab("Height (rounded closest inch)") +
  ylab("Proportion of women") +
  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])
```


---
## The logistic function maps real numbers to the [0,1] interval

The logistic function $\sigma (t)$ is defined as follows:

$$\sigma (t) = \frac{1}{1+e^{-t}}$$


```{r, echo=FALSE, out.width = "500px"}
library(data.table)
df <- data.table(x=seq(-5,5, length.out=1000))
df[,y:=1/(1+exp(-x))]
ggplot(df, aes(x, y)) +
  geom_line() +
  xlab("x") +
  ylab( expression(sigma(x)) ) 
```


---
## Alternative models

* Random Forests

* Neural Networks

* etc.

we will present you Random Forests as a further type of modles, as those are easy to understand
and still achive state of the are performance in some prediction tasks.

---
## Random Forest for Classification




---
## Random Forest for Regression






--- 
## Model evaluation


In Machine Learning models are typically assessed on test sets.

To do so the availiable data is split into a train and a test set.

If not sufficient data is availiable to create completetly distinct 
train and test sets crossvalidation is often used as a strategy to 
assess modles.


--
## Cross validation

In crossvalidation the data is randomly split into a number k of folds (typically 5 or 10)

Then a model is repeatetly trained on k-1 folds and evaluated on the fold not used
for training.

After this has been repeated for all folds the evaluation metrics can combined into
a combined evaluation curve.


-- 
## Overfitting and Feature selection





















