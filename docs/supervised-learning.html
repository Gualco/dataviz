<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Supervised Learning | Data Analysis and Visualization in R (IN2339)</title>
  <meta name="description" content="TODO This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Supervised Learning | Data Analysis and Visualization in R (IN2339)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TODO This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Supervised Learning | Data Analysis and Visualization in R (IN2339)" />
  
  <meta name="twitter:description" content="TODO This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  



<meta name="date" content="2022-02-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chap-log-reg.html"/>
<link rel="next" href="importing-data.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis and Visualization in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i>Datasets</a></li>
<li class="chapter" data-level="" data-path="feedback.html"><a href="feedback.html"><i class="fa fa-check"></i>Feedback</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#data-science-what-and-why"><i class="fa fa-check"></i>Data Science: What and why?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-you-will-learn-and-not-learn"><i class="fa fa-check"></i>What you will learn and not learn</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#the-r-language"><i class="fa fa-check"></i>The R language</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#course-overview"><i class="fa fa-check"></i>Course overview</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#complementary-reading"><i class="fa fa-check"></i>Complementary reading</a></li>
</ul></li>
<li class="part"><span><b>I Get</b></span></li>
<li class="chapter" data-level="1" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>1</b> R basics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-basics.html"><a href="r-basics.html#rstudio"><i class="fa fa-check"></i><b>1.1</b> Rstudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-basics.html"><a href="r-basics.html#first-steps-with-r"><i class="fa fa-check"></i><b>1.2</b> First steps with R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="r-basics.html"><a href="r-basics.html#objects"><i class="fa fa-check"></i><b>1.2.1</b> Objects</a></li>
<li class="chapter" data-level="1.2.2" data-path="r-basics.html"><a href="r-basics.html#the-workspace"><i class="fa fa-check"></i><b>1.2.2</b> The workspace</a></li>
<li class="chapter" data-level="1.2.3" data-path="r-basics.html"><a href="r-basics.html#functions"><i class="fa fa-check"></i><b>1.2.3</b> Functions</a></li>
<li class="chapter" data-level="1.2.4" data-path="r-basics.html"><a href="r-basics.html#other-prebuilt-objects"><i class="fa fa-check"></i><b>1.2.4</b> Other prebuilt objects</a></li>
<li class="chapter" data-level="1.2.5" data-path="r-basics.html"><a href="r-basics.html#variable-names"><i class="fa fa-check"></i><b>1.2.5</b> Variable names</a></li>
<li class="chapter" data-level="1.2.6" data-path="r-basics.html"><a href="r-basics.html#reusing-scripts"><i class="fa fa-check"></i><b>1.2.6</b> Reusing scripts</a></li>
<li class="chapter" data-level="1.2.7" data-path="r-basics.html"><a href="r-basics.html#commenting-your-code"><i class="fa fa-check"></i><b>1.2.7</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="r-basics.html"><a href="r-basics.html#data-types"><i class="fa fa-check"></i><b>1.3</b> Data types</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>1.3.1</b> Data frames</a></li>
<li class="chapter" data-level="1.3.2" data-path="r-basics.html"><a href="r-basics.html#examining-an-object"><i class="fa fa-check"></i><b>1.3.2</b> Examining an object</a></li>
<li class="chapter" data-level="1.3.3" data-path="r-basics.html"><a href="r-basics.html#the-accessor"><i class="fa fa-check"></i><b>1.3.3</b> The accessor: <code>$</code></a></li>
<li class="chapter" data-level="1.3.4" data-path="r-basics.html"><a href="r-basics.html#vectors-numerics-characters-and-logical"><i class="fa fa-check"></i><b>1.3.4</b> Vectors: numerics, characters, and logical</a></li>
<li class="chapter" data-level="1.3.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>1.3.5</b> Factors</a></li>
<li class="chapter" data-level="1.3.6" data-path="r-basics.html"><a href="r-basics.html#lists"><i class="fa fa-check"></i><b>1.3.6</b> Lists</a></li>
<li class="chapter" data-level="1.3.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>1.3.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="r-basics.html"><a href="r-basics.html#creating-vectors"><i class="fa fa-check"></i><b>1.4.1</b> Creating vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="r-basics.html"><a href="r-basics.html#names"><i class="fa fa-check"></i><b>1.4.2</b> Names</a></li>
<li class="chapter" data-level="1.4.3" data-path="r-basics.html"><a href="r-basics.html#sequences"><i class="fa fa-check"></i><b>1.4.3</b> Sequences</a></li>
<li class="chapter" data-level="1.4.4" data-path="r-basics.html"><a href="r-basics.html#subsetting"><i class="fa fa-check"></i><b>1.4.4</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="r-basics.html"><a href="r-basics.html#coercion"><i class="fa fa-check"></i><b>1.5</b> Coercion</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="r-basics.html"><a href="r-basics.html#not-availables-na"><i class="fa fa-check"></i><b>1.5.1</b> Not availables (NA)</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>1.6</b> Sorting</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>1.6.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="1.6.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>1.6.2</b> <code>order</code></a></li>
<li class="chapter" data-level="1.6.3" data-path="r-basics.html"><a href="r-basics.html#max-and-which.max"><i class="fa fa-check"></i><b>1.6.3</b> <code>max</code> and <code>which.max</code></a></li>
<li class="chapter" data-level="1.6.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>1.6.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="1.6.5" data-path="r-basics.html"><a href="r-basics.html#beware-of-recycling"><i class="fa fa-check"></i><b>1.6.5</b> Beware of recycling</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="r-basics.html"><a href="r-basics.html#vector-arithmetics"><i class="fa fa-check"></i><b>1.7</b> Vector arithmetics</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-a-vector"><i class="fa fa-check"></i><b>1.7.1</b> Rescaling a vector</a></li>
<li class="chapter" data-level="1.7.2" data-path="r-basics.html"><a href="r-basics.html#two-vectors"><i class="fa fa-check"></i><b>1.7.2</b> Two vectors</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="r-basics.html"><a href="r-basics.html#indexing"><i class="fa fa-check"></i><b>1.8</b> Indexing</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-basics.html"><a href="r-basics.html#subsetting-with-logicals"><i class="fa fa-check"></i><b>1.8.1</b> Subsetting with logicals</a></li>
<li class="chapter" data-level="1.8.2" data-path="r-basics.html"><a href="r-basics.html#logical-operators"><i class="fa fa-check"></i><b>1.8.2</b> Logical operators</a></li>
<li class="chapter" data-level="1.8.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>1.8.3</b> <code>which</code></a></li>
<li class="chapter" data-level="1.8.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>1.8.4</b> <code>match</code></a></li>
<li class="chapter" data-level="1.8.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>1.8.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-basics.html"><a href="r-basics.html#r-programming"><i class="fa fa-check"></i><b>1.9</b> R programming</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>2</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-wrangling.html"><a href="data-wrangling.html#data.tables"><i class="fa fa-check"></i><b>2.1</b> Data.tables</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-wrangling.html"><a href="data-wrangling.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-wrangling.html"><a href="data-wrangling.html#creating-and-loading-tables"><i class="fa fa-check"></i><b>2.1.2</b> Creating and loading tables</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-wrangling.html"><a href="data-wrangling.html#inspecting-tables"><i class="fa fa-check"></i><b>2.1.3</b> Inspecting tables</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-wrangling.html"><a href="data-wrangling.html#row-subsetting"><i class="fa fa-check"></i><b>2.2</b> Row subsetting</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-wrangling.html"><a href="data-wrangling.html#subsetting-rows-by-indices"><i class="fa fa-check"></i><b>2.2.1</b> Subsetting rows by indices</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-wrangling.html"><a href="data-wrangling.html#subsetting-rows-by-logical-conditions"><i class="fa fa-check"></i><b>2.2.2</b> Subsetting rows by logical conditions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-wrangling.html"><a href="data-wrangling.html#column-operations"><i class="fa fa-check"></i><b>2.3</b> Column operations</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-wrangling.html"><a href="data-wrangling.html#working-with-columns"><i class="fa fa-check"></i><b>2.3.1</b> Working with columns</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-wrangling.html"><a href="data-wrangling.html#column-operations-1"><i class="fa fa-check"></i><b>2.3.2</b> Column operations</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-wrangling.html"><a href="data-wrangling.html#advanced-commands-apply-over-columns"><i class="fa fa-check"></i><b>2.3.3</b> Advanced commands: *apply() over columns</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-wrangling.html"><a href="data-wrangling.html#the-by-option"><i class="fa fa-check"></i><b>2.4</b> The ‘by’ option</a></li>
<li class="chapter" data-level="2.5" data-path="data-wrangling.html"><a href="data-wrangling.html#counting-occurences-with-.n"><i class="fa fa-check"></i><b>2.5</b> Counting occurences with <code>.N</code></a></li>
<li class="chapter" data-level="2.6" data-path="data-wrangling.html"><a href="data-wrangling.html#extending-tables"><i class="fa fa-check"></i><b>2.6</b> Extending tables</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="data-wrangling.html"><a href="data-wrangling.html#creating-new-columns-the-command"><i class="fa fa-check"></i><b>2.6.1</b> Creating new columns (the := command)</a></li>
<li class="chapter" data-level="2.6.2" data-path="data-wrangling.html"><a href="data-wrangling.html#advanced-multiple-assignments"><i class="fa fa-check"></i><b>2.6.2</b> Advanced: Multiple assignments</a></li>
<li class="chapter" data-level="2.6.3" data-path="data-wrangling.html"><a href="data-wrangling.html#copying-tables"><i class="fa fa-check"></i><b>2.6.3</b> Copying tables</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="data-wrangling.html"><a href="data-wrangling.html#summary"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="data-wrangling.html"><a href="data-wrangling.html#data.table-resources"><i class="fa fa-check"></i><b>2.8</b> Data.table resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html"><i class="fa fa-check"></i><b>3</b> Tidy data and combining tables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#motivation"><i class="fa fa-check"></i><b>3.1.1</b> Motivation</a></li>
<li class="chapter" data-level="3.1.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#datasets-used-in-this-chapter"><i class="fa fa-check"></i><b>3.1.2</b> Datasets used in this chapter</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#tidy-and-untidy-data"><i class="fa fa-check"></i><b>3.2</b> Tidy and untidy data</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#definition-of-tidy-data"><i class="fa fa-check"></i><b>3.2.1</b> Definition of tidy data</a></li>
<li class="chapter" data-level="3.2.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#advantages-of-tidy-data"><i class="fa fa-check"></i><b>3.2.2</b> Advantages of tidy data</a></li>
<li class="chapter" data-level="3.2.3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#common-signs-of-untidy-datasets"><i class="fa fa-check"></i><b>3.2.3</b> Common signs of untidy datasets</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#tidying-up-datasets"><i class="fa fa-check"></i><b>3.3</b> Tidying up datasets</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#melting-wide-to-long"><i class="fa fa-check"></i><b>3.3.1</b> Melting (wide to long)</a></li>
<li class="chapter" data-level="3.3.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#casting-long-to-wide"><i class="fa fa-check"></i><b>3.3.2</b> Casting (long to wide)</a></li>
<li class="chapter" data-level="3.3.3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#separating-columns"><i class="fa fa-check"></i><b>3.3.3</b> Separating columns</a></li>
<li class="chapter" data-level="3.3.4" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#uniting-columns"><i class="fa fa-check"></i><b>3.3.4</b> Uniting columns</a></li>
<li class="chapter" data-level="3.3.5" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#advanced-columns-containing-sets-of-values"><i class="fa fa-check"></i><b>3.3.5</b> Advanced: Columns containing sets of values</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#concatenating-tables"><i class="fa fa-check"></i><b>3.4</b> Concatenating tables</a></li>
<li class="chapter" data-level="3.5" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#merging-tables"><i class="fa fa-check"></i><b>3.5</b> Merging tables</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#inner-merge"><i class="fa fa-check"></i><b>3.5.1</b> Inner merge</a></li>
<li class="chapter" data-level="3.5.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#outer-full-merge"><i class="fa fa-check"></i><b>3.5.2</b> Outer (full) merge</a></li>
<li class="chapter" data-level="3.5.3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#left-merge"><i class="fa fa-check"></i><b>3.5.3</b> Left merge</a></li>
<li class="chapter" data-level="3.5.4" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#right-merge"><i class="fa fa-check"></i><b>3.5.4</b> Right merge</a></li>
<li class="chapter" data-level="3.5.5" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#merging-by-several-columns"><i class="fa fa-check"></i><b>3.5.5</b> Merging by several columns</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#tidy-not-unique"><i class="fa fa-check"></i><b>3.6</b> Tidy representations are not unique</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#alternative-tidy-forms-of-a-table"><i class="fa fa-check"></i><b>3.6.1</b> Alternative tidy forms of a table</a></li>
<li class="chapter" data-level="3.6.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#on-multiple-types-of-observational-units-in-the-same-table"><i class="fa fa-check"></i><b>3.6.2</b> On multiple types of observational units in the same table</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#summary-1"><i class="fa fa-check"></i><b>3.7</b> Summary</a></li>
<li class="chapter" data-level="3.8" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#tidy-data-resources"><i class="fa fa-check"></i><b>3.8</b> Tidy data resources</a></li>
</ul></li>
<li class="part"><span><b>II Look</b></span></li>
<li class="chapter" data-level="4" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html"><i class="fa fa-check"></i><b>4</b> Low dimensional visualizations</a>
<ul>
<li class="chapter" data-level="4.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#why-plotting"><i class="fa fa-check"></i><b>4.1</b> Why plotting?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plot-vs-stat"><i class="fa fa-check"></i><b>4.1.1</b> Plotting versus summary statistics</a></li>
<li class="chapter" data-level="4.1.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plot-debug"><i class="fa fa-check"></i><b>4.1.2</b> Plotting helps finding bugs in the data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#grammar-of-graphics"><i class="fa fa-check"></i><b>4.2</b> Grammar of graphics</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#components-of-the-layered-grammar"><i class="fa fa-check"></i><b>4.2.1</b> Components of the layered grammar</a></li>
<li class="chapter" data-level="4.2.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#defining-the-data-and-layers"><i class="fa fa-check"></i><b>4.2.2</b> Defining the data and layers</a></li>
<li class="chapter" data-level="4.2.3" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#mapping-of-aesthetics"><i class="fa fa-check"></i><b>4.2.3</b> Mapping of aesthetics</a></li>
<li class="chapter" data-level="4.2.4" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#facets-axes-and-labels"><i class="fa fa-check"></i><b>4.2.4</b> Facets, axes and labels</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#different-types-of-one--and-two-dimensional-plots"><i class="fa fa-check"></i><b>4.3</b> Different types of one- and two-dimensional plots</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plots-for-one-single-continuous-variable"><i class="fa fa-check"></i><b>4.3.1</b> Plots for one single continuous variable</a></li>
<li class="chapter" data-level="4.3.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plots-for-two-variables-one-continuous-one-discrete"><i class="fa fa-check"></i><b>4.3.2</b> Plots for two variables: one continuous, one discrete</a></li>
<li class="chapter" data-level="4.3.3" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plots-for-two-continuous-variables"><i class="fa fa-check"></i><b>4.3.3</b> Plots for two continuous variables</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#further-plots-for-low-dimensional-data"><i class="fa fa-check"></i><b>4.4</b> Further plots for low dimensional data</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plot-matrix"><i class="fa fa-check"></i><b>4.4.1</b> Plot matrix</a></li>
<li class="chapter" data-level="4.4.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#correlation-plot"><i class="fa fa-check"></i><b>4.4.2</b> Correlation plot</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#summary-2"><i class="fa fa-check"></i><b>4.5</b> Summary</a></li>
<li class="chapter" data-level="4.6" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#resources"><i class="fa fa-check"></i><b>4.6</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html"><i class="fa fa-check"></i><b>5</b> High dimensional visualizations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#notations"><i class="fa fa-check"></i><b>5.1</b> Notations</a></li>
<li class="chapter" data-level="5.2" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#data-matrix-preparation"><i class="fa fa-check"></i><b>5.2</b> Data matrix preparation</a></li>
<li class="chapter" data-level="5.3" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#heatmaps"><i class="fa fa-check"></i><b>5.3</b> Heatmaps</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#centering-and-scaling-variables"><i class="fa fa-check"></i><b>5.3.1</b> Centering and scaling variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#clustering"><i class="fa fa-check"></i><b>5.4</b> Clustering</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#k-means-clustering"><i class="fa fa-check"></i><b>5.4.1</b> K-Means clustering</a></li>
<li class="chapter" data-level="5.4.2" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#hclust"><i class="fa fa-check"></i><b>5.4.2</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="5.4.3" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#comparing-clusterings-with-the-rand-index"><i class="fa fa-check"></i><b>5.4.3</b> Comparing clusterings with the Rand index</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#dimensionality-reduction-with-pca"><i class="fa fa-check"></i><b>5.5</b> Dimensionality reduction with PCA</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#a-minimal-pca-from-2d-to-1d"><i class="fa fa-check"></i><b>5.5.1</b> A minimal PCA: From 2D to 1D</a></li>
<li class="chapter" data-level="5.5.2" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#pca-in-higher-dimensions"><i class="fa fa-check"></i><b>5.5.2</b> PCA in higher dimensions</a></li>
<li class="chapter" data-level="5.5.3" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#pca-in-r"><i class="fa fa-check"></i><b>5.5.3</b> PCA in R</a></li>
<li class="chapter" data-level="5.5.4" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#plotting-pca-results-in-r"><i class="fa fa-check"></i><b>5.5.4</b> Plotting PCA results in R</a></li>
<li class="chapter" data-level="5.5.5" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#pca-summary"><i class="fa fa-check"></i><b>5.5.5</b> PCA summary</a></li>
<li class="chapter" data-level="5.5.6" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#nonlinear-dimension-reduction"><i class="fa fa-check"></i><b>5.5.6</b> Nonlinear dimension reduction</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#discussion"><i class="fa fa-check"></i><b>5.6</b> Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#summary-3"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
<li class="chapter" data-level="5.8" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#resources-1"><i class="fa fa-check"></i><b>5.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html"><i class="fa fa-check"></i><b>6</b> Graphically supported hypotheses</a>
<ul>
<li class="chapter" data-level="6.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#descriptive-vs.-associative-plots"><i class="fa fa-check"></i><b>6.1</b> Descriptive vs. associative plots</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#descriptive-plots"><i class="fa fa-check"></i><b>6.1.1</b> Descriptive plots</a></li>
<li class="chapter" data-level="6.1.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#associative-plots"><i class="fa fa-check"></i><b>6.1.2</b> Associative plots</a></li>
<li class="chapter" data-level="6.1.3" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#correctly-using-descriptive-and-associative-plots"><i class="fa fa-check"></i><b>6.1.3</b> Correctly using descriptive and associative plots</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#correlation-and-causation"><i class="fa fa-check"></i><b>6.2</b> Correlation and causation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#the-association-is-not-statistically-supported"><i class="fa fa-check"></i><b>6.2.1</b> The association is not statistically supported</a></li>
<li class="chapter" data-level="6.2.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#reversing-cause-and-effect"><i class="fa fa-check"></i><b>6.2.2</b> Reversing cause and effect</a></li>
<li class="chapter" data-level="6.2.3" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#the-association-is-induced-by-a-third-variable"><i class="fa fa-check"></i><b>6.2.3</b> The association is induced by a third variable</a></li>
<li class="chapter" data-level="6.2.4" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#simpsons-paradox"><i class="fa fa-check"></i><b>6.2.4</b> Simpson’s paradox</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#data-presentation-as-story-telling"><i class="fa fa-check"></i><b>6.3</b> Data presentation as story telling</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#what-is-a-story"><i class="fa fa-check"></i><b>6.3.1</b> What is a story?</a></li>
<li class="chapter" data-level="6.3.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#presentation-structure"><i class="fa fa-check"></i><b>6.3.2</b> Presentation structure</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#guidelines-for-coloring-in-data-visualization"><i class="fa fa-check"></i><b>6.4</b> Guidelines for coloring in data visualization</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#color-coding-in-r"><i class="fa fa-check"></i><b>6.4.1</b> Color coding in R</a></li>
<li class="chapter" data-level="6.4.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#general-rules-for-color-coding"><i class="fa fa-check"></i><b>6.4.2</b> General rules for color coding</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#general-dos-and-donts-in-data-visualization"><i class="fa fa-check"></i><b>6.5</b> General do’s and don’ts in data visualization</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#dos"><i class="fa fa-check"></i><b>6.5.1</b> Do’s</a></li>
<li class="chapter" data-level="6.5.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#donts"><i class="fa fa-check"></i><b>6.5.2</b> Don’ts</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#summary-4"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
<li class="chapter" data-level="6.7" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#resources-2"><i class="fa fa-check"></i><b>6.7</b> Resources</a></li>
</ul></li>
<li class="part"><span><b>III Conclude</b></span></li>
<li class="chapter" data-level="7" data-path="resampling-stat.html"><a href="resampling-stat.html"><i class="fa fa-check"></i><b>7</b> Resampling-based Statistical Assessment</a>
<ul>
<li class="chapter" data-level="7.1" data-path="resampling-stat.html"><a href="resampling-stat.html#yeast-dataset"><i class="fa fa-check"></i><b>7.1</b> The yeast dataset</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="resampling-stat.html"><a href="resampling-stat.html#the-experiment"><i class="fa fa-check"></i><b>7.1.1</b> The experiment</a></li>
<li class="chapter" data-level="7.1.2" data-path="resampling-stat.html"><a href="resampling-stat.html#genotype"><i class="fa fa-check"></i><b>7.1.2</b> Genotype</a></li>
<li class="chapter" data-level="7.1.3" data-path="resampling-stat.html"><a href="resampling-stat.html#growth-rates"><i class="fa fa-check"></i><b>7.1.3</b> Growth rates</a></li>
<li class="chapter" data-level="7.1.4" data-path="resampling-stat.html"><a href="resampling-stat.html#genotype-growth-rate-association-in-maltose-at-a-specific-marker"><i class="fa fa-check"></i><b>7.1.4</b> Genotype-growth rate association in maltose at a specific marker</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="resampling-stat.html"><a href="resampling-stat.html#statistical-hypothesis-testing"><i class="fa fa-check"></i><b>7.2</b> Statistical hypothesis testing</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="resampling-stat.html"><a href="resampling-stat.html#permut-test-build-up"><i class="fa fa-check"></i><b>7.2.1</b> Permutation testing: An intuitive build-up</a></li>
<li class="chapter" data-level="7.2.2" data-path="resampling-stat.html"><a href="resampling-stat.html#concepts-of-statistical-hypothesis-testing"><i class="fa fa-check"></i><b>7.2.2</b> Concepts of Statistical Hypothesis Testing</a></li>
<li class="chapter" data-level="7.2.3" data-path="resampling-stat.html"><a href="resampling-stat.html#permutation-testing-formally"><i class="fa fa-check"></i><b>7.2.3</b> Permutation testing, formally</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="resampling-stat.html"><a href="resampling-stat.html#confidence-intervals-quantifying-uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>7.3</b> Confidence intervals: Quantifying uncertainty in parameter estimates</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="resampling-stat.html"><a href="resampling-stat.html#repeating-experiments-to-quantify-uncertainty"><i class="fa fa-check"></i><b>7.3.1</b> Repeating experiments to quantify uncertainty</a></li>
<li class="chapter" data-level="7.3.2" data-path="resampling-stat.html"><a href="resampling-stat.html#simulating-repeated-experiments"><i class="fa fa-check"></i><b>7.3.2</b> Simulating repeated experiments</a></li>
<li class="chapter" data-level="7.3.3" data-path="resampling-stat.html"><a href="resampling-stat.html#quantifying-uncertainty-using-the-case-resampling-bootstrap"><i class="fa fa-check"></i><b>7.3.3</b> Quantifying uncertainty using the case resampling bootstrap</a></li>
<li class="chapter" data-level="7.3.4" data-path="resampling-stat.html"><a href="resampling-stat.html#confidence-intervals-formal-definition"><i class="fa fa-check"></i><b>7.3.4</b> Confidence Intervals: Formal definition</a></li>
<li class="chapter" data-level="7.3.5" data-path="resampling-stat.html"><a href="resampling-stat.html#visualizing-the-formal-definition-of-confidence-intervals"><i class="fa fa-check"></i><b>7.3.5</b> Visualizing the formal definition of Confidence Intervals</a></li>
<li class="chapter" data-level="7.3.6" data-path="resampling-stat.html"><a href="resampling-stat.html#hypothesis-testing-with-the-confidence-interval"><i class="fa fa-check"></i><b>7.3.6</b> Hypothesis testing with the Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="resampling-stat.html"><a href="resampling-stat.html#discussion-1"><i class="fa fa-check"></i><b>7.4</b> Discussion</a></li>
<li class="chapter" data-level="7.5" data-path="resampling-stat.html"><a href="resampling-stat.html#conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="analytical-stat.html"><a href="analytical-stat.html"><i class="fa fa-check"></i><b>8</b> Analytical Statistical Assessment</a>
<ul>
<li class="chapter" data-level="8.1" data-path="analytical-stat.html"><a href="analytical-stat.html#motivation-hypothesis-testing-in-large-datasets"><i class="fa fa-check"></i><b>8.1</b> Motivation: Hypothesis testing in large datasets</a></li>
<li class="chapter" data-level="8.2" data-path="analytical-stat.html"><a href="analytical-stat.html#the-binomial-test-testing-hypotheses-for-a-single-binary-variable"><i class="fa fa-check"></i><b>8.2</b> The Binomial Test: testing hypotheses for a single binary variable</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="analytical-stat.html"><a href="analytical-stat.html#abstraction-tossing-a-coin"><i class="fa fa-check"></i><b>8.2.1</b> Abstraction: Tossing a coin</a></li>
<li class="chapter" data-level="8.2.2" data-path="analytical-stat.html"><a href="analytical-stat.html#computing-a-binomial-test-with-r"><i class="fa fa-check"></i><b>8.2.2</b> Computing a binomial test with R</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="analytical-stat.html"><a href="analytical-stat.html#fisher-test"><i class="fa fa-check"></i><b>8.3</b> Fisher’s exact test: Testing the association between two binary variables</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="analytical-stat.html"><a href="analytical-stat.html#permutation-testing-and-the-hypergeometric-distribution"><i class="fa fa-check"></i><b>8.3.1</b> Permutation testing and the hypergeometric distribution</a></li>
<li class="chapter" data-level="8.3.2" data-path="analytical-stat.html"><a href="analytical-stat.html#fishers-exact-test"><i class="fa fa-check"></i><b>8.3.2</b> Fisher’s exact test</a></li>
<li class="chapter" data-level="8.3.3" data-path="analytical-stat.html"><a href="analytical-stat.html#fishers-exact-test-in-r"><i class="fa fa-check"></i><b>8.3.3</b> Fisher’s exact test in R</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="analytical-stat.html"><a href="analytical-stat.html#testing-the-association-between-one-quantitative-and-one-binary-variable"><i class="fa fa-check"></i><b>8.4</b> Testing the association between one quantitative and one binary variable</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="analytical-stat.html"><a href="analytical-stat.html#the-t-test"><i class="fa fa-check"></i><b>8.4.1</b> The t-test</a></li>
<li class="chapter" data-level="8.4.2" data-path="analytical-stat.html"><a href="analytical-stat.html#wilcoxon-rank-sum-test-an-alternative-to-the-t-test-for-non-gaussian-data"><i class="fa fa-check"></i><b>8.4.2</b> Wilcoxon rank-sum test: An alternative to the t-test for non-Gaussian data</a></li>
<li class="chapter" data-level="8.4.3" data-path="analytical-stat.html"><a href="analytical-stat.html#why-bother-wilcox"><i class="fa fa-check"></i><b>8.4.3</b> Why bother with the Wilcoxon rank-sum test?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="analytical-stat.html"><a href="analytical-stat.html#association-between-two-quantitative-variables"><i class="fa fa-check"></i><b>8.5</b> Association between two quantitative variables</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="analytical-stat.html"><a href="analytical-stat.html#the-pearson-correlation-test"><i class="fa fa-check"></i><b>8.5.1</b> The Pearson correlation test</a></li>
<li class="chapter" data-level="8.5.2" data-path="analytical-stat.html"><a href="analytical-stat.html#the-spearman-rank-correlation-test"><i class="fa fa-check"></i><b>8.5.2</b> The Spearman rank correlation test</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="analytical-stat.html"><a href="analytical-stat.html#testing-associations-of-two-variables-overview"><i class="fa fa-check"></i><b>8.6</b> Testing associations of two variables: Overview</a></li>
<li class="chapter" data-level="8.7" data-path="analytical-stat.html"><a href="analytical-stat.html#assessing-distributional-assumptions-with-q-q-plots"><i class="fa fa-check"></i><b>8.7</b> Assessing distributional assumptions with Q-Q Plots</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="analytical-stat.html"><a href="analytical-stat.html#limitations-of-histograms"><i class="fa fa-check"></i><b>8.7.1</b> Limitations of Histograms</a></li>
<li class="chapter" data-level="8.7.2" data-path="analytical-stat.html"><a href="analytical-stat.html#q-q-plots-comparing-empirical-to-theoretical-quantiles"><i class="fa fa-check"></i><b>8.7.2</b> Q-Q plots: Comparing empirical to theoretical quantiles</a></li>
<li class="chapter" data-level="8.7.3" data-path="analytical-stat.html"><a href="analytical-stat.html#typical-q-q-plots"><i class="fa fa-check"></i><b>8.7.3</b> Typical Q-Q plots</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="analytical-stat.html"><a href="analytical-stat.html#analytical-conf-int"><i class="fa fa-check"></i><b>8.8</b> Analytical Confidence intervals</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="analytical-stat.html"><a href="analytical-stat.html#binomial-case"><i class="fa fa-check"></i><b>8.8.1</b> Binomial case</a></li>
<li class="chapter" data-level="8.8.2" data-path="analytical-stat.html"><a href="analytical-stat.html#confidence-intervals-in-r"><i class="fa fa-check"></i><b>8.8.2</b> Confidence intervals in R</a></li>
<li class="chapter" data-level="8.8.3" data-path="analytical-stat.html"><a href="analytical-stat.html#advanced-a-note-on-overlapping-confidence-intervals"><i class="fa fa-check"></i><b>8.8.3</b> Advanced: A note on overlapping confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="analytical-stat.html"><a href="analytical-stat.html#discussion-2"><i class="fa fa-check"></i><b>8.9</b> Discussion</a></li>
<li class="chapter" data-level="8.10" data-path="analytical-stat.html"><a href="analytical-stat.html#conclusion-1"><i class="fa fa-check"></i><b>8.10</b> Conclusion</a></li>
<li class="chapter" data-level="8.11" data-path="analytical-stat.html"><a href="analytical-stat.html#resources-3"><i class="fa fa-check"></i><b>8.11</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="big-data-stat.html"><a href="big-data-stat.html"><i class="fa fa-check"></i><b>9</b> Statistical Assessments for Big Data</a>
<ul>
<li class="chapter" data-level="9.1" data-path="big-data-stat.html"><a href="big-data-stat.html#motivation-statistical-significance-in-a-big-data-context"><i class="fa fa-check"></i><b>9.1</b> Motivation: Statistical Significance in a Big Data context</a></li>
<li class="chapter" data-level="9.2" data-path="big-data-stat.html"><a href="big-data-stat.html#effect-size-actually-important-or-just-significant"><i class="fa fa-check"></i><b>9.2</b> Effect Size: Actually important or just significant?</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="big-data-stat.html"><a href="big-data-stat.html#the-relationship-of-sample-size-and-significance"><i class="fa fa-check"></i><b>9.2.1</b> The relationship of sample size and significance</a></li>
<li class="chapter" data-level="9.2.2" data-path="big-data-stat.html"><a href="big-data-stat.html#report-p-value-effect-size-and-plot"><i class="fa fa-check"></i><b>9.2.2</b> Report <span class="math inline">\(P-\)</span>value, effect size, and plot</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="big-data-stat.html"><a href="big-data-stat.html#multiple-testing"><i class="fa fa-check"></i><b>9.3</b> Multiple Testing</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="big-data-stat.html"><a href="big-data-stat.html#multiple-testing-in-real-life-p-hacking-and-fishing-expeditions"><i class="fa fa-check"></i><b>9.3.1</b> Multiple testing in real life: <span class="math inline">\(P-\)</span>Hacking and fishing expeditions</a></li>
<li class="chapter" data-level="9.3.2" data-path="big-data-stat.html"><a href="big-data-stat.html#the-land-of-counterfeit-fake-coins"><i class="fa fa-check"></i><b>9.3.2</b> The Land of Counterfeit (fake) coins</a></li>
<li class="chapter" data-level="9.3.3" data-path="big-data-stat.html"><a href="big-data-stat.html#simulation"><i class="fa fa-check"></i><b>9.3.3</b> Simulation</a></li>
<li class="chapter" data-level="9.3.4" data-path="big-data-stat.html"><a href="big-data-stat.html#nominal-p-values"><i class="fa fa-check"></i><b>9.3.4</b> Nominal <span class="math inline">\(P-\)</span>values</a></li>
<li class="chapter" data-level="9.3.5" data-path="big-data-stat.html"><a href="big-data-stat.html#family-wise-error-rate"><i class="fa fa-check"></i><b>9.3.5</b> Family-wise error rate</a></li>
<li class="chapter" data-level="9.3.6" data-path="big-data-stat.html"><a href="big-data-stat.html#false-discovery-rate"><i class="fa fa-check"></i><b>9.3.6</b> False Discovery Rate</a></li>
<li class="chapter" data-level="9.3.7" data-path="big-data-stat.html"><a href="big-data-stat.html#overview-figure"><i class="fa fa-check"></i><b>9.3.7</b> Overview figure</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="big-data-stat.html"><a href="big-data-stat.html#conclusions"><i class="fa fa-check"></i><b>9.4</b> Conclusions</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="big-data-stat.html"><a href="big-data-stat.html#to-remember"><i class="fa fa-check"></i><b>9.4.1</b> To remember</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="big-data-stat.html"><a href="big-data-stat.html#references"><i class="fa fa-check"></i><b>9.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html"><i class="fa fa-check"></i><b>10</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#motivation-and-overview"><i class="fa fa-check"></i><b>10.1</b> Motivation and overview</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#testing-conditional-dependence"><i class="fa fa-check"></i><b>10.1.1</b> Testing conditional dependence</a></li>
<li class="chapter" data-level="10.1.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#linear-regression"><i class="fa fa-check"></i><b>10.1.2</b> Linear regression</a></li>
<li class="chapter" data-level="10.1.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#limitations"><i class="fa fa-check"></i><b>10.1.3</b> Limitations</a></li>
<li class="chapter" data-level="10.1.4" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#applications"><i class="fa fa-check"></i><b>10.1.4</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#univariate-regression"><i class="fa fa-check"></i><b>10.2</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#galtons-height-dataset"><i class="fa fa-check"></i><b>10.2.1</b> Galton’s height dataset</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#ML-LSE"><i class="fa fa-check"></i><b>10.2.2</b> Maximum likelihood and least squares estimates</a></li>
<li class="chapter" data-level="10.2.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#interpretation-of-the-fitted-coefficients"><i class="fa fa-check"></i><b>10.2.3</b> Interpretation of the fitted coefficients</a></li>
<li class="chapter" data-level="10.2.4" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#predicted-values-are-random-variables"><i class="fa fa-check"></i><b>10.2.4</b> Predicted values are random variables</a></li>
<li class="chapter" data-level="10.2.5" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#explained-variance"><i class="fa fa-check"></i><b>10.2.5</b> Explained variance</a></li>
<li class="chapter" data-level="10.2.6" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#testing-the-relationship-between-y-and-x"><i class="fa fa-check"></i><b>10.2.6</b> Testing the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#multivariate-regression"><i class="fa fa-check"></i><b>10.3</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#a-multivariate-example-the-baseball-dataset"><i class="fa fa-check"></i><b>10.3.1</b> A multivariate example: The baseball dataset</a></li>
<li class="chapter" data-level="10.3.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#fitting-multivariate-regression"><i class="fa fa-check"></i><b>10.3.2</b> Fitting multivariate regression</a></li>
<li class="chapter" data-level="10.3.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#testing-sets-of-parameters"><i class="fa fa-check"></i><b>10.3.3</b> Testing sets of parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#lin-reg-diagnostic"><i class="fa fa-check"></i><b>10.4</b> Diagnostic plots</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#assessing-non-linearity-with-residual-plot"><i class="fa fa-check"></i><b>10.4.1</b> Assessing non-linearity with residual plot</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#when-error-variance-is-not-constant-heteroscedascity"><i class="fa fa-check"></i><b>10.4.2</b> When error variance is not constant: Heteroscedascity</a></li>
<li class="chapter" data-level="10.4.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#gaussianity-q-q-plot-of-the-residuals"><i class="fa fa-check"></i><b>10.4.3</b> Gaussianity: Q-Q-plot of the residuals</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#conclusions-1"><i class="fa fa-check"></i><b>10.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-log-reg.html"><a href="chap-log-reg.html"><i class="fa fa-check"></i><b>11</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#a-univariate-example-predicting-sex-given-the-height"><i class="fa fa-check"></i><b>11.1</b> A univariate example: predicting sex given the height</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#from-linear-regression-to-logistic-regression"><i class="fa fa-check"></i><b>11.1.1</b> From linear regression to logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="chap-log-reg.html"><a href="chap-log-reg.html#ML-CE"><i class="fa fa-check"></i><b>11.2</b> Maximum likelihood estimates and the cross-entropy criterion</a></li>
<li class="chapter" data-level="11.3" data-path="chap-log-reg.html"><a href="chap-log-reg.html#logistic-regression-as-a-generalized-linear-model"><i class="fa fa-check"></i><b>11.3</b> Logistic regression as a generalized linear model</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#logistic-regression-with-r"><i class="fa fa-check"></i><b>11.3.1</b> Logistic regression with R</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap-log-reg.html"><a href="chap-log-reg.html#overview-plot-of-the-univariate-example"><i class="fa fa-check"></i><b>11.3.2</b> Overview plot of the univariate example</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap-log-reg.html"><a href="chap-log-reg.html#interpreting-a-logistic-regression-fit"><i class="fa fa-check"></i><b>11.4</b> Interpreting a logistic regression fit</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#predicted-odds"><i class="fa fa-check"></i><b>11.4.1</b> Predicted odds</a></li>
<li class="chapter" data-level="11.4.2" data-path="chap-log-reg.html"><a href="chap-log-reg.html#coefficients-of-the-logistic-regression"><i class="fa fa-check"></i><b>11.4.2</b> Coefficients of the logistic regression</a></li>
<li class="chapter" data-level="11.4.3" data-path="chap-log-reg.html"><a href="chap-log-reg.html#effects-on-probabilities"><i class="fa fa-check"></i><b>11.4.3</b> Effects on probabilities</a></li>
<li class="chapter" data-level="11.4.4" data-path="chap-log-reg.html"><a href="chap-log-reg.html#class-imbalance"><i class="fa fa-check"></i><b>11.4.4</b> Class imbalance</a></li>
<li class="chapter" data-level="11.4.5" data-path="chap-log-reg.html"><a href="chap-log-reg.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>11.4.5</b> Multiple Logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="chap-log-reg.html"><a href="chap-log-reg.html#assessing-the-performance-of-a-classifier"><i class="fa fa-check"></i><b>11.5</b> Assessing the performance of a classifier</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#classification-with-logistic-regression"><i class="fa fa-check"></i><b>11.5.1</b> Classification with logistic regression</a></li>
<li class="chapter" data-level="11.5.2" data-path="chap-log-reg.html"><a href="chap-log-reg.html#confusion-matrix"><i class="fa fa-check"></i><b>11.5.2</b> Confusion Matrix</a></li>
<li class="chapter" data-level="11.5.3" data-path="chap-log-reg.html"><a href="chap-log-reg.html#classification-performance-metrics"><i class="fa fa-check"></i><b>11.5.3</b> Classification performance metrics</a></li>
<li class="chapter" data-level="11.5.4" data-path="chap-log-reg.html"><a href="chap-log-reg.html#choosing-a-classification-cutoff"><i class="fa fa-check"></i><b>11.5.4</b> Choosing a classification cutoff</a></li>
<li class="chapter" data-level="11.5.5" data-path="chap-log-reg.html"><a href="chap-log-reg.html#roc-curve"><i class="fa fa-check"></i><b>11.5.5</b> ROC curve</a></li>
<li class="chapter" data-level="11.5.6" data-path="chap-log-reg.html"><a href="chap-log-reg.html#precision-recall-curve"><i class="fa fa-check"></i><b>11.5.6</b> Precision Recall curve</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="chap-log-reg.html"><a href="chap-log-reg.html#conclusions-2"><i class="fa fa-check"></i><b>11.6</b> Conclusions</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#to-remember-1"><i class="fa fa-check"></i><b>11.6.1</b> To remember</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>12</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="supervised-learning.html"><a href="supervised-learning.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#motivation-2"><i class="fa fa-check"></i><b>12.1.1</b> Motivation</a></li>
<li class="chapter" data-level="12.1.2" data-path="supervised-learning.html"><a href="supervised-learning.html#supervised-learning-vs.-unsupervised-learning"><i class="fa fa-check"></i><b>12.1.2</b> Supervised learning vs. unsupervised learning</a></li>
<li class="chapter" data-level="12.1.3" data-path="supervised-learning.html"><a href="supervised-learning.html#notation"><i class="fa fa-check"></i><b>12.1.3</b> Notation</a></li>
<li class="chapter" data-level="12.1.4" data-path="supervised-learning.html"><a href="supervised-learning.html#basic-approach-in-supervised-machine-learning"><i class="fa fa-check"></i><b>12.1.4</b> Basic approach in supervised machine learning</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="supervised-learning.html"><a href="supervised-learning.html#over--and-under-fitting"><i class="fa fa-check"></i><b>12.2</b> Over- and Under-fitting</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#example-polynomial-curve-fitting"><i class="fa fa-check"></i><b>12.2.1</b> Example: polynomial curve fitting</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="supervised-learning.html"><a href="supervised-learning.html#splitting-the-dataset-for-performance-assessment"><i class="fa fa-check"></i><b>12.3</b> Splitting the dataset for performance assessment</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="supervised-learning.html"><a href="supervised-learning.html#over-fitting-to-the-training-dataset"><i class="fa fa-check"></i><b>12.3.1</b> Over-fitting to the training dataset</a></li>
<li class="chapter" data-level="12.3.2" data-path="supervised-learning.html"><a href="supervised-learning.html#cross-validation"><i class="fa fa-check"></i><b>12.3.2</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forests-as-alternative-models"><i class="fa fa-check"></i><b>12.4</b> Random Forests as alternative models</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="supervised-learning.html"><a href="supervised-learning.html#the-basics-of-decision-trees"><i class="fa fa-check"></i><b>12.4.1</b> The basics of decision trees</a></li>
<li class="chapter" data-level="12.4.2" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forests-for-classification-and-regression-tasks"><i class="fa fa-check"></i><b>12.4.2</b> Random Forests for classification and regression tasks</a></li>
<li class="chapter" data-level="12.4.3" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forests-in-r"><i class="fa fa-check"></i><b>12.4.3</b> Random Forests in R</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="supervised-learning.html"><a href="supervised-learning.html#conclusion-2"><i class="fa fa-check"></i><b>12.5</b> Conclusion</a></li>
<li class="chapter" data-level="12.6" data-path="supervised-learning.html"><a href="supervised-learning.html#resources-4"><i class="fa fa-check"></i><b>12.6</b> Resources</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>A</b> Importing data</a>
<ul>
<li class="chapter" data-level="A.1" data-path="importing-data.html"><a href="importing-data.html#paths-and-the-working-directory"><i class="fa fa-check"></i><b>A.1</b> Paths and the working directory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="importing-data.html"><a href="importing-data.html#the-filesystem"><i class="fa fa-check"></i><b>A.1.1</b> The filesystem</a></li>
<li class="chapter" data-level="A.1.2" data-path="importing-data.html"><a href="importing-data.html#relative-and-full-paths"><i class="fa fa-check"></i><b>A.1.2</b> Relative and full paths</a></li>
<li class="chapter" data-level="A.1.3" data-path="importing-data.html"><a href="importing-data.html#the-working-directory"><i class="fa fa-check"></i><b>A.1.3</b> The working directory</a></li>
<li class="chapter" data-level="A.1.4" data-path="importing-data.html"><a href="importing-data.html#generating-path-names"><i class="fa fa-check"></i><b>A.1.4</b> Generating path names</a></li>
<li class="chapter" data-level="A.1.5" data-path="importing-data.html"><a href="importing-data.html#copying-files-using-paths"><i class="fa fa-check"></i><b>A.1.5</b> Copying files using paths</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="importing-data.html"><a href="importing-data.html#the-readr-and-readxl-packages"><i class="fa fa-check"></i><b>A.2</b> The readr and readxl packages</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>A.2.1</b> readr</a></li>
<li class="chapter" data-level="A.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>A.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="importing-data.html"><a href="importing-data.html#exercises"><i class="fa fa-check"></i><b>A.3</b> Exercises</a></li>
<li class="chapter" data-level="A.4" data-path="importing-data.html"><a href="importing-data.html#downloading-files"><i class="fa fa-check"></i><b>A.4</b> Downloading files</a></li>
<li class="chapter" data-level="A.5" data-path="importing-data.html"><a href="importing-data.html#r-base-importing-functions"><i class="fa fa-check"></i><b>A.5</b> R-base importing functions</a>
<ul>
<li><a href="importing-data.html#scan"><code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="importing-data.html"><a href="importing-data.html#text-versus-binary-files"><i class="fa fa-check"></i><b>A.6</b> Text versus binary files</a></li>
<li class="chapter" data-level="A.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>A.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="A.8" data-path="importing-data.html"><a href="importing-data.html#organizing-data-with-spreadsheets"><i class="fa fa-check"></i><b>A.8</b> Organizing data with spreadsheets</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html"><i class="fa fa-check"></i><b>B</b> R programming</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#pipe"><i class="fa fa-check"></i><b>B.1</b> The pipe operator</a></li>
<li class="chapter" data-level="B.2" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#conditionals"><i class="fa fa-check"></i><b>B.2</b> Conditional expressions</a></li>
<li class="chapter" data-level="B.3" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#defining-functions"><i class="fa fa-check"></i><b>B.3</b> Defining functions</a></li>
<li class="chapter" data-level="B.4" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#namespaces"><i class="fa fa-check"></i><b>B.4</b> Namespaces</a></li>
<li class="chapter" data-level="B.5" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#for-loops"><i class="fa fa-check"></i><b>B.5</b> For-loops</a></li>
<li class="chapter" data-level="B.6" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#vectorization"><i class="fa fa-check"></i><b>B.6</b> Vectorization and functionals</a></li>
<li class="chapter" data-level="B.7" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#r-markdown"><i class="fa fa-check"></i><b>B.7</b> R Markdown</a></li>
<li class="chapter" data-level="B.8" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#resources-5"><i class="fa fa-check"></i><b>B.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html"><i class="fa fa-check"></i><b>C</b> Additonal plotting tools</a>
<ul>
<li class="chapter" data-level="C.1" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#plotting-themes"><i class="fa fa-check"></i><b>C.1</b> Plotting themes</a></li>
<li class="chapter" data-level="C.2" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#axes"><i class="fa fa-check"></i><b>C.2</b> Axes</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#axis-elements"><i class="fa fa-check"></i><b>C.2.1</b> Axis elements</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#plot-title"><i class="fa fa-check"></i><b>C.3</b> Plot title</a></li>
<li class="chapter" data-level="C.4" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#legend"><i class="fa fa-check"></i><b>C.4</b> Legend</a></li>
<li class="chapter" data-level="C.5" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#interactive-plots"><i class="fa fa-check"></i><b>C.5</b> Interactive plots</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html"><i class="fa fa-check"></i><b>D</b> Probabilities</a>
<ul>
<li class="chapter" data-level="D.1" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#probability-conditional-probability-and-dependence"><i class="fa fa-check"></i><b>D.1</b> Probability, conditional probability, and dependence</a></li>
<li class="chapter" data-level="D.2" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#expected-value-variance-and-covariance"><i class="fa fa-check"></i><b>D.2</b> Expected value, variance, and covariance</a></li>
<li class="chapter" data-level="D.3" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#probs-sample-estimates"><i class="fa fa-check"></i><b>D.3</b> Sample estimates</a></li>
<li class="chapter" data-level="D.4" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#appendix-lin-reg"><i class="fa fa-check"></i><b>D.4</b> Linear regression</a></li>
<li class="chapter" data-level="D.5" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#resources-6"><i class="fa fa-check"></i><b>D.5</b> Resources</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="./">Julien Gagneur, TUM</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis and Visualization in R (IN2339)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="supervised-learning" class="section level1" number="12">
<h1><span class="header-section-number">Chapter 12</span> Supervised Learning</h1>
<div id="introduction-3" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Introduction</h2>
<div id="motivation-2" class="section level3" number="12.1.1">
<h3><span class="header-section-number">12.1.1</span> Motivation</h3>
<p>In the last two chapters we introduced linear and logistic regression models. The primary motivation lied in the interpretation of the coefficients, seen as the effect of explanatory variables on the response adjusted for the effects of other, potentially correlated, variables. Under some assumptions this even allowed us – holy grail – to conclude about conditional independence. Often, these models were considered as interpretable data generative models, supposed to reflect the underlying process.</p>
<p>In this chapter, we will take a supervised machine learning angle. Here, we are interested in good predictions rather than identifying the most predictive features or drawing conclusions about statistical independence. On the one hand, the goal is less ambitious. On the other hand, focusing on prediction accuracy alone allows unleashing a wide variety of modeling techniques. The application area is immense and contains the most prominent successes of today’s artificial intelligence including medical diagnosis, image recognition, automatized translation, etc. We will learn how to evaluate the prediction performance on an independent test set and avoid problems related to poor generalization of a model to an independent test set. Additionally, we will introduce random forests as an alternative machine learning model for both regression and classification tasks.</p>
</div>
<div id="supervised-learning-vs.-unsupervised-learning" class="section level3" number="12.1.2">
<h3><span class="header-section-number">12.1.2</span> Supervised learning vs. unsupervised learning</h3>
<p>Machine learning is the study of computer algorithms that improve automatically through experience<a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a>.
Most machine learning problems can be mainly categorized into two: supervised or unsupervised problems. In <strong>supervised learning</strong>, the goal is to build a powerful algorithm that takes feature values as input and returns a prediction for an outcome, even when we do not know the value for the actual outcome. For this, we <em>train</em> an algorithm using a data set for which we know the outcome, and then use this trained model to make predictions. In particular, for building the model, we associate each set of feature values to a certain outcome. In <strong>unsupervised learning</strong>, we do not associate feature values to an outcome. The situation is referred to as unsupervised because we lack an outcome variable that can supervise our analysis and model building. Instead, unsupervised learning algorithms identify patterns in the distribution of data. Typically, clustering problems and dimensionality reduction are attributed to unsupervised machine learning problems.</p>
<p>A powerful framework to study supervised and unsupervised learning is statistical learning, which express these problems in statistical terms. Supervised learning then maps to the task of fitting conditional distribution <span class="math inline">\(p(y|\mathbf x)\)</span> where <span class="math inline">\(y\)</span> is the outcome and <span class="math inline">\(\mathbf x\)</span> are the features (or fitting aspects of the conditional distribution: the expectation, etc.). Linear regression is one method we already saw. Unsupervised learning in contrast refers to the task of fitting the distribution of the data <span class="math inline">\(p(\mathbf x)\)</span>, or some aspects of it (covariance: PCA, mixture components: clustering, etc.)</p>
<p>This chapter focuses on supervised learning. In fact, logistic and linear regression are two classical methods in supervised learning for solving classification and regression tasks.</p>
</div>
<div id="notation" class="section level3" number="12.1.3">
<h3><span class="header-section-number">12.1.3</span> Notation</h3>
<p>In this chapter, we will use the classical machine learning nomenclature:</p>
<ul>
<li><span class="math inline">\(y\)</span> denotes the <em>outcome</em> (or response) that we want to predict</li>
<li><span class="math inline">\(x_1, \dots, x_p\)</span> denote the <em>features</em> that we will use to predict the outcome.</li>
</ul>
<!-- The data set used for training is usually referred to as training data set.  -->
</div>
<div id="basic-approach-in-supervised-machine-learning" class="section level3" number="12.1.4">
<h3><span class="header-section-number">12.1.4</span> Basic approach in supervised machine learning</h3>
<p>In regression and classification tasks, we have a series of features and an unknown numeric or categorial outcome we want to predict:</p>
<table>
<thead>
<tr class="header">
<th align="center">outcome</th>
<th align="center">feature_1</th>
<th align="center">feature_2</th>
<th align="center">feature_3</th>
<th align="center">feature_4</th>
<th align="center">feature_5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">?</td>
<td align="center"><span class="math inline">\(x_1\)</span></td>
<td align="center"><span class="math inline">\(x_2\)</span></td>
<td align="center"><span class="math inline">\(x_3\)</span></td>
<td align="center"><span class="math inline">\(x_4\)</span></td>
<td align="center"><span class="math inline">\(x_5\)</span></td>
</tr>
</tbody>
</table>
<p>To <em>build a model</em> that provides a prediction for any set of observed values <span class="math inline">\(x_1, x_2, \dots x_5\)</span>, we collect data for which we know the outcome:</p>
<table>
<thead>
<tr class="header">
<th align="left">outcome</th>
<th align="left">feature_1</th>
<th align="left">feature_2</th>
<th align="left">feature_3</th>
<th align="left">feature_4</th>
<th align="left">feature_5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(y_{1}\)</span></td>
<td align="left"><span class="math inline">\(x_{1,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{1,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{1,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{1,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{1,5}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y_{2}\)</span></td>
<td align="left"><span class="math inline">\(x_{2,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{2,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{2,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{2,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{2,5}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(y_{3}\)</span></td>
<td align="left"><span class="math inline">\(x_{3,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{3,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{3,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{3,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{3,5}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y_{4}\)</span></td>
<td align="left"><span class="math inline">\(x_{4,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{4,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{4,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{4,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{4,5}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(y_{5}\)</span></td>
<td align="left"><span class="math inline">\(x_{5,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{5,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{5,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{5,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{5,5}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y_{6}\)</span></td>
<td align="left"><span class="math inline">\(x_{6,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{6,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{6,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{6,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{6,5}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(y_{7}\)</span></td>
<td align="left"><span class="math inline">\(x_{7,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{7,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{7,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{7,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{7,5}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y_{8}\)</span></td>
<td align="left"><span class="math inline">\(x_{8,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{8,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{8,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{8,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{8,5}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(y_{9}\)</span></td>
<td align="left"><span class="math inline">\(x_{9,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{9,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{9,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{9,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{9,5}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(y_{10}\)</span></td>
<td align="left"><span class="math inline">\(x_{10,1}\)</span></td>
<td align="left"><span class="math inline">\(x_{10,2}\)</span></td>
<td align="left"><span class="math inline">\(x_{10,3}\)</span></td>
<td align="left"><span class="math inline">\(x_{10,4}\)</span></td>
<td align="left"><span class="math inline">\(x_{10,5}\)</span></td>
</tr>
</tbody>
</table>
<p>We use this data to train the model and then use the trained model to apply to our (new) data for which we do not know the outcome.</p>
<p>Now… how do we evaluate our model? And how do we make sure our model will perform well on our (new) data?</p>
</div>
</div>
<div id="over--and-under-fitting" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Over- and Under-fitting</h2>
<p>Analyzing whether our built model is generalizing well and capturing the trend of the data is an essential aspect of supervised machine learning. Two situations should be avoided: (1) that the model does not capture the trends of the data resulting in a high measured error between actual and predicted outcomes (under-fitting) and (2) that the model does not generalize well but fits the data used for training the model too well (over-fitting).</p>
<div id="example-polynomial-curve-fitting" class="section level3" number="12.2.1">
<h3><span class="header-section-number">12.2.1</span> Example: polynomial curve fitting</h3>
<p>As an example, we consider a polynomial curve fitting as a regression task with a dataset of <span class="math inline">\(n =10\)</span> points shown in Figure <a href="supervised-learning.html#fig:poly1">12.1</a> , each comprising an observation of the input variable <span class="math inline">\(x\)</span> along with the corresponding outcome variable <span class="math inline">\(y\)</span>. In this example, the data is generated from the function <span class="math display">\[f(x)=\sin(2πx)\]</span> (shown in blue) with added random noise from a normal distribution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poly1"></span>
<img src="dataviz_book_files/figure-html/poly1-1.png" alt="Data set of $N =10$ points  with a blue curve showing the function $sin(2πx)$ used to generate the data" width="384" />
<p class="caption">
Figure 12.1: Data set of <span class="math inline">\(N =10\)</span> points with a blue curve showing the function <span class="math inline">\(sin(2πx)\)</span> used to generate the data
</p>
</div>
<!-- ```{r poly1, out.width = "300px", echo=FALSE, fig.cap="Data set of $N =10$ points (blue circles) with a green curve showing the function $sin(2πx)$ used to generate the data"} -->
<!-- knitr::include_graphics("assets/img/topic12_supervised_learning/overfitting_ex.png") -->
<!-- ``` -->
<p>Assuming that we do not know the function plotted in the blue curve, the goal is to predict the value of <span class="math inline">\(y\)</span> for some new arbitrary value of <span class="math inline">\(x\)</span>. For this, we want to exploit the given dataset of <span class="math inline">\(n\)</span> data points in order to make predictions of the value <span class="math inline">\(y\)</span> of the outcome variable for some new value <span class="math inline">\(x\)</span>, which involves implicitly trying to discover the underlying function <span class="math inline">\(f(x)=\sin(2πx)\)</span>. This is a difficult problem considering that we have to generalize from a finite dataset of only <span class="math inline">\(n\)</span> points. Furthermore, the given data points are include noise, and so for a given <span class="math inline">\(x\)</span>, there is a certain uncertainty regarding the correct value for <span class="math inline">\(y\)</span>.</p>
<p>We consider a simple approach based on curve fitting to make predictions for a given value of <span class="math inline">\(x\)</span>. More precisely, we fit the data using a polynomial function of the form:</p>
<p><span class="math display">\[
y(x, \boldsymbol\beta) = \beta_0 + \beta_1x + \beta_2x^2 + ... + \beta_mx^m = \sum_{j=0}^m w_j x^j
\]</span></p>
<p>Here, <span class="math inline">\(m\)</span> is the order of the polynomial function and <span class="math inline">\(x^j\)</span> denotes <span class="math inline">\(x\)</span> raised to the power of <span class="math inline">\(j\)</span>. The polynomial coefficients <span class="math inline">\(\beta_0,...,\beta_m\)</span> are denoted by the vector <span class="math inline">\(\boldsymbol\beta\)</span>. Note that, the polynomial function <span class="math inline">\(y(x, \boldsymbol\beta)\)</span> is a nonlinear function of <span class="math inline">\(x\)</span> but it is a linear function of the coefficients <span class="math inline">\(\boldsymbol\beta\)</span>. Hence, a linear regression model (see Chapter <a href="chap-lin-reg.html#chap-lin-reg">10</a>) can be applied to find optimal values of <span class="math inline">\(\boldsymbol\beta\)</span> assuming that we have selected a value for the order <span class="math inline">\(m\)</span> of the polynomial fit.</p>
<p>As described in Chapter <a href="chap-lin-reg.html#chap-lin-reg">10</a>, an optimal linear model (with optimal <span class="math inline">\(\boldsymbol\beta\)</span> values) can be found by minimizing an error function (sum of squared errors) that measures the misfit between the function <span class="math inline">\(y(x, \boldsymbol\beta)\)</span> and the data points in the considered dataset.</p>
<p>Selecting different values for the order <span class="math inline">\(m\)</span> of the polynomial has a huge impact on the resulting model. In Figure <a href="supervised-learning.html#fig:poly2">12.2</a>, we show four examples of the results of fitting polynomials with different orders <span class="math inline">\(m = 0, 1, 3\)</span> and <span class="math inline">\(9\)</span> to our dataset of <span class="math inline">\(n=10\)</span> points.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poly2"></span>
<img src="dataviz_book_files/figure-html/poly2-1.png" alt="Plots of polynomials having various orders $M$, shown as green curves, fitted to the considered data set with underlying expected value function in blue." width="384" />
<p class="caption">
Figure 12.2: Plots of polynomials having various orders <span class="math inline">\(M\)</span>, shown as green curves, fitted to the considered data set with underlying expected value function in blue.
</p>
</div>
<!-- ```{r poly2, out.width = "300px", echo=FALSE, fig.cap="Plots of polynomials having various orders $M$, shown as red curves, fitted to the considered data set"} -->
<!-- knitr::include_graphics(c("assets/img/topic12_supervised_learning/overfitting_ex_all.png")) -->
<!-- ``` -->
<p>We can clearly see that the constant (<span class="math inline">\(m = 0\)</span>) and first order (<span class="math inline">\(m = 1\)</span>) polynomials give rather poor fits to the data and consequently rather poor representations of the function <span class="math inline">\(\sin(2πx)\)</span>. These two polynomial fits fail to capture the underlying trend of the data. A high error can be computed between the predicted and the actual data samples. These are examples of under-fitting.</p>
<p>The third order (<span class="math inline">\(m = 3\)</span>) polynomial seems to give the best fit to the function <span class="math inline">\(\sin(2πx)\)</span>. By selecting <span class="math inline">\(m=3\)</span> in the previous example, we observe an appropriate balance between capturing the trends in the data to achieve good generalization and making accurate predictions for outcome values.</p>
<p>When we go to a much higher order polynomial (<span class="math inline">\(m = 9\)</span>), we obtain a polynomial that passes exactly through each data point. However, the fitted polynomial fails to give a good representation of
the function <span class="math inline">\(\sin(2πx)\)</span>. The fitted curve considers each deviation in the data points (including noise). The model is too sensitive to noise and captures random patterns which are present only in the current dataset. Poor generalization to other datasets is expected. This latter behavior is known as over-fitting.</p>
<p>It is particularly interesting to examine the behavior of a given model as the size of the data set varies. We can see that, for a given model complexity, the over-fitting problem become less severe as the size of the data set increases. In our example, by considering not only <span class="math inline">\(10\)</span>, but <span class="math inline">\(n=15\)</span> or <span class="math inline">\(n=100\)</span> data points, we observe that increasing the size of the data set reduces the over-fitting problem (see Figure <a href="supervised-learning.html#fig:poly3">12.3</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poly3"></span>
<img src="dataviz_book_files/figure-html/poly3-1.png" alt="Polynomial curve fitting with $n=15$ and $n=100$ data points and polynomial order $m=9$ in green and $\sin(2πx)$ in blue." width="384" />
<p class="caption">
Figure 12.3: Polynomial curve fitting with <span class="math inline">\(n=15\)</span> and <span class="math inline">\(n=100\)</span> data points and polynomial order <span class="math inline">\(m=9\)</span> in green and <span class="math inline">\(\sin(2πx)\)</span> in blue.
</p>
</div>
<p>This example, borrowed from the Christopher Bishop’s textbook, is very informative about the approach taken in and the issues faced by supervised learning.</p>
<p>First, supervised learning methods are more or less agnostic of the underlying processes. In our example, the underlying function is a sine. We may be looking at tides, an oscillating spring, etc. A physicist could come up with a model that leads to the right class of mathematical function. In supervised learning, we simply work with a generic class of flexible mathematical functions (here the polynomials, but there are many others), which can in principle approximate any function. One implication is that we will not aim at interpreting the coefficients. We would not pretend that the coefficient of say <span class="math inline">\(x^4\)</span> has any physical meaning. A physicist would (and could!) in contrast interpret the amplitude or the period of the sine. In supervised learning, what guides our choice of the fit is data and only data. In particular, the complexity of the function we chose (here the order of polynomials) depends on how much data we have and not on theoretical considerations about the underlying process.</p>
<p>Second, the key issue is that it is easy with flexible mathematical functions to fit extremely well to a dataset. The challenge is not reducing the error on data at hand but the error on unseen data. Expected error on unseen data is called the <strong>generalization error</strong>. We will now investigate practical techniques to control the generalization error.</p>
</div>
</div>
<div id="splitting-the-dataset-for-performance-assessment" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Splitting the dataset for performance assessment</h2>
<p>As previously stated, a supervised learning task starts with an available dataset that we use to build a model so that this model will eventually be used in completely independent datasets, as shown in Figure <a href="supervised-learning.html#fig:dataset1">12.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dataset1"></span>
<img src="assets/img/topic12_supervised_learning/dataset.png" alt="Illustration of an available set (in blue), whose outcome values are known and which is used to train a model, and an independent dataset (in grey), whose outcome values are unknown." width="500px" />
<p class="caption">
Figure 12.4: Illustration of an available set (in blue), whose outcome values are known and which is used to train a model, and an independent dataset (in grey), whose outcome values are unknown.
</p>
</div>
<p>The first challenge arises when we do not know these independent datasets. However, in such cases, we still want to make sure that our model is not only working well when applied to the available dataset but can generalize well to independent unknown datasets. In particular, we want to make sure that we are not over-fitting to the dataset that the model was trained on.</p>
<p>How to minimize error on data we have never seen? We will make the following assumption:</p>
<blockquote>
<p>We assume that the observations <span class="math inline">\(\mathbf x_i, i=1...n\)</span> of our dataset and of unseen data are i.i.d., meaning they are independent obsvervations of the same population.</p>
</blockquote>
<p>Under this assumption, one common strategy to evaluate the performance of the model on independent datasets is to select a subset of our dataset and pretend it is an independent dataset. As illustrated in Figure <a href="supervised-learning.html#fig:dataset2">12.5</a>, we divide the available dataset into a training set and a test set. We will train our algorithm exclusively on the training set and use the test set only for evaluation purposes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dataset2"></span>
<img src="assets/img/topic12_supervised_learning/train_test_dataset.png" alt="Separation of an available dataset into a training dataset (in green) and a test dataset (in orange) for building and evaluating a model." width="500px" />
<p class="caption">
Figure 12.5: Separation of an available dataset into a training dataset (in green) and a test dataset (in orange) for building and evaluating a model.
</p>
</div>
<p>Typically, we select a small piece of the dataset so that we have as much data as possible to train. However, we also want the test set to be large enough to obtain a stable estimate of the performance of the model on independent datasets. Common choices for the size of the train set are to use 10%-30% of the data for testing.</p>
<div id="over-fitting-to-the-training-dataset" class="section level3" number="12.3.1">
<h3><span class="header-section-number">12.3.1</span> Over-fitting to the training dataset</h3>
<p>Over-fitting can be detected when the measured error computed from a defined error function is notably larger for the test dataset than for the training dataset. Alternatively, one can use performance measurements such as precision or recall or visualize the performance of the model on the training vs. test dataset with ROC or precision-recall curves. Notable differences with the measurements or the plotted curves are signs of over-fitting.</p>
<p>In the previous example of the polynomial curve fitting, we can split the data consisting of <span class="math inline">\(n=10\)</span> data points into train and test datasets and compare the computed error for different polynomial orders <span class="math inline">\(n\)</span>. For <span class="math inline">\(m=9\)</span>, the computed root-mean-squared error for the test dataset notably increases while the error for the training dataset decreases.</p>
<!-- Wesee in Figure \@ref(fig:poly4) that for $M=9$, the computed root-mean-squared error $E_{RMS}$ for the test dataset notably increases while the error for the training dataset decreases: -->
<!-- ```{r poly4, out.width = "500px", echo=FALSE, fig.cap="Graphs of the root-mean-square error, evaluated on the training set and on an test set for various values of M for the polynomial curve fitting example."} -->
<!-- knitr::include_graphics("assets/img/topic12_supervised_learning/overfitting_ex_error.png") -->
<!-- ``` -->
<!-- In practice, there are several approaches to avoid over-fitting. These include: -->
<!-- * increasing the size of the dataset (which is not always possible), -->
<!-- * hyper-parameter tuning (i.e. manipulating parameters that are specific to a model type, e.g. number of trees in a random forest), -->
<!-- * removing features, -->
<!-- * regularization techniques (out of scope of this lecture),  -->
<!-- * early stopping (or early termination) and -->
<!-- * cross-validation (which will be introduced in the following section). -->
<!-- More precisely, early termination is referred to stopping the the training process before the model’s ability to generalize weakens as it begins to over-fit the training data. Graphically, this can be explained with the following figure: -->
<!-- ```{r early stopiing, out.width = "300px", echo=FALSE, fig.cap="Graphical explanation of early-termination, meaning to stop the training process when the difference between the testing and training error starts to increase notably."} -->
<!-- knitr::include_graphics("assets/img/topic12_supervised_learning/early_stopping.png") -->
<!-- ``` -->
<!-- Now, splitting the dataset into train and test, presents a new problem because for most machine learning algorithms we need to select hyper-parameters, for example the number of neighbors  k  in k-nearest neighbors. Here, we will refer to the set of hyper-parameters as  $\lambda$. We need to optimize hyper-parameters without using our test set and we can imagine that if we optimize and evaluate on the same dataset, we will probably overfit. This is where cross-validation is useful. Furthermore, if not sufficient data is available to create train and test sets cross-validation is often used as a strategy to assess the performance of a machine learning model.  -->
</div>
<div id="cross-validation" class="section level3" number="12.3.2">
<h3><span class="header-section-number">12.3.2</span> Cross-validation</h3>
<p>Cross-validation is often used as a strategy to assess the performance of a machine learning model that can help to prevent over-fitting.</p>
<p>In cross-validation the data is randomly split into a number k of folds (e.g. 3, 5 or 10), as illustrated in Figure <a href="supervised-learning.html#fig:crossval1">12.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crossval1"></span>
<img src="assets/img/topic12_supervised_learning/c_val.png" alt="Schematic Illustration of cross-validation with $k=3$ folds." width="700px" />
<p class="caption">
Figure 12.6: Schematic Illustration of cross-validation with <span class="math inline">\(k=3\)</span> folds.
</p>
</div>
<p>Then, a model is repeatedly trained on <span class="math inline">\(k-1\)</span> folds and evaluated on the fold not used for training. In this manner, we train <span class="math inline">\(k\)</span> models with a different training dataset on each fold <span class="math inline">\(i\)</span> with <span class="math inline">\(i\in\{1,...,k\}\)</span>. After the <span class="math inline">\(k\)</span> models have been trained and evaluated on the respective left-out folds, evaluation metrics can be summarized into a combined evaluation metric.</p>
<p>Now, how do we pick <span class="math inline">\(k\)</span> for the number of folds? Large values of <span class="math inline">\(k\)</span> are preferable because the training data better imitates the original dataset. However, larger values of <span class="math inline">\(k\)</span> will have much slower computation time: for example, 100-fold cross-validation will be 10 times slower than 10-fold cross-validation. For this reason, the choices of <span class="math inline">\(k=5\)</span> and <span class="math inline">\(k=10\)</span> are usually considered in practice.</p>
<div id="pitfalls-of-cross-validation" class="section level4" number="12.3.2.1">
<h4><span class="header-section-number">12.3.2.1</span> Pitfalls of cross-validation</h4>
<p>For considering cross-validation one fundamental assumption has to be met, namely that the training samples and the test samples are independently and identically distributed (i.i.d.). As a trivial example, we would consider a non identical distribution if our training set consisted of red apples and green pears, but our test set also contained green apples. The issue of having non independent distributions may arise if we have data coming in clusters that we are not aware of (e.g: repeated measures, people from same families, homologous genes).</p>
<p>As an example of this issue, we consider a simulated dataset <code>simulated_dt</code> consisting of a feature variable <code>x</code> and an outcome variable <code>y</code>:</p>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb699-1"><a href="supervised-learning.html#cb699-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(simulated_dt, <span class="at">n=</span><span class="dv">10</span>)</span></code></pre></div>
<pre><code>##     x           y
##  1: 1  0.00000000
##  2: 1 -0.03377742
##  3: 1  0.25579509
##  4: 1  0.22420401
##  5: 1  0.11684048
##  6: 1 -0.05580587
##  7: 1  0.39764426
##  8: 1  0.13625628
##  9: 1 -0.02500456
## 10: 1 -0.02142251</code></pre>
<p>We observe that the dataset contains repeated measures and that the between replicate simulated trend is <code>x=y</code></p>
<p><img src="dataviz_book_files/figure-html/unnamed-chunk-443-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>Performing cross-validation at the level of individual data points will favor models that learn the clusters:</p>
<p><img src="dataviz_book_files/figure-html/unnamed-chunk-444-1.png" width="384" style="display: block; margin: auto;" /></p>
<p>We need to perform cross-validation at the cluster level to learn the trend across clusters. However, this is difficult without application knowledge. Visualization techniques can help. Application areas may have their own strategies. In genetics, models are required to be replicated on independent populations (genetic associations on say Swedes must be replicated among say Germans). The issue also arises with temporal data. Cross-validation cannot be done by taking random subsets of past data points. For temporal data, it is common practice (and common sense) to test models my mimicking predictions of future time points. See a broader discussion of this issue by <span class="citation"><a href="#ref-Roberts2017" role="doc-biblioref">Roberts et al.</a> (<a href="#ref-Roberts2017" role="doc-biblioref">2017</a>)</span>.</p>
<p><img src="dataviz_book_files/figure-html/unnamed-chunk-445-1.png" width="384" style="display: block; margin: auto;" /></p>
</div>
<div id="cross-validation-in-r" class="section level4" number="12.3.2.2">
<h4><span class="header-section-number">12.3.2.2</span> Cross-validation in R</h4>
<p>In R, one approach to implement a cross validated model is using the <code>caret</code> package. The <code>caret</code> package consolidates different machine learning algorithms and tools from different authors into one library. It conveniently provides a function that performs cross-validation.</p>
<p>As an example, we implement a logistic regression model to solve the binary classification task from Chapter <a href="chap-log-reg.html#chap-log-reg">11</a>, which consists in predicting the gender of a person based on height values of him/herself and his/her parents. First, we load the heights dataset:</p>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="supervised-learning.html#cb701-1" aria-hidden="true" tabindex="-1"></a>heights_dt <span class="ot">&lt;-</span> <span class="fu">fread</span>(<span class="st">&quot;extdata/height.csv&quot;</span>) <span class="sc">%&gt;%</span> <span class="fu">na.omit</span>() <span class="sc">%&gt;%</span></span>
<span id="cb701-2"><a href="supervised-learning.html#cb701-2" aria-hidden="true" tabindex="-1"></a>  .[, sex<span class="sc">:</span><span class="er">=</span><span class="fu">as.factor</span>(<span class="fu">toupper</span>(sex))]</span>
<span id="cb701-3"><a href="supervised-learning.html#cb701-3" aria-hidden="true" tabindex="-1"></a>heights_dt</span></code></pre></div>
<p>Then, we define the validation specification with the help of the function <code>trainControl()</code>. Here, we set the method as <code>cv</code> for cross-validation with <code>k=5</code> folds as the <code>number</code> argument. Additionally, we define <code>twoClassSummary</code> as the summary function for computing the sensitivity, specificity and the area under the ROC curve for performance measurement in binary classification tasks.</p>
<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb702-1"><a href="supervised-learning.html#cb702-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;caret&quot;)</span></span>
<span id="cb702-2"><a href="supervised-learning.html#cb702-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb702-3"><a href="supervised-learning.html#cb702-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb702-4"><a href="supervised-learning.html#cb702-4" aria-hidden="true" tabindex="-1"></a><span class="co"># generate control structure</span></span>
<span id="cb702-5"><a href="supervised-learning.html#cb702-5" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span>    <span class="co"># number of folds</span></span>
<span id="cb702-6"><a href="supervised-learning.html#cb702-6" aria-hidden="true" tabindex="-1"></a>fitControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="co"># cv for cross-validation</span></span>
<span id="cb702-7"><a href="supervised-learning.html#cb702-7" aria-hidden="true" tabindex="-1"></a>                           <span class="at">number =</span> k, </span>
<span id="cb702-8"><a href="supervised-learning.html#cb702-8" aria-hidden="true" tabindex="-1"></a>                           <span class="at">classProbs=</span><span class="cn">TRUE</span>, <span class="co"># compute class probabilities </span></span>
<span id="cb702-9"><a href="supervised-learning.html#cb702-9" aria-hidden="true" tabindex="-1"></a>                           <span class="at">summaryFunction =</span> twoClassSummary</span>
<span id="cb702-10"><a href="supervised-learning.html#cb702-10" aria-hidden="true" tabindex="-1"></a>                           )</span></code></pre></div>
<p>Next, we can train the model with the <code>train()</code> function. Here, we set the previously defined <code>fitControl</code> as the training control for implementing cross-validation:</p>
<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb703-1"><a href="supervised-learning.html#cb703-1" aria-hidden="true" tabindex="-1"></a><span class="co"># run CV</span></span>
<span id="cb703-2"><a href="supervised-learning.html#cb703-2" aria-hidden="true" tabindex="-1"></a>lr_fit <span class="ot">&lt;-</span> <span class="fu">train</span>(<span class="do">## formula and dataset definition</span></span>
<span id="cb703-3"><a href="supervised-learning.html#cb703-3" aria-hidden="true" tabindex="-1"></a>                sex<span class="sc">~</span>., </span>
<span id="cb703-4"><a href="supervised-learning.html#cb703-4" aria-hidden="true" tabindex="-1"></a>                <span class="at">data =</span> heights_dt,</span>
<span id="cb703-5"><a href="supervised-learning.html#cb703-5" aria-hidden="true" tabindex="-1"></a>                <span class="do">## model specification</span></span>
<span id="cb703-6"><a href="supervised-learning.html#cb703-6" aria-hidden="true" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;glm&quot;</span>,</span>
<span id="cb703-7"><a href="supervised-learning.html#cb703-7" aria-hidden="true" tabindex="-1"></a>                <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>,</span>
<span id="cb703-8"><a href="supervised-learning.html#cb703-8" aria-hidden="true" tabindex="-1"></a>                <span class="do">## validation specification</span></span>
<span id="cb703-9"><a href="supervised-learning.html#cb703-9" aria-hidden="true" tabindex="-1"></a>                <span class="at">trControl =</span> fitControl,</span>
<span id="cb703-10"><a href="supervised-learning.html#cb703-10" aria-hidden="true" tabindex="-1"></a>                <span class="do">## Specify which metric to optimize</span></span>
<span id="cb703-11"><a href="supervised-learning.html#cb703-11" aria-hidden="true" tabindex="-1"></a>                <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb703-12"><a href="supervised-learning.html#cb703-12" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb703-13"><a href="supervised-learning.html#cb703-13" aria-hidden="true" tabindex="-1"></a>lr_fit</span></code></pre></div>
<pre><code>## Generalized Linear Model 
## 
## 253 samples
##   3 predictor
##   2 classes: &#39;F&#39;, &#39;M&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 203, 202, 202, 203, 202 
## Resampling results:
## 
##   ROC        Sens   Spec     
##   0.9811125  0.911  0.9310541</code></pre>
<p>As we can observe, the <code>lr_fit</code> object returns the average values for the computed values of sensitivity, specificity and the area under the ROC curve of the <code>k=5</code> trained models. The performance measurements for each model can be obtained as follows:</p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="supervised-learning.html#cb705-1" aria-hidden="true" tabindex="-1"></a>lr_fit<span class="sc">$</span>resample</span></code></pre></div>
<pre><code>##         ROC      Sens      Spec Resample
## 1 0.9967949 1.0000000 0.9615385    Fold1
## 2 0.9830769 0.9200000 0.9230769    Fold2
## 3 0.9953704 0.9583333 0.9629630    Fold3
## 4 0.9887821 0.9166667 0.9615385    Fold4
## 5 0.9415385 0.7600000 0.8461538    Fold5</code></pre>
<p>Additionally, the final model can be obtained by accessing the attribute <code>finalModel</code> from the <code>lr_fit</code> object:</p>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="supervised-learning.html#cb707-1" aria-hidden="true" tabindex="-1"></a>lr_fit<span class="sc">$</span>finalModel</span></code></pre></div>
<pre><code>## 
## Call:  NULL
## 
## Coefficients:
## (Intercept)       height       mother       father  
##    -37.4913       0.7307      -0.2899      -0.2360  
## 
## Degrees of Freedom: 252 Total (i.e. Null);  249 Residual
## Null Deviance:       350.4 
## Residual Deviance: 83.05     AIC: 91.05</code></pre>
</div>
</div>
</div>
<div id="random-forests-as-alternative-models" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> Random Forests as alternative models</h2>
<p>There is a plethora of supervised learning algorithms out there. Providing a reasonable survey of them is out of the scope of this module. We therefore introduce here a single supervised learning method beyond linear and logistic regression which is working well in many situations: Random forests. Just as linear and logistic regression, should be the first baseline models to try, random forests can be seen as the first non-interpretable supervised learning model to give a shoot for. From this basis, many other options can be considered, often depending on the application areas.</p>
<p>Random forests can be applied to both regression and classification tasks in supervised learning. In this section, we introduce random forests as a further type of model for solving both tasks, as these are easy to understand and achieve state-of-the-art performance in many prediction tasks.</p>
<p>Random forests are based on decision trees. More precisely, they are an instance of tree-based ensemble learning, which is a strategy robust to over-fitting and allows fitting very flexible functions. Each random forest is an ensemble of several decision trees, which are machine learning models for both classification and regression tasks and will be introduced in the following section.</p>
<div id="the-basics-of-decision-trees" class="section level3" number="12.4.1">
<h3><span class="header-section-number">12.4.1</span> The basics of decision trees</h3>
<p>A decision tree is a method that involves partitioning or segmenting the training data set into a number of simple regions. Typically, once the input space of the training dataset has been segmented, we can make a prediction for a new input sample by using the mean or the mode of the training observations in the region to which the new input sample is assigned to.</p>
<div id="decision-trees-for-regression-tasks" class="section level4" number="12.4.1.1">
<h4><span class="header-section-number">12.4.1.1</span> Decision trees for regression tasks</h4>
<p>As an example for a decision tree model for a regression task, we consider the <code>Hitters</code> dataset<a href="#fn49" class="footnote-ref" id="fnref49"><sup>49</sup></a> to predict a baseball player’s (log-transformed) <code>Salary</code> based on the feature <code>Years</code> (the number of years that he has played in the major leagues) and on the feature <code>Hits</code> (the number of hits that he made in the previous year). A regression tree for this dataset is shown in Figure <a href="supervised-learning.html#fig:dt1">12.7</a> The constructed tree consist of 3 splitting rules. First, observations having <code>Years&lt;4.5</code> get passed to the right branch and observations having <code>Years&gt;=4.5</code> to the left branch. The predicted log-transformed salary for these players in the right is given by the mean log-transformed salary of all training samples in this node, which is in this case <code>5.11</code>. Players assigned to the left branch are further subdivided by the feature <code>Hits</code>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dt1"></span>
<img src="assets/img/topic12_supervised_learning/dt_reg.png" alt="Illustration of a decision tree for predicting the logarithmic salary of a baseball player based on the features Years and Hits." width="300px" />
<p class="caption">
Figure 12.7: Illustration of a decision tree for predicting the logarithmic salary of a baseball player based on the features Years and Hits.
</p>
</div>
<p>After the training is finalized, the tree regions the players into three regions: players who have played for four or fewer years, players who have played for five or more years and who made fewer than 118 hits last year, and players who have played for five or more years and who made at least 118 hits last year. A graphical representation of this resulting segmentation can be observed in Figure <a href="supervised-learning.html#fig:dt2">12.8</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dt2"></span>
<img src="dataviz_book_files/figure-html/dt2-1.png" alt="Illustration of the resulting three regions from the trained decision tree based on Hits and Years." width="384" />
<p class="caption">
Figure 12.8: Illustration of the resulting three regions from the trained decision tree based on Hits and Years.
</p>
</div>
<p>Formally, the three resulting regions can be written as:</p>
<p><span class="math display">\[
\begin{align}
R_1 &amp;=\{X | \text{Years}&lt;4.5\} \\
R_2 &amp;=\{X | \text{Years}&gt;=4.5,\text{Hits}&lt;117.5\}\\
R_3 &amp;=\{X | \text{Years}&gt;=4.5, \text{Hits}&gt;=117.5\}
\end{align}
\]</span></p>
<p>Here, <span class="math inline">\(X\)</span> is the dataset consisting of all training samples and the regions <span class="math inline">\(R_1\)</span>, <span class="math inline">\(R_2\)</span> and <span class="math inline">\(R_3\)</span> are the leaf nodes of the tree. All other nodes are denoted as internal nodes of the constructed decision tree.</p>
<p>Decision trees are easy to interpret. In our example, we can interpret the tree as follows:</p>
<ul>
<li><p><code>Years</code> is the most important feature for predicting the log-transformed salary of a player. Players with less experience earn lower salaries than more experienced players.</p></li>
<li><p>If a player is more experienced, the number of hits that he made in the previous year is important for determining his salary.</p></li>
</ul>
<p>Now… why and how did we choose the feature <code>Years</code> and the threshold of <code>4.5</code> for the top split? Overall, the goal of a decision tree for regression tasks is to find the optional regions <span class="math inline">\(R_1, R_2, ..., R_J\)</span> that minimize the residual residual sum of squares (RSS) given by:</p>
<p><span class="math display">\[
\sum_{j=1}^J \sum_{i\in R_j} (y_i - \hat{y}_{R_j})^2,
\]</span></p>
<p>where <span class="math inline">\(\hat{y}_{R_j}\)</span> is the prediction for all samples in region <span class="math inline">\(R_j\)</span>, which is computed from the mean of the outcome value for the training samples in region <span class="math inline">\(R_j\)</span><a href="#fn50" class="footnote-ref" id="fnref50"><sup>50</sup></a>.</p>
<p>Unfortunately, it is computationally unfeasible to consider every possible partition of the training dataset. This is why decision trees follow a top-down greedy approach to build the tree. The approach is top-down because it begins at the top of the tree with the complete dataset and then successively splits the dataset. Here, each split results into two new branches further down on the tree. Note that the approach is greedy because at each step of the process, the best split is made at that particular step.</p>
<p>In order to perform recursive binary splitting, we first select the feature <span class="math inline">\(X_j\)</span> and a threshold <span class="math inline">\(s\)</span> such that splitting the samples into the regions <span class="math inline">\(R_1(j,s) = {X|X_j &lt; s}\)</span> and <span class="math inline">\(R_2(j,s) = {X|X_j ≥ s}\)</span> results into the greatest possible reduction in RSS. For this, we consider all possible features <span class="math inline">\(X_1,...,X_p\)</span>, and all possible values threshold values <span class="math inline">\(s\)</span> for each feature. In greater detail, for any <span class="math inline">\(j\)</span> and <span class="math inline">\(s\)</span>, we want to minimize the equation:</p>
<p><span class="math display">\[
\sum_{i\in R_1(j,s)} (y_i - \hat{y}_{R_{1}})^2 + \sum_{i\in R_2(j,s)} (y_i - \hat{y}_{R_{2}})^2,
\]</span>
Note that finding optimal values for thresholds and features can be achieved quickly with the current advances in computer science assuming that the number of features is not too large.</p>
<p>The iterative process of finding features and thresholds that define splitting rules continues until a stopping criterion is met. This prevents to consider every possible partition of the training dataset. Stopping criteria include setting a minimum number of samples in a region or defining a maximum number of resulting regions.</p>
<p>Once the regions <span class="math inline">\(R_1,...,R_J\)</span> have been created, we can predict the outcome of any given sample (from the test set) by assigning this sample to a region and returning the the mean of the training observations in the region to which that (test) sample belongs.</p>
</div>
<div id="decision-trees-for-classification-tasks" class="section level4" number="12.4.1.2">
<h4><span class="header-section-number">12.4.1.2</span> Decision trees for classification tasks</h4>
<p>As stated before, decision trees can be applied for both regression and classification tasks. The construction process of classification decision trees is very similar to the one that we previously described for regression tasks. One difference is that for a regression tree, the predicted outcome for an observation is given by the mean response of the training observations that belong to the same leaf node. In contrast, for a classification tree, we predict that each observation belongs to the most commonly occurring class of training observations in the region to which it belongs.</p>
<p>Additionally, in contrast to regression scenarios, RSS cannot be used as a criterion for making the tree splits for classification tasks. Typically, cross-entropy is used to fit the model (See Section <a href="chap-log-reg.html#ML-CE">11.2</a>).</p>
<!-- A suitable alternative to RSS is the classification error rate $E$, which is simply the fraction of the training samples in a region that do not belong to the most common class in the region: -->
<!-- $$ -->
<!-- 1 - \max_k \hat{p}_{R_jk} -->
<!-- $$ -->
<!-- Here,  $\hat{p}_{jk}$ is the proportion of training samples in the region $R_j$ that belong to the class $k$. Alternatives to this classification error rate are the Gini Index $G$ and the cross entropy $CE$ which are more sensitive for tree-growing and are preferred in practice. These are defined by: -->
<!-- $$ -->
<!-- \begin{align} -->
<!-- G &= \sum_{k=1}^K \hat{p}_{R_jk} \cdot (1 - \hat{p}_{R_jk}) \\ -->
<!-- CE &= \sum_{k=1}^K \hat{p}_{R_jk} \cdot \log \hat{p}_{R_jk} -->
<!-- \end{align} -->
<!-- $$ -->
<p>As an example we consider the binary classification task of deciding whether a person should play tennis on a given day or not. We consider the simulated data for training a model. The data contains four feature variables (<code>"Outlook"</code>, <code>"Temperature"</code>, <code>"Humidity"</code> and <code>"Windy"</code>) and the outcome variable <code>"Play.Tennis"</code>.</p>
<div class="sourceCode" id="cb709"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb709-1"><a href="supervised-learning.html#cb709-1" aria-hidden="true" tabindex="-1"></a>play_tennis_dt</span></code></pre></div>
<pre><code>##     Day Outlook Temperature Humidity Windy Play.Tennis
##  1:   1   Sunny        15.5   Normal   Yes          No
##  2:   2   Rainy        24.5     High   Yes          No
##  3:   3   Rainy        24.0   Normal    No         Yes
##  4:   4   Rainy        15.0   Normal   Yes         Yes
##  5:   5   Rainy        27.0   Normal   Yes         Yes
##  6:   6   Rainy        27.5   Normal    No         Yes
##  7:   7   Sunny        16.0     High    No         Yes
##  8:   8   Sunny        29.5   Normal   Yes         Yes
##  9:   9   Sunny        17.0     High    No         Yes
## 10:  10   Rainy        20.5   Normal   Yes         Yes</code></pre>
<p>Figure <a href="supervised-learning.html#fig:dt3">12.9</a> shows an example of a decision tree for solving the described classification task.The initial splitting rule tests whether the value of the <code>Outlook</code> feature of each sample is either sunny or rainy. Samples with the feature value being sunny are passed to the right node. If the outlook is rainy for a subset of the initial samples in the dataset, these samples are passed to the left node. Here, a second feature test is performed. This test involves checking the humidity feature for all samples. If the humidity value is normal, a third splitting rule is defined for checking whether the day is windy or not. In total, four regions results from the training process of the decision tree.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dt3"></span>
<img src="assets/img/topic12_supervised_learning/dt_class.png" alt="Example of a decision tree for the binary classification task of deciding whether to play tennis or not based on the features Outlook, Humidity and Windy." width="300px" />
<p class="caption">
Figure 12.9: Example of a decision tree for the binary classification task of deciding whether to play tennis or not based on the features Outlook, Humidity and Windy.
</p>
</div>
<p>Note that decision trees for classification tasks can be constructed with both numerical and categorical features, even though our tennis example only contains categorical data. Feature tests for numerical features can be formulated with an (un)equality for a certain feature, as described before for regression trees. For instance, a node in a decision tree could test whether the temperature feature is higher than 20 degrees Celsius (<code>Temperature&gt;20</code>).</p>
<!-- #### Decision trees in R -->
<!-- In R, we can use the `rpart` function from the `rpart` package to train decision trees (for both classification and regression problems). -->
<!-- ```{r} -->
<!-- #install.packages("rpart") -->
<!-- library(rpart) -->
<!-- dt_classifier <- rpart(Play.Tennis ~ ., data = play_tennis_dt) -->
<!-- dt_classifier -->
<!-- ``` -->
<!-- We can plot the resulting decision tree as follows: -->
<!-- ```{r} -->
<!-- plot(dt_classifier, margin = 0.1) -->
<!-- text(dt_classifier, cex = 0.75) -->
<!-- ``` -->
</div>
</div>
<div id="random-forests-for-classification-and-regression-tasks" class="section level3" number="12.4.2">
<h3><span class="header-section-number">12.4.2</span> Random Forests for classification and regression tasks</h3>
<p>For constructing a random forest, the first step is called aggregation (or bagging), which involves generating many predictors, each using regression or classification trees. To ensure that the individual trees are not the same, we use bootstrap to induce randomness. These two features combined explain the name: the bootstrap makes the individual trees randomly different, and the combination of trees is the forest. In a random forest for classification tasks, the predicted output is the majority vote of the contained decision trees. In contrast, the predicted value of a random forest regressor is the mean of the predictions from each decision tree in the random forest.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf"></span>
<img src="assets/img/topic12_supervised_learning/rf_wiki.png" alt="Illustration of a random forest for classification tasks. Source: By Venkata Jagannath - https://community.tibco.com/wiki/random-forest-template-tibco-spotfirer-wiki-page, CC BY-SA 4.0, https://commons.wikimedia.org/w/index.php?curid=68995764" width="500px" />
<p class="caption">
Figure 12.10: Illustration of a random forest for classification tasks. Source: By Venkata Jagannath - <a href="https://community.tibco.com/wiki/random-forest-template-tibco-spotfirer-wiki-page" class="uri">https://community.tibco.com/wiki/random-forest-template-tibco-spotfirer-wiki-page</a>, CC BY-SA 4.0, <a href="https://commons.wikimedia.org/w/index.php?curid=68995764" class="uri">https://commons.wikimedia.org/w/index.php?curid=68995764</a>
</p>
</div>
<p>The specific steps for building a random forest can be formulated as follows:</p>
<ol style="list-style-type: decimal">
<li><p>Build <span class="math inline">\(B\)</span> decision trees using the training set. We refer to the fitted models as <span class="math inline">\(T_1,T_2,...,T_B\)</span>.</p></li>
<li><p>For every observation in the test set, form a prediction <span class="math inline">\(\hat{y}_j\)</span> using tree <span class="math inline">\(T_j\)</span></p></li>
<li><p>For continuous outcomes, form a final prediction with the average <span class="math inline">\(\hat{y} =\frac{1}{B}\sum_{j=1}^B \hat{y}_j\)</span>. For categorical outcomes, predict <span class="math inline">\(\hat{y}\)</span> with majority vote (most frequent class among <span class="math inline">\(\hat{y}_1, ..., \hat{y}_B\)</span>.</p></li>
</ol>
<p>Now… how do we assure that the decision trees are different in the random forest, even if they are trained on the same training dataset? For this, we use randomness in two ways. Let <span class="math inline">\(N\)</span> be the number of observations in the training set. To create the trees <span class="math inline">\(T_1,T_2,...,T_B\)</span> from the training set we do the following with the feature and sample selection:</p>
<ol style="list-style-type: decimal">
<li><p>Create a bootstrap training set by sampling <span class="math inline">\(N\)</span> observations from the training set with replacement. In this manner, each tree is trained on a different dataset.</p></li>
<li><p>Randomly select a subset of the features to be included in the building of each tree so that not all features are considered for each tree. A different random subset is selected for each tree. This reduces correlation between trees in the forest and prevents over-fitting.</p></li>
</ol>
</div>
<div id="random-forests-in-r" class="section level3" number="12.4.3">
<h3><span class="header-section-number">12.4.3</span> Random Forests in R</h3>
<p>We use function <code>randomForest</code> from the package <code>randomForest</code> for training random forests in R.</p>
<div class="sourceCode" id="cb711"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb711-1"><a href="supervised-learning.html#cb711-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;randomForest&quot;)</span></span>
<span id="cb711-2"><a href="supervised-learning.html#cb711-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span></code></pre></div>
<p>As an example, we train a random forest classifier for predicting the gender of each person in the <code>heights_dt</code> data table given the height values of the person and the person’s parents as before. We define the values for a few hyper parameters such as the number of trees in the random forest (<code>ntree</code>), the minimum number of samples in a leaf node (<code>nodesize</code>) and the maximum number of leaf nodes in a tree (<code>maxnodes</code>).</p>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="supervised-learning.html#cb712-1" aria-hidden="true" tabindex="-1"></a>rf_classifier <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(<span class="do">## Define formula and data</span></span>
<span id="cb712-2"><a href="supervised-learning.html#cb712-2" aria-hidden="true" tabindex="-1"></a>                              sex<span class="sc">~</span>.,</span>
<span id="cb712-3"><a href="supervised-learning.html#cb712-3" aria-hidden="true" tabindex="-1"></a>                              <span class="at">data=</span>heights_dt,</span>
<span id="cb712-4"><a href="supervised-learning.html#cb712-4" aria-hidden="true" tabindex="-1"></a>                              </span>
<span id="cb712-5"><a href="supervised-learning.html#cb712-5" aria-hidden="true" tabindex="-1"></a>                              <span class="do">## Hyper parameters</span></span>
<span id="cb712-6"><a href="supervised-learning.html#cb712-6" aria-hidden="true" tabindex="-1"></a>                              <span class="at">ntree=</span><span class="dv">100</span>,     <span class="co"># Define number of trees</span></span>
<span id="cb712-7"><a href="supervised-learning.html#cb712-7" aria-hidden="true" tabindex="-1"></a>                              <span class="at">nodesize =</span> <span class="dv">5</span>,  <span class="co"># Minimum size of leaf nodes</span></span>
<span id="cb712-8"><a href="supervised-learning.html#cb712-8" aria-hidden="true" tabindex="-1"></a>                              <span class="at">maxnodes =</span> <span class="dv">30</span> <span class="co"># Maximum number of leaf nodes</span></span>
<span id="cb712-9"><a href="supervised-learning.html#cb712-9" aria-hidden="true" tabindex="-1"></a>                              )</span>
<span id="cb712-10"><a href="supervised-learning.html#cb712-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb712-11"><a href="supervised-learning.html#cb712-11" aria-hidden="true" tabindex="-1"></a>rf_classifier</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = sex ~ ., data = heights_dt, ntree = 100,      nodesize = 5, maxnodes = 30) 
##                Type of random forest: classification
##                      Number of trees: 100
## No. of variables tried at each split: 1
## 
##         OOB estimate of  error rate: 10.28%
## Confusion matrix:
##     F   M class.error
## F 109  13  0.10655738
## M  13 118  0.09923664</code></pre>
<p>We remark that the number of threes (<code>ntree</code>), the minimum size of leaf nodes (<code>nodesize</code>) and the maximum amount of leaf nodes (<code>maxnodes</code>) in each tree are hyper parameters of random forests. These should be optimized (or tuned) for each application to achieve a better performance of the models. Two popular approaches are grid search and random reach. Hyper-parameter tuning is out the scope of this lecture. We refer the interested reader to the practical methodology chapter of the Deep learning book <span class="citation">(<a href="#ref-Goodfellow-et-al-2016" role="doc-biblioref">Goodfellow, Bengio, and Courville 2016</a>)</span>.</p>
<p>For predicting the gender of an input sample using the trained model stored in the variable <code>rf_classifier</code>, we can use the function <code>predict</code> as follows:</p>
<div class="sourceCode" id="cb714"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb714-1"><a href="supervised-learning.html#cb714-1" aria-hidden="true" tabindex="-1"></a>heights_dt[, sex_predicted<span class="sc">:</span><span class="er">=</span> <span class="fu">predict</span>(rf_classifier,</span>
<span id="cb714-2"><a href="supervised-learning.html#cb714-2" aria-hidden="true" tabindex="-1"></a>                                     heights_dt[,<span class="sc">-</span><span class="fu">c</span>(<span class="st">&quot;sex&quot;</span>)])]</span>
<span id="cb714-3"><a href="supervised-learning.html#cb714-3" aria-hidden="true" tabindex="-1"></a>heights_dt</span></code></pre></div>
<pre><code>##      height sex mother father sex_predicted
##   1:    197   M    175    191             M
##   2:    196   M    163    192             M
##   3:    195   M    171    185             M
##   4:    195   M    168    191             M
##   5:    194   M    164    189             M
##  ---                                       
## 249:    152   F    150    165             F
## 250:    172   F    165    188             F
## 251:    154   F    155    165             F
## 252:    178   M    169    174             M
## 253:    175   F    171    189             F</code></pre>
</div>
</div>
<div id="conclusion-2" class="section level2" number="12.5">
<h2><span class="header-section-number">12.5</span> Conclusion</h2>
<p>This chapter introduced concepts from supervised machine learning for prediction tasks. Here, the main focus is to build and evaluate models that identify trends in the training dataset but are still able to generalize to (new) independent datasets. We described cross-validation as a strategy to prevent over-fitting.</p>
<p>Additionally, we introduced random forests as alternative machine learning models to linear and logistic regression for regression and classification tasks, since they are easy to understand and interpret and still show good performance in many application tasks. However, there are many more algorithms for solving regression and/or classification tasks in supervised learning. A few of the most popular methods include support vector machines (SVMs), (Deep) Neural Networks, k-Nearest Neighbors and Naive Bayes classifiers.</p>
</div>
<div id="resources-4" class="section level2" number="12.6">
<h2><span class="header-section-number">12.6</span> Resources</h2>
<ul>
<li><p>Christopher M. Bishop. 2006. Pattern Recognition and Machine Learning (Information Science and Statistics). Springer-Verlag, Berlin, Heidelberg.</p></li>
<li><p>Rafael A. Irizarry. 2019. Introduction to Data Science. Data Analysis and Prediction Algorithms with R .</p></li>
<li><p>Gareth James, Daniela Witten, Trevor Hastie &amp; Robert Tibshirani. 2013. An Introduction to Statistical Learning with Applications in R.</p></li>
<li><p>The caret package manual:
<a href="https://topepo.github.io/caret/" class="uri">https://topepo.github.io/caret/</a></p></li>
<li><p>The Datacamp machine learning courses:
<a href="https://www.datacamp.com/courses/machine-learning-toolbox" class="uri">https://www.datacamp.com/courses/machine-learning-toolbox</a></p></li>
</ul>

</div>
</div>



<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Goodfellow-et-al-2016" class="csl-entry">
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. <em>Deep Learning</em>. MIT Press.
</div>
<div id="ref-Roberts2017" class="csl-entry">
Roberts, David R., Volker Bahn, Simone Ciuti, Mark S. Boyce, Jane Elith, Gurutzeta Guillera-Arroita, Severin Hauenstein, et al. 2017. <span>“Cross-Validation Strategies for Data with Temporal, Spatial, Hierarchical, or Phylogenetic Structure.”</span> <em>Ecography</em> 40 (8): 913–29. https://doi.org/<a href="https://doi.org/10.1111/ecog.02881">https://doi.org/10.1111/ecog.02881</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="48">
<li id="fn48"><p><a href="https://en.wikipedia.org/wiki/Machine_learning" class="uri">https://en.wikipedia.org/wiki/Machine_learning</a><a href="supervised-learning.html#fnref48" class="footnote-back">↩︎</a></p></li>
<li id="fn49"><p><a href="https://rdrr.io/cran/ISLR/man/Hitters.html" class="uri">https://rdrr.io/cran/ISLR/man/Hitters.html</a><a href="supervised-learning.html#fnref49" class="footnote-back">↩︎</a></p></li>
<li id="fn50"><p>Show that sample mean minimizes the least square errors to a set of observations.<a href="supervised-learning.html#fnref50" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chap-log-reg.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="importing-data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
