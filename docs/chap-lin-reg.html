<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Linear Regression | Data Analysis and Visualization in R (IN2339)</title>
  <meta name="description" content="TODO This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Linear Regression | Data Analysis and Visualization in R (IN2339)" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="TODO This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Linear Regression | Data Analysis and Visualization in R (IN2339)" />
  
  <meta name="twitter:description" content="TODO This book introduces concepts and skills that can help you tackle real-world data analysis challenges. It covers concepts from probability, statistical inference, linear regression and machine learning and helps you develop skills such as R programming, data wrangling with dplyr, data visualization with ggplot2, file organization with UNIX/Linux shell, version control with GitHub, and reproducible document preparation with R markdown." />
  

<meta name="author" content="Chair of Computational Molecular Medicine" />


<meta name="date" content="2021-03-15" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="big-data-stat.html"/>
<link rel="next" href="chap-log-reg.html"/>
<script src="libs/header-attrs-2.5/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis and Visualization in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i>Prerequisites</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a>
<ul>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#data-science-what-and-why"><i class="fa fa-check"></i>Data Science: What and why?</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#what-you-will-learn-and-not-learn"><i class="fa fa-check"></i>What you will learn and not learn</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#the-r-language"><i class="fa fa-check"></i>The R language</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#course-overview"><i class="fa fa-check"></i>Course overview</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html#complementary-reading"><i class="fa fa-check"></i>Complementary reading</a></li>
</ul></li>
<li class="part"><span><b>I Get</b></span></li>
<li class="chapter" data-level="1" data-path="r-basics.html"><a href="r-basics.html"><i class="fa fa-check"></i><b>1</b> R basics</a>
<ul>
<li class="chapter" data-level="1.1" data-path="r-basics.html"><a href="r-basics.html#rstudio"><i class="fa fa-check"></i><b>1.1</b> Rstudio</a></li>
<li class="chapter" data-level="1.2" data-path="r-basics.html"><a href="r-basics.html#first-steps-with-r"><i class="fa fa-check"></i><b>1.2</b> First steps with R</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="r-basics.html"><a href="r-basics.html#objects"><i class="fa fa-check"></i><b>1.2.1</b> Objects</a></li>
<li class="chapter" data-level="1.2.2" data-path="r-basics.html"><a href="r-basics.html#the-workspace"><i class="fa fa-check"></i><b>1.2.2</b> The workspace</a></li>
<li class="chapter" data-level="1.2.3" data-path="r-basics.html"><a href="r-basics.html#functions"><i class="fa fa-check"></i><b>1.2.3</b> Functions</a></li>
<li class="chapter" data-level="1.2.4" data-path="r-basics.html"><a href="r-basics.html#other-prebuilt-objects"><i class="fa fa-check"></i><b>1.2.4</b> Other prebuilt objects</a></li>
<li class="chapter" data-level="1.2.5" data-path="r-basics.html"><a href="r-basics.html#variable-names"><i class="fa fa-check"></i><b>1.2.5</b> Variable names</a></li>
<li class="chapter" data-level="1.2.6" data-path="r-basics.html"><a href="r-basics.html#reusing-scripts"><i class="fa fa-check"></i><b>1.2.6</b> Reusing scripts</a></li>
<li class="chapter" data-level="1.2.7" data-path="r-basics.html"><a href="r-basics.html#commenting-your-code"><i class="fa fa-check"></i><b>1.2.7</b> Commenting your code</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="r-basics.html"><a href="r-basics.html#data-types"><i class="fa fa-check"></i><b>1.3</b> Data types</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="r-basics.html"><a href="r-basics.html#data-frames"><i class="fa fa-check"></i><b>1.3.1</b> Data frames</a></li>
<li class="chapter" data-level="1.3.2" data-path="r-basics.html"><a href="r-basics.html#examining-an-object"><i class="fa fa-check"></i><b>1.3.2</b> Examining an object</a></li>
<li class="chapter" data-level="1.3.3" data-path="r-basics.html"><a href="r-basics.html#the-accessor"><i class="fa fa-check"></i><b>1.3.3</b> The accessor: <code>$</code></a></li>
<li class="chapter" data-level="1.3.4" data-path="r-basics.html"><a href="r-basics.html#vectors-numerics-characters-and-logical"><i class="fa fa-check"></i><b>1.3.4</b> Vectors: numerics, characters, and logical</a></li>
<li class="chapter" data-level="1.3.5" data-path="r-basics.html"><a href="r-basics.html#factors"><i class="fa fa-check"></i><b>1.3.5</b> Factors</a></li>
<li class="chapter" data-level="1.3.6" data-path="r-basics.html"><a href="r-basics.html#lists"><i class="fa fa-check"></i><b>1.3.6</b> Lists</a></li>
<li class="chapter" data-level="1.3.7" data-path="r-basics.html"><a href="r-basics.html#matrices"><i class="fa fa-check"></i><b>1.3.7</b> Matrices</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="r-basics.html"><a href="r-basics.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="r-basics.html"><a href="r-basics.html#creating-vectors"><i class="fa fa-check"></i><b>1.4.1</b> Creating vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="r-basics.html"><a href="r-basics.html#names"><i class="fa fa-check"></i><b>1.4.2</b> Names</a></li>
<li class="chapter" data-level="1.4.3" data-path="r-basics.html"><a href="r-basics.html#sequences"><i class="fa fa-check"></i><b>1.4.3</b> Sequences</a></li>
<li class="chapter" data-level="1.4.4" data-path="r-basics.html"><a href="r-basics.html#subsetting"><i class="fa fa-check"></i><b>1.4.4</b> Subsetting</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="r-basics.html"><a href="r-basics.html#coercion"><i class="fa fa-check"></i><b>1.5</b> Coercion</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="r-basics.html"><a href="r-basics.html#not-availables-na"><i class="fa fa-check"></i><b>1.5.1</b> Not availables (NA)</a></li>
</ul></li>
<li class="chapter" data-level="1.6" data-path="r-basics.html"><a href="r-basics.html#sorting"><i class="fa fa-check"></i><b>1.6</b> Sorting</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="r-basics.html"><a href="r-basics.html#sort"><i class="fa fa-check"></i><b>1.6.1</b> <code>sort</code></a></li>
<li class="chapter" data-level="1.6.2" data-path="r-basics.html"><a href="r-basics.html#order"><i class="fa fa-check"></i><b>1.6.2</b> <code>order</code></a></li>
<li class="chapter" data-level="1.6.3" data-path="r-basics.html"><a href="r-basics.html#max-and-which.max"><i class="fa fa-check"></i><b>1.6.3</b> <code>max</code> and <code>which.max</code></a></li>
<li class="chapter" data-level="1.6.4" data-path="r-basics.html"><a href="r-basics.html#rank"><i class="fa fa-check"></i><b>1.6.4</b> <code>rank</code></a></li>
<li class="chapter" data-level="1.6.5" data-path="r-basics.html"><a href="r-basics.html#beware-of-recycling"><i class="fa fa-check"></i><b>1.6.5</b> Beware of recycling</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="r-basics.html"><a href="r-basics.html#vector-arithmetics"><i class="fa fa-check"></i><b>1.7</b> Vector arithmetics</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="r-basics.html"><a href="r-basics.html#rescaling-a-vector"><i class="fa fa-check"></i><b>1.7.1</b> Rescaling a vector</a></li>
<li class="chapter" data-level="1.7.2" data-path="r-basics.html"><a href="r-basics.html#two-vectors"><i class="fa fa-check"></i><b>1.7.2</b> Two vectors</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="r-basics.html"><a href="r-basics.html#indexing"><i class="fa fa-check"></i><b>1.8</b> Indexing</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="r-basics.html"><a href="r-basics.html#subsetting-with-logicals"><i class="fa fa-check"></i><b>1.8.1</b> Subsetting with logicals</a></li>
<li class="chapter" data-level="1.8.2" data-path="r-basics.html"><a href="r-basics.html#logical-operators"><i class="fa fa-check"></i><b>1.8.2</b> Logical operators</a></li>
<li class="chapter" data-level="1.8.3" data-path="r-basics.html"><a href="r-basics.html#which"><i class="fa fa-check"></i><b>1.8.3</b> <code>which</code></a></li>
<li class="chapter" data-level="1.8.4" data-path="r-basics.html"><a href="r-basics.html#match"><i class="fa fa-check"></i><b>1.8.4</b> <code>match</code></a></li>
<li class="chapter" data-level="1.8.5" data-path="r-basics.html"><a href="r-basics.html#in"><i class="fa fa-check"></i><b>1.8.5</b> <code>%in%</code></a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="r-basics.html"><a href="r-basics.html#r-programming"><i class="fa fa-check"></i><b>1.9</b> R programming</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="data-wrangling.html"><a href="data-wrangling.html"><i class="fa fa-check"></i><b>2</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="2.1" data-path="data-wrangling.html"><a href="data-wrangling.html#data.tables"><i class="fa fa-check"></i><b>2.1</b> Data.tables</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="data-wrangling.html"><a href="data-wrangling.html#overview"><i class="fa fa-check"></i><b>2.1.1</b> Overview</a></li>
<li class="chapter" data-level="2.1.2" data-path="data-wrangling.html"><a href="data-wrangling.html#creating-and-loading-tables"><i class="fa fa-check"></i><b>2.1.2</b> Creating and loading tables</a></li>
<li class="chapter" data-level="2.1.3" data-path="data-wrangling.html"><a href="data-wrangling.html#inspecting-tables"><i class="fa fa-check"></i><b>2.1.3</b> Inspecting tables</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="data-wrangling.html"><a href="data-wrangling.html#row-subsetting"><i class="fa fa-check"></i><b>2.2</b> Row subsetting</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="data-wrangling.html"><a href="data-wrangling.html#subsetting-rows-by-indices"><i class="fa fa-check"></i><b>2.2.1</b> Subsetting rows by indices</a></li>
<li class="chapter" data-level="2.2.2" data-path="data-wrangling.html"><a href="data-wrangling.html#subsetting-rows-by-logical-conditions"><i class="fa fa-check"></i><b>2.2.2</b> Subsetting rows by logical conditions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="data-wrangling.html"><a href="data-wrangling.html#column-operations"><i class="fa fa-check"></i><b>2.3</b> Column operations</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="data-wrangling.html"><a href="data-wrangling.html#working-with-columns"><i class="fa fa-check"></i><b>2.3.1</b> Working with columns</a></li>
<li class="chapter" data-level="2.3.2" data-path="data-wrangling.html"><a href="data-wrangling.html#column-operations-1"><i class="fa fa-check"></i><b>2.3.2</b> Column operations</a></li>
<li class="chapter" data-level="2.3.3" data-path="data-wrangling.html"><a href="data-wrangling.html#advanced-commands-apply-over-columns"><i class="fa fa-check"></i><b>2.3.3</b> Advanced commands: *apply() over columns</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="data-wrangling.html"><a href="data-wrangling.html#the-by-option"><i class="fa fa-check"></i><b>2.4</b> The ‘by’ option</a></li>
<li class="chapter" data-level="2.5" data-path="data-wrangling.html"><a href="data-wrangling.html#counting-occurences-with-.n"><i class="fa fa-check"></i><b>2.5</b> Counting occurences with <code>.N</code></a></li>
<li class="chapter" data-level="2.6" data-path="data-wrangling.html"><a href="data-wrangling.html#extending-tables"><i class="fa fa-check"></i><b>2.6</b> Extending tables</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="data-wrangling.html"><a href="data-wrangling.html#creating-new-columns-the-command"><i class="fa fa-check"></i><b>2.6.1</b> Creating new columns (the := command)</a></li>
<li class="chapter" data-level="2.6.2" data-path="data-wrangling.html"><a href="data-wrangling.html#advanced-multiple-assignments"><i class="fa fa-check"></i><b>2.6.2</b> Advanced: Multiple assignments</a></li>
<li class="chapter" data-level="2.6.3" data-path="data-wrangling.html"><a href="data-wrangling.html#copying-tables"><i class="fa fa-check"></i><b>2.6.3</b> Copying tables</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="data-wrangling.html"><a href="data-wrangling.html#summary"><i class="fa fa-check"></i><b>2.7</b> Summary</a></li>
<li class="chapter" data-level="2.8" data-path="data-wrangling.html"><a href="data-wrangling.html#data.table-resources"><i class="fa fa-check"></i><b>2.8</b> Data.table resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html"><i class="fa fa-check"></i><b>3</b> Tidy data and combining tables</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#motivation"><i class="fa fa-check"></i><b>3.1.1</b> Motivation</a></li>
<li class="chapter" data-level="3.1.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#datasets-used-in-this-chapter"><i class="fa fa-check"></i><b>3.1.2</b> Datasets used in this chapter</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#tidy-and-untidy-data"><i class="fa fa-check"></i><b>3.2</b> Tidy and untidy data</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#definition-of-tidy-data"><i class="fa fa-check"></i><b>3.2.1</b> Definition of tidy data</a></li>
<li class="chapter" data-level="3.2.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#advantages-of-tidy-data"><i class="fa fa-check"></i><b>3.2.2</b> Advantages of tidy data</a></li>
<li class="chapter" data-level="3.2.3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#common-signs-of-untidy-datasets"><i class="fa fa-check"></i><b>3.2.3</b> Common signs of untidy datasets</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#tidying-up-datasets"><i class="fa fa-check"></i><b>3.3</b> Tidying up datasets</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#melting-wide-to-long"><i class="fa fa-check"></i><b>3.3.1</b> Melting (wide to long)</a></li>
<li class="chapter" data-level="3.3.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#casting-long-to-wide"><i class="fa fa-check"></i><b>3.3.2</b> Casting (long to wide)</a></li>
<li class="chapter" data-level="3.3.3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#separating-columns"><i class="fa fa-check"></i><b>3.3.3</b> Separating columns</a></li>
<li class="chapter" data-level="3.3.4" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#uniting-columns"><i class="fa fa-check"></i><b>3.3.4</b> Uniting columns</a></li>
<li class="chapter" data-level="3.3.5" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#advanced-columns-containing-sets-of-values"><i class="fa fa-check"></i><b>3.3.5</b> Advanced: Columns containing sets of values</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#concatenating-tables"><i class="fa fa-check"></i><b>3.4</b> Concatenating tables</a></li>
<li class="chapter" data-level="3.5" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#merging-tables"><i class="fa fa-check"></i><b>3.5</b> Merging tables</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#inner-merge"><i class="fa fa-check"></i><b>3.5.1</b> Inner merge</a></li>
<li class="chapter" data-level="3.5.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#outer-full-merge"><i class="fa fa-check"></i><b>3.5.2</b> Outer (full) merge</a></li>
<li class="chapter" data-level="3.5.3" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#left-merge"><i class="fa fa-check"></i><b>3.5.3</b> Left merge</a></li>
<li class="chapter" data-level="3.5.4" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#right-merge"><i class="fa fa-check"></i><b>3.5.4</b> Right merge</a></li>
<li class="chapter" data-level="3.5.5" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#merging-by-several-columns"><i class="fa fa-check"></i><b>3.5.5</b> Merging by several columns</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#tidy-not-unique"><i class="fa fa-check"></i><b>3.6</b> Tidy representations are not unique</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#alternative-tidy-forms-of-a-table"><i class="fa fa-check"></i><b>3.6.1</b> Alternative tidy forms of a table</a></li>
<li class="chapter" data-level="3.6.2" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#on-multiple-types-of-observational-units-in-the-same-table"><i class="fa fa-check"></i><b>3.6.2</b> On multiple types of observational units in the same table</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#summary-1"><i class="fa fa-check"></i><b>3.7</b> Summary</a></li>
<li class="chapter" data-level="3.8" data-path="tidy-data-and-combining-tables.html"><a href="tidy-data-and-combining-tables.html#tidy-data-resources"><i class="fa fa-check"></i><b>3.8</b> Tidy data resources</a></li>
</ul></li>
<li class="part"><span><b>II Look</b></span></li>
<li class="chapter" data-level="4" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html"><i class="fa fa-check"></i><b>4</b> Low dimensional visualizations</a>
<ul>
<li class="chapter" data-level="4.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#why-plotting"><i class="fa fa-check"></i><b>4.1</b> Why plotting?</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plot-vs-stat"><i class="fa fa-check"></i><b>4.1.1</b> Plotting versus summary statistics</a></li>
<li class="chapter" data-level="4.1.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plot-debug"><i class="fa fa-check"></i><b>4.1.2</b> Plotting helps finding bugs in the data</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#grammar-of-graphics"><i class="fa fa-check"></i><b>4.2</b> Grammar of graphics</a></li>
<li class="chapter" data-level="4.3" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#components-of-the-layered-grammar"><i class="fa fa-check"></i><b>4.3</b> Components of the layered grammar</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#components-of-the-grammar-of-graphics"><i class="fa fa-check"></i><b>4.3.1</b> Components of the grammar of graphics</a></li>
<li class="chapter" data-level="4.3.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#defining-the-data-and-layers"><i class="fa fa-check"></i><b>4.3.2</b> Defining the data and layers</a></li>
<li class="chapter" data-level="4.3.3" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#mapping-of-aesthetics"><i class="fa fa-check"></i><b>4.3.3</b> Mapping of aesthetics</a></li>
<li class="chapter" data-level="4.3.4" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#facets-axes-and-labels"><i class="fa fa-check"></i><b>4.3.4</b> Facets, axes and labels</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#different-types-of-one--and-two-dimensional-plots"><i class="fa fa-check"></i><b>4.4</b> Different types of one- and two-dimensional plots</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plots-for-one-single-continuous-variable"><i class="fa fa-check"></i><b>4.4.1</b> Plots for one single continuous variable</a></li>
<li class="chapter" data-level="4.4.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plots-for-two-variables-one-continuous-one-discrete"><i class="fa fa-check"></i><b>4.4.2</b> Plots for two variables: one continuous, one discrete</a></li>
<li class="chapter" data-level="4.4.3" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plots-for-two-continuos-variables"><i class="fa fa-check"></i><b>4.4.3</b> Plots for two continuos variables</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#further-plots-for-low-dimensional-data"><i class="fa fa-check"></i><b>4.5</b> Further plots for low dimensional data</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#plot-matrix"><i class="fa fa-check"></i><b>4.5.1</b> Plot matrix</a></li>
<li class="chapter" data-level="4.5.2" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#correlation-plot"><i class="fa fa-check"></i><b>4.5.2</b> Correlation plot</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#summary-2"><i class="fa fa-check"></i><b>4.6</b> Summary</a></li>
<li class="chapter" data-level="4.7" data-path="low-dimensional-visualizations.html"><a href="low-dimensional-visualizations.html#resources"><i class="fa fa-check"></i><b>4.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html"><i class="fa fa-check"></i><b>5</b> High dimensional visualizations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#notations"><i class="fa fa-check"></i><b>5.1</b> Notations</a></li>
<li class="chapter" data-level="5.2" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#data-matrix-preparation"><i class="fa fa-check"></i><b>5.2</b> Data matrix preparation</a></li>
<li class="chapter" data-level="5.3" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#heatmaps"><i class="fa fa-check"></i><b>5.3</b> Heatmaps</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#centering-and-scaling-variables"><i class="fa fa-check"></i><b>5.3.1</b> Centering and scaling variables</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#clustering"><i class="fa fa-check"></i><b>5.4</b> Clustering</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#k-means-clustering"><i class="fa fa-check"></i><b>5.4.1</b> K-Means clustering</a></li>
<li class="chapter" data-level="5.4.2" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#hclust"><i class="fa fa-check"></i><b>5.4.2</b> Hierarchical clustering</a></li>
<li class="chapter" data-level="5.4.3" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#comparing-clusterings-with-the-rand-index"><i class="fa fa-check"></i><b>5.4.3</b> Comparing clusterings with the Rand index</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#dimensionality-reduction-with-pca"><i class="fa fa-check"></i><b>5.5</b> Dimensionality reduction with PCA</a>
<ul>
<li class="chapter" data-level="5.5.1" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#a-minimal-pca-from-2d-to-1d"><i class="fa fa-check"></i><b>5.5.1</b> A minimal PCA: From 2D to 1D</a></li>
<li class="chapter" data-level="5.5.2" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#pca-in-higher-dimensions"><i class="fa fa-check"></i><b>5.5.2</b> PCA in higher dimensions</a></li>
<li class="chapter" data-level="5.5.3" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#pca-in-r"><i class="fa fa-check"></i><b>5.5.3</b> PCA in R</a></li>
<li class="chapter" data-level="5.5.4" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#plotting-pca-results-in-r"><i class="fa fa-check"></i><b>5.5.4</b> Plotting PCA results in R</a></li>
<li class="chapter" data-level="5.5.5" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#pca-summary"><i class="fa fa-check"></i><b>5.5.5</b> PCA summary</a></li>
<li class="chapter" data-level="5.5.6" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#nonlinear-dimension-reduction"><i class="fa fa-check"></i><b>5.5.6</b> Nonlinear dimension reduction</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#discussion"><i class="fa fa-check"></i><b>5.6</b> Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#summary-3"><i class="fa fa-check"></i><b>5.7</b> Summary</a></li>
<li class="chapter" data-level="5.8" data-path="high-dimensional-visualizations.html"><a href="high-dimensional-visualizations.html#resources-1"><i class="fa fa-check"></i><b>5.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html"><i class="fa fa-check"></i><b>6</b> Graphically supported hypotheses</a>
<ul>
<li class="chapter" data-level="6.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#descriptive-vs.-associative-plots"><i class="fa fa-check"></i><b>6.1</b> Descriptive vs. associative plots</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#descriptive-plots"><i class="fa fa-check"></i><b>6.1.1</b> Descriptive plots</a></li>
<li class="chapter" data-level="6.1.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#associative-plots"><i class="fa fa-check"></i><b>6.1.2</b> Associative plots</a></li>
<li class="chapter" data-level="6.1.3" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#correctly-using-descriptive-and-demonstrative-plots"><i class="fa fa-check"></i><b>6.1.3</b> Correctly using descriptive and demonstrative plots</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#correlation-and-causation"><i class="fa fa-check"></i><b>6.2</b> Correlation and causation</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#the-association-is-not-statistically-supported"><i class="fa fa-check"></i><b>6.2.1</b> The association is not statistically supported</a></li>
<li class="chapter" data-level="6.2.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#reversing-cause-and-effect"><i class="fa fa-check"></i><b>6.2.2</b> Reversing cause and effect</a></li>
<li class="chapter" data-level="6.2.3" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#the-association-is-induced-by-a-third-variable"><i class="fa fa-check"></i><b>6.2.3</b> The association is induced by a third variable</a></li>
<li class="chapter" data-level="6.2.4" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#simpsons-paradox"><i class="fa fa-check"></i><b>6.2.4</b> Simpson’s paradox</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#data-presentation-as-story-telling"><i class="fa fa-check"></i><b>6.3</b> Data presentation as story telling</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#what-is-a-story"><i class="fa fa-check"></i><b>6.3.1</b> What is a story?</a></li>
<li class="chapter" data-level="6.3.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#presentation-structure"><i class="fa fa-check"></i><b>6.3.2</b> Presentation structure</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#guidelines-for-coloring-in-data-visualization"><i class="fa fa-check"></i><b>6.4</b> Guidelines for coloring in data visualization</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#color-coding-in-r"><i class="fa fa-check"></i><b>6.4.1</b> Color coding in R</a></li>
<li class="chapter" data-level="6.4.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#general-rules-for-color-coding"><i class="fa fa-check"></i><b>6.4.2</b> General rules for color coding</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#general-dos-and-donts-in-data-visualization"><i class="fa fa-check"></i><b>6.5</b> General do’s and don’ts in data visualization</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#dos"><i class="fa fa-check"></i><b>6.5.1</b> Do’s</a></li>
<li class="chapter" data-level="6.5.2" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#donts"><i class="fa fa-check"></i><b>6.5.2</b> don’ts</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#summary-4"><i class="fa fa-check"></i><b>6.6</b> Summary</a></li>
<li class="chapter" data-level="6.7" data-path="graph-supported-hypos.html"><a href="graph-supported-hypos.html#resources-2"><i class="fa fa-check"></i><b>6.7</b> Resources</a></li>
</ul></li>
<li class="part"><span><b>III Conclude</b></span></li>
<li class="chapter" data-level="7" data-path="resampling-stat.html"><a href="resampling-stat.html"><i class="fa fa-check"></i><b>7</b> Resampling-based Statistical Assessment</a>
<ul>
<li class="chapter" data-level="7.1" data-path="resampling-stat.html"><a href="resampling-stat.html#yeast-dataset"><i class="fa fa-check"></i><b>7.1</b> The yeast dataset</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="resampling-stat.html"><a href="resampling-stat.html#the-experiment"><i class="fa fa-check"></i><b>7.1.1</b> The experiment</a></li>
<li class="chapter" data-level="7.1.2" data-path="resampling-stat.html"><a href="resampling-stat.html#genotype"><i class="fa fa-check"></i><b>7.1.2</b> Genotype</a></li>
<li class="chapter" data-level="7.1.3" data-path="resampling-stat.html"><a href="resampling-stat.html#growth-rates"><i class="fa fa-check"></i><b>7.1.3</b> Growth rates</a></li>
<li class="chapter" data-level="7.1.4" data-path="resampling-stat.html"><a href="resampling-stat.html#genotype-growth-rate-association-in-maltose-at-a-specific-marker"><i class="fa fa-check"></i><b>7.1.4</b> Genotype-growth rate association in maltose at a specific marker</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="resampling-stat.html"><a href="resampling-stat.html#statistical-hypothesis-testing"><i class="fa fa-check"></i><b>7.2</b> Statistical hypothesis testing</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="resampling-stat.html"><a href="resampling-stat.html#permut-test-build-up"><i class="fa fa-check"></i><b>7.2.1</b> Permutation testing: An intuitive build-up</a></li>
<li class="chapter" data-level="7.2.2" data-path="resampling-stat.html"><a href="resampling-stat.html#concepts-of-statistical-hypothesis-testing"><i class="fa fa-check"></i><b>7.2.2</b> Concepts of Statistical Hypothesis Testing</a></li>
<li class="chapter" data-level="7.2.3" data-path="resampling-stat.html"><a href="resampling-stat.html#permutation-testing-formally"><i class="fa fa-check"></i><b>7.2.3</b> Permutation testing, formally</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="resampling-stat.html"><a href="resampling-stat.html#confidence-intervals-quantifying-uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>7.3</b> Confidence intervals: Quantifying uncertainty in parameter estimates</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="resampling-stat.html"><a href="resampling-stat.html#repeating-experiments-to-quantify-uncertainty"><i class="fa fa-check"></i><b>7.3.1</b> Repeating experiments to quantify uncertainty</a></li>
<li class="chapter" data-level="7.3.2" data-path="resampling-stat.html"><a href="resampling-stat.html#simulating-repeated-experiments"><i class="fa fa-check"></i><b>7.3.2</b> Simulating repeated experiments</a></li>
<li class="chapter" data-level="7.3.3" data-path="resampling-stat.html"><a href="resampling-stat.html#quantifying-uncertainty-using-the-case-resampling-bootstrap"><i class="fa fa-check"></i><b>7.3.3</b> Quantifying uncertainty using the case resampling bootstrap</a></li>
<li class="chapter" data-level="7.3.4" data-path="resampling-stat.html"><a href="resampling-stat.html#confidence-intervals-formal-definition"><i class="fa fa-check"></i><b>7.3.4</b> Confidence Intervals: Formal definition</a></li>
<li class="chapter" data-level="7.3.5" data-path="resampling-stat.html"><a href="resampling-stat.html#visualizing-the-formal-definition-of-confidence-intervals"><i class="fa fa-check"></i><b>7.3.5</b> Visualizing the formal definition of Confidence Intervals</a></li>
<li class="chapter" data-level="7.3.6" data-path="resampling-stat.html"><a href="resampling-stat.html#hypothesis-testing-with-the-confidence-interval"><i class="fa fa-check"></i><b>7.3.6</b> Hypothesis testing with the Confidence Interval</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="resampling-stat.html"><a href="resampling-stat.html#discussion-1"><i class="fa fa-check"></i><b>7.4</b> Discussion</a></li>
<li class="chapter" data-level="7.5" data-path="resampling-stat.html"><a href="resampling-stat.html#conclusion"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="analytical-stat.html"><a href="analytical-stat.html"><i class="fa fa-check"></i><b>8</b> Analytical Statistical Assessment</a>
<ul>
<li class="chapter" data-level="8.1" data-path="analytical-stat.html"><a href="analytical-stat.html#motivation-hypothesis-testing-in-large-datasets"><i class="fa fa-check"></i><b>8.1</b> Motivation: Hypothesis testing in large datasets</a></li>
<li class="chapter" data-level="8.2" data-path="analytical-stat.html"><a href="analytical-stat.html#the-binomial-test-testing-hypotheses-for-a-single-binary-variable"><i class="fa fa-check"></i><b>8.2</b> The Binomial Test: testing hypotheses for a single binary variable</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="analytical-stat.html"><a href="analytical-stat.html#abstraction-tossing-a-coin"><i class="fa fa-check"></i><b>8.2.1</b> Abstraction: Tossing a coin</a></li>
<li class="chapter" data-level="8.2.2" data-path="analytical-stat.html"><a href="analytical-stat.html#computing-a-binomial-test-with-r"><i class="fa fa-check"></i><b>8.2.2</b> Computing a binomial test with R</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="analytical-stat.html"><a href="analytical-stat.html#fisher-test"><i class="fa fa-check"></i><b>8.3</b> Fisher’s exact test: Testing the association between two binary variables</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="analytical-stat.html"><a href="analytical-stat.html#permutation-testing-and-the-hypergeometric-distribution"><i class="fa fa-check"></i><b>8.3.1</b> Permutation testing and the hypergeometric distribution</a></li>
<li class="chapter" data-level="8.3.2" data-path="analytical-stat.html"><a href="analytical-stat.html#fishers-exact-test"><i class="fa fa-check"></i><b>8.3.2</b> Fisher’s exact test</a></li>
<li class="chapter" data-level="8.3.3" data-path="analytical-stat.html"><a href="analytical-stat.html#fishers-exact-test-in-r"><i class="fa fa-check"></i><b>8.3.3</b> Fisher’s exact test in R</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="analytical-stat.html"><a href="analytical-stat.html#testing-the-association-between-one-quantitative-and-one-binary-variable"><i class="fa fa-check"></i><b>8.4</b> Testing the association between one quantitative and one binary variable</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="analytical-stat.html"><a href="analytical-stat.html#the-t-test"><i class="fa fa-check"></i><b>8.4.1</b> The t-test</a></li>
<li class="chapter" data-level="8.4.2" data-path="analytical-stat.html"><a href="analytical-stat.html#wilcoxon-rank-sum-test-an-alternative-to-the-t-test-for-non-gaussian-data"><i class="fa fa-check"></i><b>8.4.2</b> Wilcoxon rank-sum test: An alternative to the t-test for non-Gaussian data</a></li>
<li class="chapter" data-level="8.4.3" data-path="analytical-stat.html"><a href="analytical-stat.html#why-bother-with-the-wilcoxon-rank-sum-test"><i class="fa fa-check"></i><b>8.4.3</b> Why bother with the Wilcoxon rank-sum test?</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="analytical-stat.html"><a href="analytical-stat.html#association-between-two-quantitative-variables"><i class="fa fa-check"></i><b>8.5</b> Association between two quantitative variables</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="analytical-stat.html"><a href="analytical-stat.html#the-pearson-correlation-test"><i class="fa fa-check"></i><b>8.5.1</b> The Pearson correlation test</a></li>
<li class="chapter" data-level="8.5.2" data-path="analytical-stat.html"><a href="analytical-stat.html#the-spearman-rank-correlation-test"><i class="fa fa-check"></i><b>8.5.2</b> The Spearman rank correlation test</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="analytical-stat.html"><a href="analytical-stat.html#testing-associations-of-two-variables-overview"><i class="fa fa-check"></i><b>8.6</b> Testing associations of two variables: Overview</a></li>
<li class="chapter" data-level="8.7" data-path="analytical-stat.html"><a href="analytical-stat.html#assessing-distributional-assumptions-with-q-q-plots"><i class="fa fa-check"></i><b>8.7</b> Assessing distributional assumptions with Q-Q Plots</a>
<ul>
<li class="chapter" data-level="8.7.1" data-path="analytical-stat.html"><a href="analytical-stat.html#limitations-of-histograms"><i class="fa fa-check"></i><b>8.7.1</b> Limitations of Histograms</a></li>
<li class="chapter" data-level="8.7.2" data-path="analytical-stat.html"><a href="analytical-stat.html#q-q-plots-comparing-empirical-to-theoretical-quantiles"><i class="fa fa-check"></i><b>8.7.2</b> Q-Q plots: Comparing empirical to theoretical quantiles</a></li>
<li class="chapter" data-level="8.7.3" data-path="analytical-stat.html"><a href="analytical-stat.html#typical-q-q-plots"><i class="fa fa-check"></i><b>8.7.3</b> Typical Q-Q plots</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="analytical-stat.html"><a href="analytical-stat.html#analytical-conf-int"><i class="fa fa-check"></i><b>8.8</b> Analytical Confidence intervals</a>
<ul>
<li class="chapter" data-level="8.8.1" data-path="analytical-stat.html"><a href="analytical-stat.html#binomial-case"><i class="fa fa-check"></i><b>8.8.1</b> Binomial case</a></li>
<li class="chapter" data-level="8.8.2" data-path="analytical-stat.html"><a href="analytical-stat.html#confidence-intervals-in-r"><i class="fa fa-check"></i><b>8.8.2</b> Confidence intervals in R</a></li>
<li class="chapter" data-level="8.8.3" data-path="analytical-stat.html"><a href="analytical-stat.html#advanced-a-note-on-overlapping-confidence-intervals"><i class="fa fa-check"></i><b>8.8.3</b> Advanced: A note on overlapping confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="8.9" data-path="analytical-stat.html"><a href="analytical-stat.html#discussion-2"><i class="fa fa-check"></i><b>8.9</b> Discussion</a></li>
<li class="chapter" data-level="8.10" data-path="analytical-stat.html"><a href="analytical-stat.html#conclusion-1"><i class="fa fa-check"></i><b>8.10</b> Conclusion</a></li>
<li class="chapter" data-level="8.11" data-path="analytical-stat.html"><a href="analytical-stat.html#resources-3"><i class="fa fa-check"></i><b>8.11</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="big-data-stat.html"><a href="big-data-stat.html"><i class="fa fa-check"></i><b>9</b> Statistical Assessments for Big Data</a>
<ul>
<li class="chapter" data-level="9.1" data-path="big-data-stat.html"><a href="big-data-stat.html#motivation-statistical-significance-in-a-big-data-context"><i class="fa fa-check"></i><b>9.1</b> Motivation: Statistical Significance in a Big Data context</a></li>
<li class="chapter" data-level="9.2" data-path="big-data-stat.html"><a href="big-data-stat.html#effect-size-actually-important-or-just-significant"><i class="fa fa-check"></i><b>9.2</b> Effect Size: Actually important or just significant?</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="big-data-stat.html"><a href="big-data-stat.html#the-relationship-of-sample-size-and-significance"><i class="fa fa-check"></i><b>9.2.1</b> The relationship of sample size and significance</a></li>
<li class="chapter" data-level="9.2.2" data-path="big-data-stat.html"><a href="big-data-stat.html#report-p-value-effect-size-and-plot"><i class="fa fa-check"></i><b>9.2.2</b> Report P-value, effect size, and plot</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="big-data-stat.html"><a href="big-data-stat.html#multiple-testing"><i class="fa fa-check"></i><b>9.3</b> Multiple Testing</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="big-data-stat.html"><a href="big-data-stat.html#multiple-testing-in-real-life-p-hacking-and-fishing-expeditions"><i class="fa fa-check"></i><b>9.3.1</b> Multiple testing in real life: p-Hacking and fishing expeditions</a></li>
<li class="chapter" data-level="9.3.2" data-path="big-data-stat.html"><a href="big-data-stat.html#the-land-of-counterfeit-fake-coins"><i class="fa fa-check"></i><b>9.3.2</b> The Land of Counterfeit (fake) coins</a></li>
<li class="chapter" data-level="9.3.3" data-path="big-data-stat.html"><a href="big-data-stat.html#simulation"><i class="fa fa-check"></i><b>9.3.3</b> Simulation</a></li>
<li class="chapter" data-level="9.3.4" data-path="big-data-stat.html"><a href="big-data-stat.html#nominal-p-values"><i class="fa fa-check"></i><b>9.3.4</b> Nominal p-values</a></li>
<li class="chapter" data-level="9.3.5" data-path="big-data-stat.html"><a href="big-data-stat.html#family-wise-error-rate"><i class="fa fa-check"></i><b>9.3.5</b> Family-wise error rate</a></li>
<li class="chapter" data-level="9.3.6" data-path="big-data-stat.html"><a href="big-data-stat.html#false-discovery-rate"><i class="fa fa-check"></i><b>9.3.6</b> False Discovery Rate</a></li>
<li class="chapter" data-level="9.3.7" data-path="big-data-stat.html"><a href="big-data-stat.html#overview-figure"><i class="fa fa-check"></i><b>9.3.7</b> Overview figure</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="big-data-stat.html"><a href="big-data-stat.html#conclusions"><i class="fa fa-check"></i><b>9.4</b> Conclusions</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="big-data-stat.html"><a href="big-data-stat.html#to-remember"><i class="fa fa-check"></i><b>9.4.1</b> To remember</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="big-data-stat.html"><a href="big-data-stat.html#references"><i class="fa fa-check"></i><b>9.5</b> References</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html"><i class="fa fa-check"></i><b>10</b> Linear Regression</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#motivation-and-overview"><i class="fa fa-check"></i><b>10.1</b> Motivation and overview</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#testing-conditional-dependence"><i class="fa fa-check"></i><b>10.1.1</b> Testing conditional dependence</a></li>
<li class="chapter" data-level="10.1.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#linear-regression"><i class="fa fa-check"></i><b>10.1.2</b> Linear regression</a></li>
<li class="chapter" data-level="10.1.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#limitations"><i class="fa fa-check"></i><b>10.1.3</b> Limitations</a></li>
<li class="chapter" data-level="10.1.4" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#applications"><i class="fa fa-check"></i><b>10.1.4</b> Applications</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#univariate-regression"><i class="fa fa-check"></i><b>10.2</b> Univariate regression</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#galtons-height-dataset"><i class="fa fa-check"></i><b>10.2.1</b> Galton’s height dataset</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#ML-LSE"><i class="fa fa-check"></i><b>10.2.2</b> Maximum likelihood and least squares estimates</a></li>
<li class="chapter" data-level="10.2.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#interpretation-of-the-fitted-coefficients"><i class="fa fa-check"></i><b>10.2.3</b> Interpretation of the fitted coefficients</a></li>
<li class="chapter" data-level="10.2.4" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#predicted-values-are-random-variables"><i class="fa fa-check"></i><b>10.2.4</b> Predicted values are random variables</a></li>
<li class="chapter" data-level="10.2.5" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#explained-variance"><i class="fa fa-check"></i><b>10.2.5</b> Explained variance</a></li>
<li class="chapter" data-level="10.2.6" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#testing-the-relationship-between-y-and-x"><i class="fa fa-check"></i><b>10.2.6</b> Testing the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span></a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#multivariate-regression"><i class="fa fa-check"></i><b>10.3</b> Multivariate regression</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#a-multivariate-example-the-baseball-dataset"><i class="fa fa-check"></i><b>10.3.1</b> A multivariate example: The baseball dataset</a></li>
<li class="chapter" data-level="10.3.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#fitting-multivariate-regression"><i class="fa fa-check"></i><b>10.3.2</b> Fitting multivariate regression</a></li>
<li class="chapter" data-level="10.3.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#testing-sets-of-parameters"><i class="fa fa-check"></i><b>10.3.3</b> Testing sets of parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#lin-reg-diagnostic"><i class="fa fa-check"></i><b>10.4</b> Diagnostic plots</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#assessing-non-linearity-with-residual-plot"><i class="fa fa-check"></i><b>10.4.1</b> Assessing non-linearity with residual plot</a></li>
<li class="chapter" data-level="10.4.2" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#when-error-variance-is-not-constant-heteroscedascity"><i class="fa fa-check"></i><b>10.4.2</b> When error variance is not constant: Heteroscedascity</a></li>
<li class="chapter" data-level="10.4.3" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#gaussianity-q-q-plot-of-the-residuals"><i class="fa fa-check"></i><b>10.4.3</b> Gaussianity: Q-Q-plot of the residuals</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="chap-lin-reg.html"><a href="chap-lin-reg.html#conclusions-1"><i class="fa fa-check"></i><b>10.5</b> Conclusions</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-log-reg.html"><a href="chap-log-reg.html"><i class="fa fa-check"></i><b>11</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#a-univariate-example-predicting-sex-given-the-height"><i class="fa fa-check"></i><b>11.1</b> A univariate example: predicting sex given the height</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#from-linear-regression-to-logistic-regression"><i class="fa fa-check"></i><b>11.1.1</b> From linear regression to logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="chap-log-reg.html"><a href="chap-log-reg.html#ML-CE"><i class="fa fa-check"></i><b>11.2</b> Maximum likelihood estimates and the cross-entropy criterion</a></li>
<li class="chapter" data-level="11.3" data-path="chap-log-reg.html"><a href="chap-log-reg.html#logistic-regression-as-a-generalized-linear-model"><i class="fa fa-check"></i><b>11.3</b> Logistic regression as a generalized linear model</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#logistic-regression-with-r"><i class="fa fa-check"></i><b>11.3.1</b> Logistic regression with R</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap-log-reg.html"><a href="chap-log-reg.html#overview-plot-of-the-univariate-example"><i class="fa fa-check"></i><b>11.3.2</b> Overview plot of the univariate example</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="chap-log-reg.html"><a href="chap-log-reg.html#interpreting-a-logistic-regression-fit"><i class="fa fa-check"></i><b>11.4</b> Interpreting a logistic regression fit</a>
<ul>
<li class="chapter" data-level="11.4.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#predicted-odds"><i class="fa fa-check"></i><b>11.4.1</b> Predicted odds</a></li>
<li class="chapter" data-level="11.4.2" data-path="chap-log-reg.html"><a href="chap-log-reg.html#coefficients-of-the-logistic-regression"><i class="fa fa-check"></i><b>11.4.2</b> Coefficients of the logistic regression</a></li>
<li class="chapter" data-level="11.4.3" data-path="chap-log-reg.html"><a href="chap-log-reg.html#effects-on-probabilities"><i class="fa fa-check"></i><b>11.4.3</b> Effects on probabilities</a></li>
<li class="chapter" data-level="11.4.4" data-path="chap-log-reg.html"><a href="chap-log-reg.html#class-imbalance"><i class="fa fa-check"></i><b>11.4.4</b> Class imbalance</a></li>
<li class="chapter" data-level="11.4.5" data-path="chap-log-reg.html"><a href="chap-log-reg.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>11.4.5</b> Multiple Logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="chap-log-reg.html"><a href="chap-log-reg.html#assessing-the-performance-of-a-classifier"><i class="fa fa-check"></i><b>11.5</b> Assessing the performance of a classifier</a>
<ul>
<li class="chapter" data-level="11.5.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#classification-with-logistic-regression"><i class="fa fa-check"></i><b>11.5.1</b> Classification with logistic regression</a></li>
<li class="chapter" data-level="11.5.2" data-path="chap-log-reg.html"><a href="chap-log-reg.html#confusion-matrix"><i class="fa fa-check"></i><b>11.5.2</b> Confusion Matrix</a></li>
<li class="chapter" data-level="11.5.3" data-path="chap-log-reg.html"><a href="chap-log-reg.html#classification-performance-metrics"><i class="fa fa-check"></i><b>11.5.3</b> Classification performance metrics</a></li>
<li class="chapter" data-level="11.5.4" data-path="chap-log-reg.html"><a href="chap-log-reg.html#choosing-a-classification-cutoff"><i class="fa fa-check"></i><b>11.5.4</b> Choosing a classification cutoff</a></li>
<li class="chapter" data-level="11.5.5" data-path="chap-log-reg.html"><a href="chap-log-reg.html#roc-curve"><i class="fa fa-check"></i><b>11.5.5</b> ROC curve</a></li>
<li class="chapter" data-level="11.5.6" data-path="chap-log-reg.html"><a href="chap-log-reg.html#precision-recall-curve"><i class="fa fa-check"></i><b>11.5.6</b> Precision Recall curve</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="chap-log-reg.html"><a href="chap-log-reg.html#conclusions-2"><i class="fa fa-check"></i><b>11.6</b> Conclusions</a>
<ul>
<li class="chapter" data-level="11.6.1" data-path="chap-log-reg.html"><a href="chap-log-reg.html#to-remember-1"><i class="fa fa-check"></i><b>11.6.1</b> To remember</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="supervised-learning.html"><a href="supervised-learning.html"><i class="fa fa-check"></i><b>12</b> Supervised Learning</a>
<ul>
<li class="chapter" data-level="12.1" data-path="supervised-learning.html"><a href="supervised-learning.html#introduction-3"><i class="fa fa-check"></i><b>12.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="supervised-learning.html"><a href="supervised-learning.html#motivation-2"><i class="fa fa-check"></i><b>12.1.1</b> Motivation</a></li>
<li class="chapter" data-level="12.1.2" data-path="supervised-learning.html"><a href="supervised-learning.html#supervised-learning-vs.-unsupervised-learning"><i class="fa fa-check"></i><b>12.1.2</b> Supervised learning vs. unsupervised learning</a></li>
<li class="chapter" data-level="12.1.3" data-path="supervised-learning.html"><a href="supervised-learning.html#notation"><i class="fa fa-check"></i><b>12.1.3</b> Notation</a></li>
<li class="chapter" data-level="12.1.4" data-path="supervised-learning.html"><a href="supervised-learning.html#basic-approach-in-supervised-machine-learning"><i class="fa fa-check"></i><b>12.1.4</b> Basic approach in supervised machine learning</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="supervised-learning.html"><a href="supervised-learning.html#over--and-under-fitting"><i class="fa fa-check"></i><b>12.2</b> Over- and Under-fitting</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="supervised-learning.html"><a href="supervised-learning.html#example-polynomial-curve-fitting"><i class="fa fa-check"></i><b>12.2.1</b> Example: polynomial curve fitting</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="supervised-learning.html"><a href="supervised-learning.html#splitting-the-dataset-for-performance-assessment"><i class="fa fa-check"></i><b>12.3</b> Splitting the dataset for performance assessment</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="supervised-learning.html"><a href="supervised-learning.html#over-fitting-to-the-training-dataset"><i class="fa fa-check"></i><b>12.3.1</b> Over-fitting to the training dataset</a></li>
<li class="chapter" data-level="12.3.2" data-path="supervised-learning.html"><a href="supervised-learning.html#cross-validation"><i class="fa fa-check"></i><b>12.3.2</b> Cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forests-as-alternative-models"><i class="fa fa-check"></i><b>12.4</b> Random Forests as alternative models</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="supervised-learning.html"><a href="supervised-learning.html#the-basics-of-decision-trees"><i class="fa fa-check"></i><b>12.4.1</b> The basics of decision trees</a></li>
<li class="chapter" data-level="12.4.2" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forests-for-classification-and-regression-tasks"><i class="fa fa-check"></i><b>12.4.2</b> Random Forests for classification and regression tasks</a></li>
<li class="chapter" data-level="12.4.3" data-path="supervised-learning.html"><a href="supervised-learning.html#random-forests-in-r"><i class="fa fa-check"></i><b>12.4.3</b> Random Forests in R</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="supervised-learning.html"><a href="supervised-learning.html#conclusion-2"><i class="fa fa-check"></i><b>12.5</b> Conclusion</a></li>
<li class="chapter" data-level="12.6" data-path="supervised-learning.html"><a href="supervised-learning.html#resources-4"><i class="fa fa-check"></i><b>12.6</b> Resources</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="importing-data.html"><a href="importing-data.html"><i class="fa fa-check"></i><b>A</b> Importing data</a>
<ul>
<li class="chapter" data-level="A.1" data-path="importing-data.html"><a href="importing-data.html#paths-and-the-working-directory"><i class="fa fa-check"></i><b>A.1</b> Paths and the working directory</a>
<ul>
<li class="chapter" data-level="A.1.1" data-path="importing-data.html"><a href="importing-data.html#the-filesystem"><i class="fa fa-check"></i><b>A.1.1</b> The filesystem</a></li>
<li class="chapter" data-level="A.1.2" data-path="importing-data.html"><a href="importing-data.html#relative-and-full-paths"><i class="fa fa-check"></i><b>A.1.2</b> Relative and full paths</a></li>
<li class="chapter" data-level="A.1.3" data-path="importing-data.html"><a href="importing-data.html#the-working-directory"><i class="fa fa-check"></i><b>A.1.3</b> The working directory</a></li>
<li class="chapter" data-level="A.1.4" data-path="importing-data.html"><a href="importing-data.html#generating-path-names"><i class="fa fa-check"></i><b>A.1.4</b> Generating path names</a></li>
<li class="chapter" data-level="A.1.5" data-path="importing-data.html"><a href="importing-data.html#copying-files-using-paths"><i class="fa fa-check"></i><b>A.1.5</b> Copying files using paths</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="importing-data.html"><a href="importing-data.html#the-readr-and-readxl-packages"><i class="fa fa-check"></i><b>A.2</b> The readr and readxl packages</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="importing-data.html"><a href="importing-data.html#readr"><i class="fa fa-check"></i><b>A.2.1</b> readr</a></li>
<li class="chapter" data-level="A.2.2" data-path="importing-data.html"><a href="importing-data.html#readxl"><i class="fa fa-check"></i><b>A.2.2</b> readxl</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="importing-data.html"><a href="importing-data.html#exercises"><i class="fa fa-check"></i><b>A.3</b> Exercises</a></li>
<li class="chapter" data-level="A.4" data-path="importing-data.html"><a href="importing-data.html#downloading-files"><i class="fa fa-check"></i><b>A.4</b> Downloading files</a></li>
<li class="chapter" data-level="A.5" data-path="importing-data.html"><a href="importing-data.html#r-base-importing-functions"><i class="fa fa-check"></i><b>A.5</b> R-base importing functions</a>
<ul>
<li><a href="importing-data.html#scan"><code>scan</code></a></li>
</ul></li>
<li class="chapter" data-level="A.6" data-path="importing-data.html"><a href="importing-data.html#text-versus-binary-files"><i class="fa fa-check"></i><b>A.6</b> Text versus binary files</a></li>
<li class="chapter" data-level="A.7" data-path="importing-data.html"><a href="importing-data.html#unicode-versus-ascii"><i class="fa fa-check"></i><b>A.7</b> Unicode versus ASCII</a></li>
<li class="chapter" data-level="A.8" data-path="importing-data.html"><a href="importing-data.html#organizing-data-with-spreadsheets"><i class="fa fa-check"></i><b>A.8</b> Organizing data with spreadsheets</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html"><i class="fa fa-check"></i><b>B</b> R programming</a>
<ul>
<li class="chapter" data-level="B.1" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#conditionals"><i class="fa fa-check"></i><b>B.1</b> Conditional expressions</a></li>
<li class="chapter" data-level="B.2" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#defining-functions"><i class="fa fa-check"></i><b>B.2</b> Defining functions</a></li>
<li class="chapter" data-level="B.3" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#namespaces"><i class="fa fa-check"></i><b>B.3</b> Namespaces</a></li>
<li class="chapter" data-level="B.4" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#for-loops"><i class="fa fa-check"></i><b>B.4</b> For-loops</a></li>
<li class="chapter" data-level="B.5" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#vectorization"><i class="fa fa-check"></i><b>B.5</b> Vectorization and functionals</a></li>
<li class="chapter" data-level="B.6" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#r-markdown"><i class="fa fa-check"></i><b>B.6</b> R Markdown</a></li>
<li class="chapter" data-level="B.7" data-path="appendix-r-programming.html"><a href="appendix-r-programming.html#resources-5"><i class="fa fa-check"></i><b>B.7</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html"><i class="fa fa-check"></i><b>C</b> Additonal plotting tools</a>
<ul>
<li class="chapter" data-level="C.1" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#plotting-themes"><i class="fa fa-check"></i><b>C.1</b> Plotting themes</a></li>
<li class="chapter" data-level="C.2" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#axes"><i class="fa fa-check"></i><b>C.2</b> Axes</a>
<ul>
<li class="chapter" data-level="C.2.1" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#axis-elements"><i class="fa fa-check"></i><b>C.2.1</b> Axis elements</a></li>
</ul></li>
<li class="chapter" data-level="C.3" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#plot-title"><i class="fa fa-check"></i><b>C.3</b> Plot title</a></li>
<li class="chapter" data-level="C.4" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#legend"><i class="fa fa-check"></i><b>C.4</b> Legend</a></li>
<li class="chapter" data-level="C.5" data-path="additonal-plotting-tools.html"><a href="additonal-plotting-tools.html#interactive-plots"><i class="fa fa-check"></i><b>C.5</b> Interactive plots</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html"><i class="fa fa-check"></i><b>D</b> Probabilities</a>
<ul>
<li class="chapter" data-level="D.1" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#probability-conditional-probability-and-dependence"><i class="fa fa-check"></i><b>D.1</b> Probability, conditional probability, and dependence</a></li>
<li class="chapter" data-level="D.2" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#expected-value-variance-and-covariance"><i class="fa fa-check"></i><b>D.2</b> Expected value, variance, and covariance</a></li>
<li class="chapter" data-level="D.3" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#sample-estimates"><i class="fa fa-check"></i><b>D.3</b> Sample estimates</a></li>
<li class="chapter" data-level="D.4" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#appendix-lin-reg"><i class="fa fa-check"></i><b>D.4</b> Linear regression</a></li>
<li class="chapter" data-level="D.5" data-path="appendix-probabilities.html"><a href="appendix-probabilities.html#resources-6"><i class="fa fa-check"></i><b>D.5</b> Resources</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="./">Julien Gagneur, TUM</a></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis and Visualization in R (IN2339)</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chap-lin-reg" class="section level1" number="10">
<h1><span class="header-section-number">Chapter 10</span> Linear Regression</h1>
<div id="motivation-and-overview" class="section level2" number="10.1">
<h2><span class="header-section-number">10.1</span> Motivation and overview</h2>
<div id="testing-conditional-dependence" class="section level3" number="10.1.1">
<h3><span class="header-section-number">10.1.1</span> Testing conditional dependence</h3>
<p>Up to this point, we have focused on testing associations between pairs of variables. However, in data science applications, it is very common to study three or more variables jointly.</p>
<p>For pairs of variables, say <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, one variable may be considered as the explanatory variable (let us take <span class="math inline">\(x\)</span>) and the other the response variable (<span class="math inline">\(y\)</span>). Asking whether <span class="math inline">\(y\)</span> depends on <span class="math inline">\(x\)</span> amounts to ask whether the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> varies when <span class="math inline">\(x\)</span> varies. For instance, one may state that body height depends on sex by showing that the distribution of body heights is not same among males than among females. In general statistical terms, testing for independence can be expressed as the null hypothesis:</p>
<p><span class="math display" id="eq:H0-uni-cond-indep">\[\begin{align}
H_0: p(y|x) = p(y).
\tag{10.1}
\end{align}\]</span></p>
<p>Let us assume now we are considering multiple explanatory variables <span class="math inline">\(x_1, ...,x_p\)</span> and a response variable <span class="math inline">\(y\)</span>. We are still interested in the association of one particular explanatory variable, say <span class="math inline">\(x_j\)</span> with the response variable <span class="math inline">\(y\)</span> but shall take into consideration all the other explanatory variables. In other words, we would like to know whether <span class="math inline">\(y\)</span> depends on <span class="math inline">\(x_j\)</span> <em>everything else being the same</em>. For instance, we may observe that drinking coffee positively correlates with lung cancer, but, neither among smokers, nor among non-smokers, drinking coffee associates with lung cancer. In this example, everything else being same (i.e. for same smoking status), there is no association between lung cancer and drinking coffee. In this thought experiment, the association found in the overall population probably comes from the fact that smokers tend to be coffee drinkers.
Generally, we ask whether the conditional distribution of <span class="math inline">\(y\)</span> given all explanatory variables varies when <span class="math inline">\(x_j\)</span> alone varies. Hence, to move beyond pairs of variables, we need a strategy to assess null hypotheses such as:</p>
<p><span class="math display" id="eq:H0-cond-indep">\[\begin{align}
H_0:  p(y|x_1,...,x_j,...x_p) = p(y|x_1,...,x_{j-1}, x_{j+1},...x_p)
\tag{10.2}
\end{align}\]</span></p>
<p>Considered so generally, i.e. for any type of variable <span class="math inline">\(y\)</span> and for any number and types of explanatory variables <span class="math inline">\(x_1,...,x_p\)</span>, the null hypothesis in Equation <a href="chap-lin-reg.html#eq:H0-cond-indep">(10.2)</a> is impractical because:</p>
<ol style="list-style-type: decimal">
<li><p>We need to be able to deal with any type of distribution: Gaussian, Binomial, Poisson, but also mixtures, or even distributions that are not functionally parameterized.</p></li>
<li><p>We need to be able to condition on continuous variables. Conditioning on categorical variables leads to obvious stratification of the data. However, how shall we deal with continuous ones? Shall we do some binning? Based on what criteria? Could the binning strategy affect the results?</p></li>
<li><p>Even when all variables are categorical, there is a combinatorial explosion of strata, as each stratum gets further split by every new added variables (say by smoking status, and further by sex, and further by diabetes,…). For instance, there are <span class="math inline">\(2^p\)</span> different strata for <span class="math inline">\(p\)</span> binary variables. This combinatorial explosion makes the analysis difficult. Moreover the statistical power, which is driven by the number of samples in each stratum, gets drastically reduced.</p></li>
</ol>
</div>
<div id="linear-regression" class="section level3" number="10.1.2">
<h3><span class="header-section-number">10.1.2</span> Linear regression</h3>
<p>This Chapter introduces linear regression as an effective way to deal with these three issues. Linear regression addresses these three issues by making the following assumptions:</p>
<ul>
<li><p>The conditional distribution <span class="math inline">\(p(y | x_1,...,x_p)\)</span> is a Gaussian distribution whose variance is independent of <span class="math inline">\(x_1,...x_p\)</span>, simplifying greatly issue number 1.</p></li>
<li><p>The conditional expectation of <span class="math inline">\(y\)</span> is a simple linear combinations of the explanatory variables, that is:</p></li>
</ul>
<p><span class="math display" id="eq:E-linreg">\[\begin{align}
\operatorname{E}[y|x_1,...,x_p] = \beta_0 + \beta_1 x_1+...\beta_p x_p
\tag{10.3}
\end{align}\]</span></p>
<p>We can think of Equation <a href="chap-lin-reg.html#eq:E-linreg">(10.3)</a> as a simple score for which explanatory variables contribute in a weighted fashion and independently of each other to variations in the expected value of the response.
For instance, one could model the expected body height of an adult as:<a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a></p>
<p><span class="math display">\[\begin{align}
\operatorname{E}[\text{height}| \text{sex}, \text{mother}, \text{father}] = &amp;165 + 15 \times \text{sex} +0.5\times(\text{mother} -165) \\
&amp;+ 0.5\times(\text{father} -180),
\end{align}\]</span></p>
<p>where sex is 1 for male and 0 for female, and mother and father designate each parent’s body height.</p>
<p>The linear model in Equation <a href="chap-lin-reg.html#eq:E-linreg">(10.3)</a>, can deal with continuous as well as discrete explanatory variables, solving the issue number 2. Moreover, because Equation <a href="chap-lin-reg.html#eq:E-linreg">(10.3)</a> models independent additive contributions of the explanatory variables, it has just one parameter per explanatory variable. The effects of the explanatory variables do not depend on the value of the other variables. These effects do not change between strata. Hence, linear regression does not suffer from a combinatorial explosion of strata (issue number 3).</p>
</div>
<div id="limitations" class="section level3" number="10.1.3">
<h3><span class="header-section-number">10.1.3</span> Limitations</h3>
<p>These assumptions of linear regression makes the task easier, but how limiting are they?</p>
<p>The Gaussian assumption is limiting. We cannot deal with binary response variables for instance. The next chapter will address such cases.</p>
<p>The assumption that variance is independent of the strata, if violated, can be a problem for fitting this model as well as for statistical testing. We show how to diagnose such issue in Section <a href="chap-lin-reg.html#lin-reg-diagnostic">10.4</a>.</p>
<p>The assumption of additivity in Equation <a href="chap-lin-reg.html#eq:E-linreg">(10.3)</a> seems more limiting than it actually is. First, there are many real-life situations where additivity of the effects of the explanatory variables turns out to be reasonable. Moreover, there is always the possibility to pre-compute non-linear transformations of explanatory variables and include them to the model. For instance this model of expected body weight is a valid linear model with respect to body height cubed – it just needs the cube to be pre-computed<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a>:</p>
<p><span class="math display">\[\operatorname{E}[\text{weight}| \text{height}] = \beta_0 + \beta_1\text{height} ^3\]</span>
All in all, linear regression turns out to be in practice often reasonable.</p>
</div>
<div id="applications" class="section level3" number="10.1.4">
<h3><span class="header-section-number">10.1.4</span> Applications</h3>
<p>Linear regression can be used for various purposes:</p>
<ul>
<li><p>To <strong>test conditional dependence</strong>. This is done by testing the null hypothesis:
<span class="math display">\[H_0: \beta_j=0.\]</span></p></li>
<li><p>To <strong>estimate the effects of one variable on the response variable</strong>. This is done by providing an estimate of the coefficient <span class="math inline">\(\beta_j\)</span>.</p></li>
<li><p>To <strong>predict</strong> the value of the response variable given values of the explanatory variables. The predicted value is then an estimate of the conditional expectation <span class="math inline">\(\operatorname{E}[y|x]\)</span>.</p></li>
<li><p>To <strong>quantify how much variation</strong> of a response variable can be explained by a set of explanatory variables.</p></li>
</ul>
<p>This Chapter explains first univariate linear regression using a historical dataset of body height. We will then go to the multivariate case using an example from baseball. Finally we will assess what is the practical impact of violations of the modeling assumptions, and how to diagnose them. A substantial part of the Chapter is based on an adaptation of Rafael Irizzary’s book.</p>
</div>
</div>
<div id="univariate-regression" class="section level2" number="10.2">
<h2><span class="header-section-number">10.2</span> Univariate regression</h2>
<div id="galtons-height-dataset" class="section level3" number="10.2.1">
<h3><span class="header-section-number">10.2.1</span> Galton’s height dataset</h3>
<p>We start with linear regression against a single variable, or univariate regression. We use the dataset from which regression was born. The example is from genetics. Francis Galton<a href="#fn29" class="footnote-ref" id="fnref29"><sup>29</sup></a> studied the variation and heredity of human traits. Among many other traits, Galton collected and studied height data from families to try to understand heredity<a href="#fn30" class="footnote-ref" id="fnref30"><sup>30</sup></a>. While doing this, he developed the concepts of correlation and regression, as well as a connection to pairs of data that follow a normal distribution. A very specific question Galton tried to answer was: how well can we predict a child’s height based on the parents’ height?</p>
<p>We have access to Galton’s family height data through the <strong>HistData</strong> package. This data records height in inches from several dozen families: mothers, fathers, daughters, and sons. To imitate Galton’s analysis, we will create a dataset with the heights of fathers and a randomly selected son of each family:</p>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="chap-lin-reg.html#cb605-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb605-2"><a href="chap-lin-reg.html#cb605-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(HistData)</span>
<span id="cb605-3"><a href="chap-lin-reg.html#cb605-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(data.table)</span>
<span id="cb605-4"><a href="chap-lin-reg.html#cb605-4" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;GaltonFamilies&quot;</span>)</span>
<span id="cb605-5"><a href="chap-lin-reg.html#cb605-5" aria-hidden="true" tabindex="-1"></a>GaltonFamilies <span class="ot">&lt;-</span> <span class="fu">as.data.table</span>(GaltonFamilies)</span>
<span id="cb605-6"><a href="chap-lin-reg.html#cb605-6" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1983</span>)</span>
<span id="cb605-7"><a href="chap-lin-reg.html#cb605-7" aria-hidden="true" tabindex="-1"></a>galton_heights <span class="ot">&lt;-</span> GaltonFamilies[gender <span class="sc">==</span> <span class="st">&#39;male&#39;</span>][,.SD[<span class="fu">sample</span>(.N, 1L)],by <span class="ot">=</span> family][,.(father, childHeight)]</span>
<span id="cb605-8"><a href="chap-lin-reg.html#cb605-8" aria-hidden="true" tabindex="-1"></a><span class="fu">setnames</span>(galton_heights, <span class="st">&quot;childHeight&quot;</span>, <span class="st">&quot;son&quot;</span>)</span></code></pre></div>
<p>Plotting clearly shows that the taller the father, the taller the son.</p>
<div class="sourceCode" id="cb606"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb606-1"><a href="chap-lin-reg.html#cb606-1" aria-hidden="true" tabindex="-1"></a>galton_heights <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(father, son)) <span class="sc">+</span> </span>
<span id="cb606-2"><a href="chap-lin-reg.html#cb606-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="dataviz_book_files/figure-html/lecture-02-388-1.png" width="40%" /></p>
<p>One can visualize more concretely the conditional distributions of the son heights given the father heights by stratifying the father heights (i.e. <span class="math inline">\(p(\text{son} | \text{father})\)</span>):</p>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="chap-lin-reg.html#cb607-1" aria-hidden="true" tabindex="-1"></a>galton_heights[, father_strata <span class="sc">:</span><span class="er">=</span> <span class="fu">factor</span>(<span class="fu">round</span>(father))] <span class="sc">%&gt;%</span> </span>
<span id="cb607-2"><a href="chap-lin-reg.html#cb607-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(father_strata, son)) <span class="sc">+</span> </span>
<span id="cb607-3"><a href="chap-lin-reg.html#cb607-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span> </span>
<span id="cb607-4"><a href="chap-lin-reg.html#cb607-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="dataviz_book_files/figure-html/lecture-02-389-1.png" width="40%" /></p>
<p>The centers of the groups are increasing with the father height, giving a first glimpse as how hereditary body height is.</p>
<p>By strata, we can estimate the conditional expectations. In our example, we end up with the following prediction for the son of a father who is 72 inches tall:</p>
<div class="sourceCode" id="cb608"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb608-1"><a href="chap-lin-reg.html#cb608-1" aria-hidden="true" tabindex="-1"></a>conditional_avg <span class="ot">&lt;-</span> galton_heights[<span class="fu">round</span>(father) <span class="sc">==</span> <span class="dv">72</span>,</span>
<span id="cb608-2"><a href="chap-lin-reg.html#cb608-2" aria-hidden="true" tabindex="-1"></a>               <span class="fu">mean</span>(son)]</span>
<span id="cb608-3"><a href="chap-lin-reg.html#cb608-3" aria-hidden="true" tabindex="-1"></a>conditional_avg</span></code></pre></div>
<pre><code>## [1] 70.5</code></pre>
<p>Furthermore, these centers appear to follow a linear relationship. Below we plot the averages of each group. If we take into account that these averages are random variables, the data is consistent with these points following a straight line:</p>
<div class="figure"><span id="fig:mean-son-vs-father"></span>
<img src="dataviz_book_files/figure-html/mean-son-vs-father-1.png" alt="Average son heights by strata of the father heights" width="40%" />
<p class="caption">
Figure 10.1: Average son heights by strata of the father heights
</p>
</div>
<p>Hence this plot suggests that the linear model:</p>
<p><span class="math display">\[ \operatorname{E}[\text{son} | \text{father}] =\beta_0 + \beta_1 \text{father}\]</span></p>
<p>is reasonable. Fitting such a model would allow us to avoid doing stratifications, as we could continuously estimate the expected value of son’s height. How do we estimate these coefficients? To this end, we need a bit more theory.</p>
</div>
<div id="ML-LSE" class="section level3" number="10.2.2">
<h3><span class="header-section-number">10.2.2</span> Maximum likelihood and least squares estimates</h3>
<p>Fitting a linear regression, i.e. estimating the parameters, is based on a widely used principle called <strong>maximum likelihood</strong>. The maximum likelihood principle consists in choosing as parameter values those for which the data is most probable. What is the probability of our data?</p>
<p>Here we model the probability of the heights of the sons given the heights of their fathers. Generally, we will always consider the values of the explanatory variables to be given. Furthermore we will assume that, conditioned on the values of the explanatory variable, the observations are independent. Here, this means that the height of the sons are independent given the heights of the fathers. Hence the likelihood is:
<span class="math display">\[\begin{align}
p(\text{Data}|\beta_0,\beta_1) = \prod_i p(y_i|x_i, \beta_0, \beta_1)
\end{align}\]</span></p>
<p>Hence, we look for the values of the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to maximimize <span class="math inline">\(\prod_i p(y_i|x_i, \beta_0, \beta_1)\)</span>.</p>
<p>Taking the logarithm of the likelihood, which is a monotonically increasing function, does not change the value of the optimal parameters. Plugging in furthermore the density of the Gaussian<a href="#fn31" class="footnote-ref" id="fnref31"><sup>31</sup></a> and discarding terms not affected by the values of <span class="math inline">\(\beta_0\)</span> <span class="math inline">\(\beta_1\)</span>,
we obtain that:</p>
<p><span class="math display">\[\begin{align}
\arg \max_{\beta_0, \beta_1}\prod_i p(y_i|x_i, \beta_0, \beta_1) &amp;= \arg \max_{\beta_0, \beta_1}\sum_i \log(N(y_i|x_i, \beta_0, \beta_1))\\
&amp;= \arg \min_{\beta_0, \beta_1}\sum_i (y_i - (\beta_0 + \beta_1x_i))^2
\end{align}\]</span></p>
<p>The differences between the observed values and their expected values denoted <span class="math inline">\(\epsilon_i = y_i - (\beta_0 + \beta_1x_i)\)</span> are called the errors. Hence, maximizing the likelihood of this model is equivalent to minimizing the sum of the squared errors. One talks about <strong>least squares estimates</strong> (LSE).</p>
<div class="figure"><span id="fig:lecture-02-391"></span>
<img src="assets/img/lec12_linear_model_geometric0.png" alt="\label{fig:linModel} Visualization of a Linear Regression model." width="1298" />
<p class="caption">
Figure 10.2:  Visualization of a Linear Regression model.
</p>
</div>
</div>
<div id="interpretation-of-the-fitted-coefficients" class="section level3" number="10.2.3">
<h3><span class="header-section-number">10.2.3</span> Interpretation of the fitted coefficients</h3>
<p>Minimizing the squared errors for a linear model can be solved analytically (see Appendix <a href="appendix-probabilities.html#appendix-lin-reg">D.4</a>). We obtain that our estimated conditional expected value, denoted <span class="math inline">\(\hat y\)</span> is:</p>
<p><span class="math display" id="eq:coef-univariate">\[\begin{align}
\hat y= \hat \beta_0 + \hat \beta_1x \mbox{ with slope } \hat \beta_1 = \rho \frac{\sigma_y}{\sigma_x} \mbox{ and intercept } \hat \beta_0=\mu_y - \hat \beta_1 \mu_x
\tag{10.4}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mu_x,\mu_y\)</span> are the means of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(\sigma_x,\sigma_y\)</span> are the standard deviations of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(\rho = \frac{1}{n} \sum_{i=1}^n \left( \frac{x_i-\mu_x}{\sigma_x} \right)\left( \frac{y_i-\mu_y}{\sigma_y} \right)\)</span> is the Pearson correlation coefficient between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</li>
</ul>
<p>We can rewrite this result as:</p>
<p><span class="math display">\[ 
\hat y = \mu_y + \rho \left( \frac{x-\mu_x}{\sigma_x} \right) \sigma_y
\]</span>
If there is perfect correlation, the regression line predicts an increase in the response by the same number of standard deviations. If there is 0 correlation, then we don’t use <span class="math inline">\(x\)</span> at all for the prediction and simply predict the population average <span class="math inline">\(\mu_y\)</span>. For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase. Note that if the correlation is positive but smaller than 1, our prediction of <span class="math inline">\(y\)</span> is closer to its average than <span class="math inline">\(x\)</span> to its average (in standard units).</p>
<p>In R, we can obtain the least squares estimates using the <code>lm</code> function:</p>
<div class="sourceCode" id="cb610"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb610-1"><a href="chap-lin-reg.html#cb610-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(son <span class="sc">~</span> father, <span class="at">data =</span> galton_heights)</span>
<span id="cb610-2"><a href="chap-lin-reg.html#cb610-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coefficients</span>(fit)</span></code></pre></div>
<pre><code>## (Intercept)      father 
##   37.775451    0.454742</code></pre>
<p>The most common way we use <code>lm</code> is by using the character <code>~</code> to let <code>lm</code> know which is the variable we are predicting (left of <code>~</code>) and which we are using to predict (right of <code>~</code>). The intercept is added automatically to the model that will be fit.</p>
<p>Here we add the regression line to the original data using the ggplot2 function <code>geom_smooth(method = "lm")</code> which computes and adds the linear regression line to a plot along with confidence intervals:</p>
<div class="sourceCode" id="cb612"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb612-1"><a href="chap-lin-reg.html#cb612-1" aria-hidden="true" tabindex="-1"></a>galton_heights <span class="sc">%&gt;%</span> </span>
<span id="cb612-2"><a href="chap-lin-reg.html#cb612-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(father, son)) <span class="sc">+</span> </span>
<span id="cb612-3"><a href="chap-lin-reg.html#cb612-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb612-4"><a href="chap-lin-reg.html#cb612-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="dataviz_book_files/figure-html/lecture-02-393-1.png" width="40%" /></p>
<p>In our example, the correlation between sons’ and fathers’ heights is about 0.5. Our predicted value for the son’s height <span class="math inline">\(y=\)</span> 70.5 for a 72-inch father was only 0.48 standard deviations larger than the average son in contrast to the father’s height which was 1.1 standard deviations above average. This is why we call it <em>regression</em>: the son regresses to the average height. In fact, the title of Galton’s paper was: <em>Regression toward mediocrity in hereditary stature</em>.</p>
<p>It is a fact that children of extremely tall parents are taller than average, yet typically shorter than their parents. You can appreciate it on Figure <a href="chap-lin-reg.html#fig:mean-son-vs-father">10.1</a>. Extremely tall parents have an extreme combinations of alleles, and have also benefited from environmental factors by chance, which may not repeat in the next generation. The same phenomenon, called <em>regression toward the mean</em>, happens in sport. Athletes who perform extremely well one year are certainly good (high expectation) but have also probably been lucky (a positive error) so they are more likely to perform relatively worse the next year, etc. This phenomenon is ubiquitous and can lead to fallacies.<a href="#fn32" class="footnote-ref" id="fnref32"><sup>32</sup></a></p>
</div>
<div id="predicted-values-are-random-variables" class="section level3" number="10.2.4">
<h3><span class="header-section-number">10.2.4</span> Predicted values are random variables</h3>
<p>Once we fit our model, we can obtain predictions of <span class="math inline">\(y\)</span> by plugging the estimates into the regression model. For example, if the father’s height is <span class="math inline">\(x\)</span>, then our prediction <span class="math inline">\(\hat{y}\)</span> for the son’s height is:</p>
<p><span class="math display">\[\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\]</span></p>
<p>When we plot <span class="math inline">\(\hat{y}\)</span> versus <span class="math inline">\(x\)</span>, we see the regression line.</p>
<p>Keep in mind that the prediction <span class="math inline">\(\hat{y}\)</span> is also a random variable and mathematical theory tells us what the standard errors are. If we assume the errors are normal, or we have a large enough sample size, we can use theory to construct confidence intervals as well. In fact, the ggplot2 layer <code>geom_smooth(method = "lm")</code> that we previously used plots <span class="math inline">\(\hat{y}\)</span> and surrounds it by confidence intervals:</p>
<div class="sourceCode" id="cb614"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb614-1"><a href="chap-lin-reg.html#cb614-1" aria-hidden="true" tabindex="-1"></a>galton_heights <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="fu">aes</span>(son, father)) <span class="sc">+</span></span>
<span id="cb614-2"><a href="chap-lin-reg.html#cb614-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb614-3"><a href="chap-lin-reg.html#cb614-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="dataviz_book_files/figure-html/father-son-regression-1.png" width="672" /></p>
<p>The R function <code>predict</code> takes an <code>lm</code> object as input and returns the prediction. If requested, the standard errors and other information from which we can construct confidence intervals are provided:</p>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="chap-lin-reg.html#cb616-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> galton_heights <span class="sc">%&gt;%</span> <span class="fu">lm</span>(son <span class="sc">~</span> father, <span class="at">data =</span> .) </span>
<span id="cb616-2"><a href="chap-lin-reg.html#cb616-2" aria-hidden="true" tabindex="-1"></a>y_hat <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">se.fit =</span> <span class="cn">TRUE</span>)</span>
<span id="cb616-3"><a href="chap-lin-reg.html#cb616-3" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(y_hat)</span></code></pre></div>
<pre><code>## [1] &quot;fit&quot;            &quot;se.fit&quot;         &quot;df&quot;            
## [4] &quot;residual.scale&quot;</code></pre>
</div>
<div id="explained-variance" class="section level3" number="10.2.5">
<h3><span class="header-section-number">10.2.5</span> Explained variance</h3>
<p>Any dataset will give us estimates of the model, but how well does the model represent our data?
To investigate this we can compute quality metrics and visually assess the model, which is explained later.</p>
<p>Under the linear regression assumptions, the conditional standard deviation is:</p>
<p><span class="math display">\[
  \mbox{SD}(y \mid x ) = \sigma_y \sqrt{1-\rho^2} 
\]</span></p>
<p>To see why this is intuitive, notice that without conditioning, <span class="math inline">\(\mbox{SD}(y) = \sigma_y\)</span>, we are looking at the variability of all the sons. But once we condition, we are only looking at the variability of the sons with the same father’s height (for instance all those with a 72-inch, father). This group will tend to have similar heights so the standard deviation is reduced.</p>
<p>This is usually quantified in terms of proportion of variance. So we say that <span class="math inline">\(X\)</span> explains <span class="math inline">\(1- (1-\rho^2)=\rho^2\)</span> (the Pearson correlation squared) of the variance. The proportion of variance explained by the model is commonly called the <em>coefficient of determination</em> or <span class="math inline">\(R^2\)</span>. Another way of deriving <span class="math inline">\(R^2\)</span> is described in the following steps:</p>
<p>First, we compute the model predictions:
<span class="math display">\[\hat{y}_i = \hat{\beta_0} + \hat{\beta_1} x_i .\]</span></p>
<p>Then, we compute the <em>residuals</em> (by comparing the predictions with the actual values)
<span class="math display">\[\hat{\epsilon}_i = \hat{y}_i - y_i ,\]</span></p>
<p>and the <em>residual sum of squares</em>
<span class="math display">\[RSS = \sum_{i=1}^N \hat{\epsilon}_i^2 .\]</span></p>
<p>Lastly, we can compare the <em>residual sum of squares</em> to the total <em>sum of squares</em> (<em>SS</em>) of <span class="math inline">\(y\)</span>
<span class="math display">\[R^2 = 1 - \frac{\sum_{i=1}^N \hat{\epsilon}_i^2}{\sum_{i=1}^N (y_i - \bar{y})^2} = 1 - \frac{RSS}{SS} .\]</span></p>
<p>It can take any value between 0 and 1, since the sum of squares represents the variation around the global mean and the residual sum of squares represents the variation around the model predictions.
In case the model learned no variation, it learned at least the global mean. In this case the sum of squares and the residual sum of squares are equal and <span class="math inline">\(R^2 = 0\)</span>.
In case the model is perfect, the residuals are zero and hence the second term vanishes and the <span class="math inline">\(R^2\)</span> becomes 1.</p>
<p>The <span class="math inline">\(R^2\)</span> is usually best combined with a scatter plot of <span class="math inline">\(y\)</span> against <span class="math inline">\(\hat{y}\)</span>.</p>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="chap-lin-reg.html#cb618-1" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(son <span class="sc">~</span> father, <span class="at">data =</span> galton_heights)</span>
<span id="cb618-2"><a href="chap-lin-reg.html#cb618-2" aria-hidden="true" tabindex="-1"></a>r2 <span class="ot">&lt;-</span> <span class="fu">summary</span>(m)<span class="sc">$</span>r.squared</span>
<span id="cb618-3"><a href="chap-lin-reg.html#cb618-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb618-4"><a href="chap-lin-reg.html#cb618-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(galton_heights, <span class="fu">aes</span>(<span class="at">x=</span><span class="fu">predict</span>(m), <span class="at">y=</span>son)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb618-5"><a href="chap-lin-reg.html#cb618-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_abline</span>(<span class="at">intercept=</span><span class="dv">0</span>, <span class="at">slope=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb618-6"><a href="chap-lin-reg.html#cb618-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">x=</span><span class="dv">67</span>, <span class="at">y=</span><span class="dv">77</span>, </span>
<span id="cb618-7"><a href="chap-lin-reg.html#cb618-7" aria-hidden="true" tabindex="-1"></a>                  <span class="at">label=</span><span class="fu">deparse</span>(<span class="fu">bquote</span>(R<span class="sc">^</span><span class="dv">2</span> <span class="sc">==</span> .(<span class="fu">signif</span>(r2, <span class="dv">2</span>))))</span>
<span id="cb618-8"><a href="chap-lin-reg.html#cb618-8" aria-hidden="true" tabindex="-1"></a>                  ), <span class="at">parse=</span><span class="cn">TRUE</span>) <span class="sc">+</span></span>
<span id="cb618-9"><a href="chap-lin-reg.html#cb618-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Predicted height&quot;</span>, <span class="at">y=</span><span class="st">&quot;height&quot;</span>) </span></code></pre></div>
<p><img src="dataviz_book_files/figure-html/lecture-02-394-1.png" width="672" /></p>
</div>
<div id="testing-the-relationship-between-y-and-x" class="section level3" number="10.2.6">
<h3><span class="header-section-number">10.2.6</span> Testing the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span></h3>
<p>The LSE is derived from the data <span class="math inline">\(y_1,\dots,y_n\)</span>, which are a realization of random variables. This implies that our estimates are random variables. Using the assumption of independent Gaussian noise, we obtain the following distribution:</p>
<p><span class="math display">\[ p(\hat \beta) = N(\beta, \sigma^2/ns^2_x)\]</span></p>
<p>Hence, if the modeling assumptions hold, the estimates are unbiased, meaning that their expected value are the true values of the parameter <span class="math inline">\(\operatorname{E}[\hat \beta] = \beta\)</span>. Moreover they are consistent, meaning that for infinitely large sample size they converge to the true values.</p>
<p>Another result allows us to build a hypothesis test, namely:
<span class="math display">\[p\left(\frac{\hat{\beta} - \beta}{\hat{se}(\hat{\beta})}\right) = t_{n-2}\left(\frac{\hat{\beta} - \beta}{\hat{se}(\hat{\beta})}\right)\]</span>
where <span class="math inline">\(t_{n-2}\)</span> denotes the student’s <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.</p>
<p>Remember, the p-value of a statistical test is the <strong>probability</strong> of the value of a <strong>test statistic</strong> being at least as extreme as the one observed in our data <strong>under the null hypothesis</strong>.</p>
<p>In our case we can formulate the null hypothesis, that <span class="math inline">\(y\)</span> does not depend on <span class="math inline">\(x_i\)</span> as follows:</p>
<ul>
<li>The null hypothesis for parameter <span class="math inline">\(\beta_i\)</span> is <span class="math inline">\(H_0: \beta_i = 0\)</span></li>
<li>The test statistic is <span class="math inline">\(\hat{t} = \frac{\hat{\beta} - \beta_i}{\hat{se}(\hat{\beta})} = \frac{\hat{\beta}}{\hat{se}(\hat{\beta})}\)</span></li>
<li>The probability under the null model is <span class="math inline">\(P(t \geq \hat{t}), \mbox{where } t \sim t_{n-2}\)</span></li>
</ul>
<p>To confirm a linear relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span>, we need to reject the null hypothesis at significance level <span class="math inline">\(\alpha\)</span> (= 0.05):</p>
<ul>
<li>Accept <span class="math inline">\(H_0\)</span> if <span class="math inline">\(P(|t| \geq |\hat{t}|) &gt; \alpha\)</span></li>
<li>Reject <span class="math inline">\(H_0\)</span> if <span class="math inline">\(P(|t| \geq |\hat{t}|) \leq \alpha\)</span></li>
</ul>
<p>Now the question remains, how do we do this in R? Luckily the <code>summary(fit)</code> function helps us here. It reports t-statistics (<code>t value</code>) and p-values (<code>Pr(&gt;|t|)</code>) for each estimate.</p>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb619-1"><a href="chap-lin-reg.html#cb619-1" aria-hidden="true" tabindex="-1"></a>  <span class="fu">lm</span>(son <span class="sc">~</span> father, <span class="at">data =</span> galton_heights) <span class="sc">%&gt;%</span> </span>
<span id="cb619-2"><a href="chap-lin-reg.html#cb619-2" aria-hidden="true" tabindex="-1"></a>  summary <span class="sc">%&gt;%</span> .<span class="sc">$</span>coef</span></code></pre></div>
<pre><code>##              Estimate Std. Error  t value     Pr(&gt;|t|)
## (Intercept) 37.775451 4.97272309 7.596532 1.693821e-12
## father       0.454742 0.07192596 6.322363 2.034076e-09</code></pre>
</div>
</div>
<div id="multivariate-regression" class="section level2" number="10.3">
<h2><span class="header-section-number">10.3</span> Multivariate regression</h2>
<p>Since Galton’s original development, regression has become one of the most widely used tools in data science. One reason for this has to do with the fact that regression permits us to find relationships between two variables taking into account the effects of other variables. This has been particularly popular in fields where randomized experiments are hard to run, such as economics and epidemiology.</p>
<p>When we are not able to randomly assign each individual to a treatment or control group, confounding is particularly prevalent. For example, consider estimating the effect of eating fast foods on life expectancy using data collected from a random sample of people in a jurisdiction. Fast food consumers are more likely to be smokers, drinkers, and have lower incomes. Therefore, a naive regression model may lead to an overestimate of the negative health effect of fast food. So how do we account for confounding in practice? In this section we learn how linear regression against multiple variables, or multivariate regression, can help with such situations and can be used to describe how one or more variables affect an outcome variable.</p>
<div id="a-multivariate-example-the-baseball-dataset" class="section level3" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> A multivariate example: The baseball dataset</h3>
<p>We will use data from baseball, leveraging a famous real application of regression which has led to improved estimations of baseball player values in the 90’s<a href="#fn33" class="footnote-ref" id="fnref33"><sup>33</sup></a> Here we will not build a model of player value but, instead, we will focus on predicting the game scores of a team.</p>
<div id="baseball-basics" class="section level4" number="10.3.1.1">
<h4><span class="header-section-number">10.3.1.1</span> Baseball basics</h4>
<p>The goal of a baseball game is to score more runs (points) than the other team. Each team has 9 batters that have an opportunity to hit a ball with a bat in a predetermined order. After the 9th batter, the first batter bats again, then the second, and so on.</p>
<p>Each time a batter has an opportunity to bat, the other team’s <em>pitcher</em> throws the ball and the batter tries to hit it. The batter either makes an <em>out</em> and returns to the bench or the batter hits it.</p>
<p>When the batter hits the ball, the batter wants to pass as many <em>bases</em> as possible before the opponent team catches the ball. There are four bases with the fourth one called <em>home plate</em>. Home plate is where batters start by trying to hit, so the bases form a cycle.</p>
<p><img src="assets/img/Baseball_Diamond1.png" width="50%" /></p>
<p>(Courtesy of Cburnett<a href="#fn34" class="footnote-ref" id="fnref34"><sup>34</sup></a>. CC BY-SA 3.0 license<a href="#fn35" class="footnote-ref" id="fnref35"><sup>35</sup></a>.)
<!--Source: [Wikipedia Commons](https://commons.wikimedia.org/wiki/File:Baseball_diamond_simplified.svg))--></p>
<p>A batter who stops at one of the intermediate three bases can resume running along the base cycle when next batters hit the ball. A batter who goes around the bases and arrives home (directly or indirectly with stops at intermediate bases), scores a <strong>run</strong>.</p>
<p>We want to understand what makes a team score well on average. Hence, the average number of runs is our response variable.</p>
</div>
<div id="home-runs" class="section level4" number="10.3.1.2">
<h4><span class="header-section-number">10.3.1.2</span> Home runs</h4>
<p>A <strong>home run</strong> happens when the batter who hits the ball goes all the way home. This happens when the ball is hit very far, giving time for the batter to perform the full run. It is very good for the score because not only the batter scores a run, but the other players of the same team who are already on the pitch standing at intermediate bases typically finish their runs too.</p>
<p>Not surprisingly, average home runs positively correlate with average runs:</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb621-1"><a href="chap-lin-reg.html#cb621-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Lahman)</span>
<span id="cb621-2"><a href="chap-lin-reg.html#cb621-2" aria-hidden="true" tabindex="-1"></a>Teams <span class="ot">&lt;-</span> <span class="fu">as.data.table</span>(Teams)</span>
<span id="cb621-3"><a href="chap-lin-reg.html#cb621-3" aria-hidden="true" tabindex="-1"></a>Teams_filt <span class="ot">&lt;-</span> Teams[yearID <span class="sc">%in%</span> <span class="dv">1961</span><span class="sc">:</span><span class="dv">2001</span>]</span>
<span id="cb621-4"><a href="chap-lin-reg.html#cb621-4" aria-hidden="true" tabindex="-1"></a>Teams_filt[,<span class="fu">c</span>(<span class="st">&#39;HR_per_game&#39;</span>,<span class="st">&#39;R_per_game&#39;</span>) <span class="sc">:</span><span class="er">=</span> <span class="fu">list</span>(HR<span class="sc">/</span>G, R<span class="sc">/</span>G)]<span class="sc">%&gt;%</span></span>
<span id="cb621-5"><a href="chap-lin-reg.html#cb621-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(HR_per_game, R_per_game)) <span class="sc">+</span> </span>
<span id="cb621-6"><a href="chap-lin-reg.html#cb621-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb621-7"><a href="chap-lin-reg.html#cb621-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<p><img src="dataviz_book_files/figure-html/runs-vs-hrs-1.png" width="672" /></p>
<p>A univariate linear regression gives the following parameter estimates:</p>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb622-1"><a href="chap-lin-reg.html#cb622-1" aria-hidden="true" tabindex="-1"></a>fit_r_vs_hr <span class="ot">&lt;-</span> <span class="fu">lm</span>(R_per_game<span class="sc">~</span>HR_per_game, <span class="at">data=</span>Teams_filt)</span>
<span id="cb622-2"><a href="chap-lin-reg.html#cb622-2" aria-hidden="true" tabindex="-1"></a>fit_r_vs_hr</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = R_per_game ~ HR_per_game, data = Teams_filt)
## 
## Coefficients:
## (Intercept)  HR_per_game  
##       2.778        1.845</code></pre>
<p>So this tells us that teams that hit 1 more HR per game than the average team, score 1.8448241 more runs per game than the average team. Given that the most common final score is a difference of a run, this can certainly lead to a large increase in wins. Not surprisingly, HR hitters are very expensive. Because we are working on a budget, we will need to find some other way to increase wins. So, in the next section, we move our attention to another possible predictive variable of runs.</p>
</div>
<div id="base-on-balls" class="section level4" number="10.3.1.3">
<h4><span class="header-section-number">10.3.1.3</span> Base on balls</h4>
<p>Obviously the pitcher is encouraged to throw the ball at the batter. This is achieved thanks to the so-called <strong>base on balls</strong> rule stating that if the pitcher fails to throw the ball through a predefined area considered to be hittable (the strikezone), the batter is permitted to go to the first base.</p>
<p>A base on ball is not as great as a home run but it makes the batter progress by one base so it should be rather beneficial for the score. Let us now scatter plot average runs against average base on balls:</p>
<div class="sourceCode" id="cb624"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb624-1"><a href="chap-lin-reg.html#cb624-1" aria-hidden="true" tabindex="-1"></a>Teams_filt[,BB_per_game <span class="sc">:</span><span class="er">=</span> BB<span class="sc">/</span>G]<span class="sc">%&gt;%</span></span>
<span id="cb624-2"><a href="chap-lin-reg.html#cb624-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(BB_per_game, R_per_game)) <span class="sc">+</span> </span>
<span id="cb624-3"><a href="chap-lin-reg.html#cb624-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb624-4"><a href="chap-lin-reg.html#cb624-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="dataviz_book_files/figure-html/runs-vs-bb-1.png" width="672" /></p>
<p>Here, again we see a clear association. If we find the regression line for predicting runs from bases on balls, we a get slope of:</p>
<div class="sourceCode" id="cb626"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb626-1"><a href="chap-lin-reg.html#cb626-1" aria-hidden="true" tabindex="-1"></a>get_slope <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y) <span class="fu">cor</span>(x, y) <span class="sc">*</span> <span class="fu">sd</span>(y) <span class="sc">/</span> <span class="fu">sd</span>(x)</span>
<span id="cb626-2"><a href="chap-lin-reg.html#cb626-2" aria-hidden="true" tabindex="-1"></a>bb_slope <span class="ot">&lt;-</span> Teams_filt[,.(<span class="at">slope =</span> <span class="fu">get_slope</span>(BB_per_game, R_per_game) )] </span>
<span id="cb626-3"><a href="chap-lin-reg.html#cb626-3" aria-hidden="true" tabindex="-1"></a>bb_slope </span></code></pre></div>
<pre><code>##        slope
## 1: 0.7353288</code></pre>
<p>So does this mean that if we go and hire low salary players with many BB, and who therefore increase the number of walks per game by 2, our team will score 1.5 more runs per game?</p>
<p>In fact, it looks like BBs and HRs are also associated:</p>
<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb628-1"><a href="chap-lin-reg.html#cb628-1" aria-hidden="true" tabindex="-1"></a>Teams_filt <span class="sc">%&gt;%</span></span>
<span id="cb628-2"><a href="chap-lin-reg.html#cb628-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(HR_per_game, BB_per_game)) <span class="sc">+</span> </span>
<span id="cb628-3"><a href="chap-lin-reg.html#cb628-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb628-4"><a href="chap-lin-reg.html#cb628-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">&quot;lm&quot;</span>)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="dataviz_book_files/figure-html/bb-vs-hrs-1.png" width="672" /></p>
<p>We know that HRs cause runs because, as the name “home run” implies, when a player hits a HR they are guaranteed at least one run. Could it be that HRs also cause BB and this makes it appear as if BB cause runs?</p>
<p>It turns out that pitchers, afraid of HRs, will sometimes avoid throwing strikes to HR hitters. As a result, HR hitters tend to have more BBs and a team with many HRs will also have more BBs. Although it may appear that BBs cause runs, it is actually the HRs that cause most of these runs. We say that BBs are <em>confounded</em> with HRs. Nonetheless, could it be that BBs still help? To find out, we somehow have to adjust for the HR effect. Linear regression will help us parse all this out and quantify the associations. This can in turn help determine what players to recruit.</p>
</div>
<div id="understanding-confounding-through-stratification" class="section level4" number="10.3.1.4">
<h4><span class="header-section-number">10.3.1.4</span> Understanding confounding through stratification</h4>
<p>We first untangle the direct from the indirect effects step-by-step by stratification to get a concrete understanding of the situation.</p>
<p>A first approach is to keep HRs fixed at a certain value and then examine the relationship between BB and runs. As we did when we stratified fathers by rounding to the closest inch, here we can stratify HR per game to the closest tenth. We filter out the strata with few points to avoid highly variable estimates:</p>
<div class="sourceCode" id="cb630"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb630-1"><a href="chap-lin-reg.html#cb630-1" aria-hidden="true" tabindex="-1"></a>dat <span class="ot">&lt;-</span> Teams_filt[,HR_strata <span class="sc">:</span><span class="er">=</span> <span class="fu">round</span>(HR<span class="sc">/</span>G, <span class="dv">1</span>)][HR_strata <span class="sc">&gt;=</span> <span class="fl">0.4</span> <span class="sc">&amp;</span> HR_strata <span class="sc">&lt;=</span><span class="fl">1.2</span>]</span></code></pre></div>
<p>and then make a scatterplot for each strata:</p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="chap-lin-reg.html#cb631-1" aria-hidden="true" tabindex="-1"></a>dat <span class="sc">%&gt;%</span> </span>
<span id="cb631-2"><a href="chap-lin-reg.html#cb631-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(BB_per_game, R_per_game)) <span class="sc">+</span>  </span>
<span id="cb631-3"><a href="chap-lin-reg.html#cb631-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb631-4"><a href="chap-lin-reg.html#cb631-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb631-5"><a href="chap-lin-reg.html#cb631-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">facet_wrap</span>( <span class="sc">~</span> HR_strata) </span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="dataviz_book_files/figure-html/runs-vs-bb-by-hr-strata-1.png" width="80%" /></p>
<p>Remember that the regression slope for predicting runs with BB was 0.7. Once we stratify by HR, these slopes are substantially reduced:</p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="chap-lin-reg.html#cb633-1" aria-hidden="true" tabindex="-1"></a>dat[<span class="fu">order</span>(HR_strata),.(<span class="at">slope =</span> <span class="fu">get_slope</span>(BB_per_game, R_per_game)), by<span class="ot">=</span><span class="st">&#39;HR_strata&#39;</span>] </span></code></pre></div>
<pre><code>##    HR_strata     slope
## 1:       0.4 0.7342910
## 2:       0.5 0.5659067
## 3:       0.6 0.4119129
## 4:       0.7 0.2853933
## 5:       0.8 0.3650361
## 6:       0.9 0.2608882
## 7:       1.0 0.5115687
## 8:       1.1 0.4539252
## 9:       1.2 0.4403274</code></pre>
<p>The slopes are reduced, but they are not 0, which indicates that BBs are helpful for producing runs, just not as much as previously thought<a href="#fn36" class="footnote-ref" id="fnref36"><sup>36</sup></a></p>
<p>Although our understanding of the application tells us that HR cause BB but not the other way around, we can still check if stratifying by BB makes the effect of BB go down. To do this, we use the same code except that we swap HR and BBs to get this plot:</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="dataviz_book_files/figure-html/runs-vs-hr-by-bb-strata-1.png" width="100%" /></p>
<p>In this case, the slopes are reduced a bit, which is consistent with the fact that BB do in fact cause some runs:</p>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb636-1"><a href="chap-lin-reg.html#cb636-1" aria-hidden="true" tabindex="-1"></a>dat[<span class="fu">order</span>(BB_strata),.(<span class="at">slope =</span> <span class="fu">get_slope</span>(HR_per_game, R_per_game)), by <span class="ot">=</span> <span class="st">&#39;BB_strata&#39;</span>]</span></code></pre></div>
<pre><code>##     BB_strata    slope
##  1:       2.8 1.518056
##  2:       2.9 1.567879
##  3:       3.0 1.518179
##  4:       3.1 1.494498
##  5:       3.2 1.582159
##  6:       3.3 1.560302
##  7:       3.4 1.481832
##  8:       3.5 1.631314
##  9:       3.6 1.829929
## 10:       3.7 1.451895
## 11:       3.8 1.704564
## 12:       3.9 1.302576</code></pre>
<p>Compared to the original:</p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="chap-lin-reg.html#cb638-1" aria-hidden="true" tabindex="-1"></a>hr_slope <span class="ot">&lt;-</span> Teams_filt[,.(<span class="at">slope =</span> <span class="fu">get_slope</span>(HR_per_game, R_per_game))]</span>
<span id="cb638-2"><a href="chap-lin-reg.html#cb638-2" aria-hidden="true" tabindex="-1"></a>hr_slope</span></code></pre></div>
<pre><code>##       slope
## 1: 1.844824</code></pre>
</div>
<div id="data-suggests-additive-effects" class="section level4" number="10.3.1.5">
<h4><span class="header-section-number">10.3.1.5</span> Data suggests additive effects</h4>
<p>It is somewhat complex to be computing regression lines for each strata. We are essentially fitting models like this:</p>
<p><span class="math display">\[
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1(x_2) x_1 + \beta_2(x_1) x_2
\]</span></p>
<p>with the slopes for <span class="math inline">\(x_1\)</span> changing for different values of <span class="math inline">\(x_2\)</span> and vice versa. But is there an easier approach?</p>
<p>If we take random variability into account, the slopes in the strata don’t appear to change much. If these slopes are in fact the same, this implies that <span class="math inline">\(\beta_1(x_2)\)</span> and <span class="math inline">\(\beta_2(x_1)\)</span> are constants. This in turn implies that the expectation of runs conditioned on HR and BB can be written like this:</p>
<p><span class="math display">\[
\mbox{E}[R \mid BB = x_1, \, HR = x_2] = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\]</span></p>
<p>This model suggests that if the number of HR is fixed at <span class="math inline">\(x_2\)</span>, we observe a linear relationship between runs and BB with an intercept of <span class="math inline">\(\beta_0 + \beta_2 x_2\)</span>. Our exploratory data analysis suggested this. The model also suggests that as the number of HR grows, the intercept growth is linear as well and determined by <span class="math inline">\(\beta_1 x_1\)</span>.</p>
<p>In this analysis, referred to as <em>multivariate regression</em>, you will often hear people say that the BB slope <span class="math inline">\(\beta_1\)</span> is <em>adjusted</em> for the HR effect. If the model is correct then confounding has been accounted for. But how do we estimate <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> from the data? For this, we need to derive some theoretical results that generalize the results from univariate linear regression.</p>
</div>
</div>
<div id="fitting-multivariate-regression" class="section level3" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Fitting multivariate regression</h3>
<p>For a data set <span class="math inline">\((\mathbf x_i, y_i)\)</span> with <span class="math inline">\(i \in \{1 \dots n\}\)</span> and <span class="math inline">\(\mathbf x_i\)</span> a vector of length <span class="math inline">\(p\)</span>, the multiple linear regression model is defined as:
<span class="math display">\[y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{i,j} + \epsilon_i\]</span>
with free parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\boldsymbol{\beta}\)</span> and a random error
<span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> that is i.i.d. (independently and identically distributed).</p>
<p>The model can be written in matrix notation
<span class="math display">\[ \mathbf y= \mathbf X\boldsymbol{\beta} + \boldsymbol{\epsilon} \]</span>
here the matrix <span class="math inline">\(\mathbf X\)</span> is of dimension <span class="math inline">\((n \times p + 1)\)</span> where each row <span class="math inline">\(i\)</span> corresponds to the vector <span class="math inline">\(\mathbf x_i\)</span> with a 1 prepended to accommodate the intercept. The error is distributed as <span class="math inline">\(\boldsymbol{\epsilon} \sim N(\mathbf{0}, \boldsymbol\Sigma)\)</span> as a multivariate Gaussian with covariance <span class="math inline">\(\boldsymbol\Sigma = \sigma^2 \mathbf I\)</span> (i.i.d).</p>
<p>In the same way as for the simple linear model, parameters can be estimated by finding the LSE. For multiple linear regression, we obtain
<span class="math display">\[\hat{\boldsymbol{\beta}} = (\mathbf X^\top\mathbf X)^{-1}\mathbf X^\top \mathbf y\]</span>
<span class="math display">\[\hat{\sigma}^2 = \frac{\hat{\boldsymbol\epsilon}^\top\hat{\boldsymbol\epsilon}}{n - p} .\]</span></p>
<p>As for the univariate case, we have that, under the assumptions of the model, the least squares estimates are unbiased. This means that, if the data truly originates from such a data generative model, the expected value of the estimates over repeated random realizations, equals the true underlying parameter values:</p>
<p><span class="math display">\[E[\hat \beta_j] = \beta_j\]</span></p>
<p>Moreover, the estimates are consistent: They converge to the true values with increasing sample sizes:</p>
<p><span class="math display">\[\hat\beta_j \xrightarrow[n \to \infty]{} \beta_j\]</span></p>
<p>Remarkably, this holds true even if the explanatory variables are correlated (unless perfectly correlated, in which case the parameters become not identifiable).</p>
<p>To fit a multiple linear regression model in R, we can use the same <code>lm</code> function as for the simple linear regression, we only need to adapt the formula to include all predictor variables. Here is the application to fitting the average runs against the home runs and the bases on balls:</p>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb640-1"><a href="chap-lin-reg.html#cb640-1" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(R_per_game <span class="sc">~</span> HR_per_game <span class="sc">+</span> BB_per_game,</span>
<span id="cb640-2"><a href="chap-lin-reg.html#cb640-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">data=</span>Teams_filt)</span>
<span id="cb640-3"><a href="chap-lin-reg.html#cb640-3" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit)</span></code></pre></div>
<pre><code>## (Intercept) HR_per_game BB_per_game 
##   1.7443011   1.5611689   0.3874238</code></pre>
<p>We see that the coefficient for home runs and for bases on balls are similar to those we tediously estimated by stratifications.</p>
<p>The object <code>fit</code> includes more information about the fit, including statistical assessments. We can use the function <code>summary</code> to extract more of this information:</p>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="chap-lin-reg.html#cb642-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = R_per_game ~ HR_per_game + BB_per_game, data = Teams_filt)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.87325 -0.24507 -0.01449  0.23866  1.24218 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.74430    0.08236   21.18   &lt;2e-16 ***
## HR_per_game  1.56117    0.04896   31.89   &lt;2e-16 ***
## BB_per_game  0.38742    0.02701   14.34   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3484 on 1023 degrees of freedom
## Multiple R-squared:  0.6503, Adjusted R-squared:  0.6496 
## F-statistic: 951.2 on 2 and 1023 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Other useful functions to extract information from <code>lm</code> objects are</p>
<ul>
<li><code>predict</code> to compute the fitted values or predict response for new data</li>
<li><code>resid</code> to compute the residuals</li>
</ul>
<p>To understand the statistical assessments included in the summary we need to remember that the LSE are random variables. Mathematical statistics gives us some ideas of the distribution of these random variables.</p>
</div>
<div id="testing-sets-of-parameters" class="section level3" number="10.3.3">
<h3><span class="header-section-number">10.3.3</span> Testing sets of parameters</h3>
<div id="nested-models" class="section level4" number="10.3.3.1">
<h4><span class="header-section-number">10.3.3.1</span> Nested models</h4>
<p>Hypothesis testing can be done on individual coefficients of a multivariate regression, with an appropriate t-test just as in the univariate case.
But testing individual parameters may not suffice. Sometimes, we are interested in testing an entire set of variables. We then compare so-called <strong>nested models</strong>. We compare a full model <span class="math inline">\(\Omega\)</span> to a reduced model <span class="math inline">\(\omega\)</span> where model <span class="math inline">\(\omega\)</span> is a special case of the more general model <span class="math inline">\(\Omega\)</span>. For multivariate regression, this typically consists of setting a set of parameters to 0 in the reduced model.</p>
<p>Consider the following example model: <span class="math inline">\(y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3\)</span></p>
<p>For testing whether the coefficients of two explanatory variables <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> should be 0, one would consider:</p>
<ul>
<li>Full model: all <span class="math inline">\(\beta\)</span>’s can take any value.</li>
<li>Reduced model: <span class="math inline">\(\beta_1 = \beta_2 = 0\)</span> (only the mean <span class="math inline">\(\beta_0\)</span> and the third parameter <span class="math inline">\(\beta_3\)</span> can take any value).</li>
</ul>
</div>
<div id="f-test-and-anova" class="section level4" number="10.3.3.2">
<h4><span class="header-section-number">10.3.3.2</span> F-test and ANOVA</h4>
<p>The F-test can be applied to compare two nested linear regressions. It is based on comparing the fit improvements as measured by the change residual sum of squares. The idea is that the larger model, which has more parameters, always fits better to the data. With the F-test, we ask whether this improvement is significant under the null hypothesis that the smaller model is the correct one.</p>
<p><span class="math display">\[F = \frac{(RSS_{\omega} - RSS_{\Omega}) / (q - p)}{RSS_{\Omega} / (n - q)} , \]</span></p>
<p>where <span class="math inline">\(q\)</span> is the number of parameters in (dimension of) model <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(p\)</span> the number of parameters in (dimension of) model <span class="math inline">\(\omega\)</span> and <span class="math inline">\(RSS\)</span> designates residual sums of squares.</p>
<p>The statistic <span class="math inline">\(F\)</span> is distributed according to the F distribution with (q - p) and (n - q) degrees of freedom respectively. We reject the LRT if <span class="math inline">\(F\)</span> is larger than the critical value corresponding to the significance level. This analysis is also frequently referred to as “Analysis of Variance” or <strong>ANOVA</strong>.</p>
</div>
<div id="example-testing-the-difference-of-means-in-3-groups" class="section level4 unnumbered">
<h4>Example: Testing the difference of means in 3 groups</h4>
<p>As an example let us consider testing whether fuel consumption of cars depend on the number of cylinders using the <code>mtcars</code> dataset:</p>
<p><img src="dataviz_book_files/figure-html/lecture12-linear-regression-mpg-1.png" width="480" /></p>
<p>In this example, we are interested in whether there is a difference in the fuel consumption of cars depending on the number of cylinders in the car.</p>
<p>For this purpose, we will work with the following model:</p>
<p><span class="math display">\[y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 .\]</span></p>
<p>Here, <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> will be indicator variables:</p>
<ul>
<li>group “6 cylinders”: <span class="math inline">\(x_1 = 1\)</span></li>
<li>group “8 cylinders”: <span class="math inline">\(x_2 = 1\)</span></li>
</ul>
<p>We want to test the effect of both indicators at the same time:</p>
<ul>
<li><strong>H0:</strong> <span class="math inline">\(\beta_1 = \beta_2 = 0\)</span></li>
<li>Full model: <span class="math inline">\(\Omega\)</span> is the space where all three <span class="math inline">\(\beta\)</span> can take any value.</li>
<li>Reduced model: <span class="math inline">\(\omega\)</span> is the space where only <span class="math inline">\(\beta_0\)</span> can take any value.</li>
</ul>
<p>In R, we can easily test this:</p>
<div class="sourceCode" id="cb644"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb644-1"><a href="chap-lin-reg.html#cb644-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(<span class="st">&quot;mtcars&quot;</span>)</span>
<span id="cb644-2"><a href="chap-lin-reg.html#cb644-2" aria-hidden="true" tabindex="-1"></a><span class="do">## for the example we need a factor</span></span>
<span id="cb644-3"><a href="chap-lin-reg.html#cb644-3" aria-hidden="true" tabindex="-1"></a><span class="do">## else it will be interpreted as number</span></span>
<span id="cb644-4"><a href="chap-lin-reg.html#cb644-4" aria-hidden="true" tabindex="-1"></a>mtcars<span class="sc">$</span>cyl <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(mtcars<span class="sc">$</span>cyl)</span>
<span id="cb644-5"><a href="chap-lin-reg.html#cb644-5" aria-hidden="true" tabindex="-1"></a><span class="do">## fit the full model</span></span>
<span id="cb644-6"><a href="chap-lin-reg.html#cb644-6" aria-hidden="true" tabindex="-1"></a>full <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> cyl, <span class="at">data=</span>mtcars)</span>
<span id="cb644-7"><a href="chap-lin-reg.html#cb644-7" aria-hidden="true" tabindex="-1"></a><span class="do">## have a look at the model matrix</span></span>
<span id="cb644-8"><a href="chap-lin-reg.html#cb644-8" aria-hidden="true" tabindex="-1"></a><span class="do">## which is automatically created</span></span>
<span id="cb644-9"><a href="chap-lin-reg.html#cb644-9" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(<span class="fu">model.matrix</span>(full))</span></code></pre></div>
<pre><code>##                   (Intercept) cyl6 cyl8
## Mazda RX4                   1    1    0
## Mazda RX4 Wag               1    1    0
## Datsun 710                  1    0    0
## Hornet 4 Drive              1    1    0
## Hornet Sportabout           1    0    1
## Valiant                     1    1    0</code></pre>
<div class="sourceCode" id="cb646"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb646-1"><a href="chap-lin-reg.html#cb646-1" aria-hidden="true" tabindex="-1"></a><span class="do">## fit the reduced model (only the intercept &quot;1&quot;)</span></span>
<span id="cb646-2"><a href="chap-lin-reg.html#cb646-2" aria-hidden="true" tabindex="-1"></a>reduced <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data=</span>mtcars)</span></code></pre></div>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="chap-lin-reg.html#cb647-1" aria-hidden="true" tabindex="-1"></a><span class="do">## compare the models</span></span>
<span id="cb647-2"><a href="chap-lin-reg.html#cb647-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(reduced, full)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: mpg ~ 1
## Model 2: mpg ~ cyl
##   Res.Df     RSS Df Sum of Sq      F    Pr(&gt;F)    
## 1     31 1126.05                                  
## 2     29  301.26  2    824.78 39.697 4.979e-09 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>From the result, we can see that the full model models the data significantly better than the reduced model containing only the intercept. Therefore, we can conclude that there is a difference in the means of the 3 groups.</p>
</div>
</div>
</div>
<div id="lin-reg-diagnostic" class="section level2" number="10.4">
<h2><span class="header-section-number">10.4</span> Diagnostic plots</h2>
<p>The assumptions of a mathematical model never hold exactly in practice. The questions for the practitioners are threefold: 1) How badly are the assumptions violated on the dataset at hand? 2) What are the implications for the claimed conclusions? How can the issue be addressed?</p>
<p>The assumptions of linear regressions are:</p>
<ul>
<li>The expected values of the response are a linear combinations of the explanatory variables</li>
<li>Errors are identically and independently distributed.</li>
<li>Errors follow a normal distribution.</li>
</ul>
<p>We will see that two diagnostic plots will be helpful: the residual plot and the q-q plot of the residuals.</p>
<div id="assessing-non-linearity-with-residual-plot" class="section level3" number="10.4.1">
<h3><span class="header-section-number">10.4.1</span> Assessing non-linearity with residual plot</h3>
<p><strong>Diagnostic plot</strong></p>
<p>Non-linearity is typically revealed by noticing that the average of the residual depends on the predicted values. A smooth fit (<code>geom_smooth</code> default) on the residual plot can help spotting systematic non-linear dependencies (See Figure <a href="chap-lin-reg.html#fig:non-linear-qc">10.3</a>).</p>
<pre><code>## `geom_smooth()` using method = &#39;loess&#39;</code></pre>
<div class="figure"><span id="fig:non-linear-qc"></span>
<img src="dataviz_book_files/figure-html/non-linear-qc-1.png" alt="Detecting non-linearity. (Left). Observed value  against predictions for linear fit on data generated as y = x^3 + noise. The fitted line is shown in blue. (Right) Residual versus predicted value of the same data. A smooth fit to the residual is shown in blue. The smooth fit highlights that the errors depend on the predicted expected values, indicative of non-linearity." width="768" />
<p class="caption">
Figure 10.3: Detecting non-linearity. (Left). Observed value against predictions for linear fit on data generated as y = x^3 + noise. The fitted line is shown in blue. (Right) Residual versus predicted value of the same data. A smooth fit to the residual is shown in blue. The smooth fit highlights that the errors depend on the predicted expected values, indicative of non-linearity.
</p>
</div>
<p><strong>So what?</strong>
The implications of non-linearity depends on the application purposes:</p>
<ul>
<li><p>Predictions are suboptimal. They could be improved with a more complex model. However, they may be good enough for the use case, and they would not necessarily deteriorate on unseen data.</p></li>
<li><p>Explained variance is underestimated.</p></li>
<li><p>The i.i.d assumption is violated: The residuals depend on the predicted mean, suggesting that the errors depend on <span class="math inline">\(\operatorname{E}[y|x]\)</span> and therefore on each other. Therefore, statistical tests are flawed.</p></li>
<li><p>Conditional dependencies can be affected. To see the latter assume a model in which two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> depend on a common cause <span class="math inline">\(z\)</span> in a non-linear fashion (eg. <span class="math inline">\(y=z^2 + \epsilon\)</span> and <span class="math inline">\(x=z^2+\epsilon&#39;\)</span>). These two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent conditioned on their common cause <span class="math inline">\(z\)</span>. However, a linear regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span> and <span class="math inline">\(z\)</span> would fail to discard the contribution of <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p><strong>What to do?</strong>
If non-linearity is revealed in the fit, one can either transform the explanatory variables or the response (eg log-transformtaion, powers, etc). Note that the appropriate transformation is difficult to know in practice and finding it likely requires trying out several options. The <code>formula</code> in R implements a few convenience functions to build such non-linear transformation on the fly while calling lm(). For instance, the R call: <code>model &lt;- lm(y ~ poly(x,3))</code> fits a polynomial of degree 3.</p>
</div>
<div id="when-error-variance-is-not-constant-heteroscedascity" class="section level3" number="10.4.2">
<h3><span class="header-section-number">10.4.2</span> When error variance is not constant: Heteroscedascity</h3>
<p>It happens not so rarely that the variance of the residuals is not constant across all data points. This property is called <strong>heteroscedascity</strong>. Heteroscedascity violates the i.i.d assumption of the errors.</p>
<p><strong>Diagnostic plot</strong></p>
<p>The residual plots can help spotting when error variance depends on response mean. Here is a synthetic example:</p>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb650-1"><a href="chap-lin-reg.html#cb650-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb650-2"><a href="chap-lin-reg.html#cb650-2" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">mean=</span><span class="dv">5</span> <span class="sc">*</span> x, <span class="at">sd=</span><span class="fl">0.1</span><span class="sc">*</span>x)</span>
<span id="cb650-3"><a href="chap-lin-reg.html#cb650-3" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb650-4"><a href="chap-lin-reg.html#cb650-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(<span class="at">data=</span><span class="cn">NULL</span>, <span class="fu">aes</span>(<span class="fu">predict</span>(m), <span class="fu">resid</span>(m))) <span class="sc">+</span> </span>
<span id="cb650-5"><a href="chap-lin-reg.html#cb650-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_abline</span>(<span class="at">intercept=</span><span class="dv">0</span>, <span class="at">slope=</span><span class="dv">0</span>) <span class="sc">+</span></span>
<span id="cb650-6"><a href="chap-lin-reg.html#cb650-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">x=</span><span class="fu">expression</span>(<span class="fu">hat</span>(y)), <span class="at">y=</span><span class="st">&quot;residual&quot;</span>)</span></code></pre></div>
<p><img src="dataviz_book_files/figure-html/lecture-02-407-1.png" width="480" /></p>
<p><strong>So what?</strong>
For prediction, the problem may or not be a real issue. Indeed the fit can be driven by a few data points because the least squares errors give too much importance to the points with high noise. This is particularly a problem with low number of points in areas with large noise.</p>
<p>As the residuals are not i.i.d., the statistical tests are flawed.</p>
<p><strong>What to do?</strong></p>
<p>In case of heteroscedascity, one can try to transform the response variable <span class="math inline">\(y\)</span>, such as: log-transformation, square root, or variance stabilizing transformation.<a href="#fn37" class="footnote-ref" id="fnref37"><sup>37</sup></a></p>
<p>However, as before, the appropriate transformation is difficult to know in practice and finding it likely requires trying out several options. Alternatively, one can use methods with a different noise model. One possibility is to consider weighted least squares, when there is the possibility to estimate before end the relative error variances on the data. This is sometimes the case, for instance, if one has access to experimental uncertainties. Another direction is to use generalized linear models.<a href="#fn38" class="footnote-ref" id="fnref38"><sup>38</sup></a></p>
</div>
<div id="gaussianity-q-q-plot-of-the-residuals" class="section level3" number="10.4.3">
<h3><span class="header-section-number">10.4.3</span> Gaussianity: Q-Q-plot of the residuals</h3>
<p>The Gaussian assumption of the errors is key to all statistical tests. An implication that the errors follow a Gaussian distribution is that the residuals also follow a Gaussian distribution.</p>
<p><strong>Diagnostic plot</strong></p>
<p>We use here a QQ-plot of the residuals against the normal distribution.</p>
<p>The R default function <code>qqnorm()</code> and its companion <code>qqline()</code> generate qq-plot against a Gaussian fitted to the input vector. We do not have to worry about the mean and the standard deviation of the input vector. Here is a “good” example based on simulated data.</p>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb651-1"><a href="chap-lin-reg.html#cb651-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb651-2"><a href="chap-lin-reg.html#cb651-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb651-3"><a href="chap-lin-reg.html#cb651-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="at">mean=</span><span class="dv">5</span> <span class="sc">*</span> x, <span class="at">sd=</span><span class="dv">10</span>)</span>
<span id="cb651-4"><a href="chap-lin-reg.html#cb651-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb651-5"><a href="chap-lin-reg.html#cb651-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(<span class="fu">residuals</span>(m))</span>
<span id="cb651-6"><a href="chap-lin-reg.html#cb651-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(<span class="fu">residuals</span>(m))</span></code></pre></div>
<p><img src="dataviz_book_files/figure-html/lecture-02-408-1.png" width="480" /></p>
<p>And here is a “bad” example based on a simulated example, in which the errors are Student t-distributed with degree of freedom 1:</p>
<div class="sourceCode" id="cb652"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb652-1"><a href="chap-lin-reg.html#cb652-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">0</span>)</span>
<span id="cb652-2"><a href="chap-lin-reg.html#cb652-2" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span></span>
<span id="cb652-3"><a href="chap-lin-reg.html#cb652-3" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> x <span class="sc">+</span> <span class="fu">rt</span>(<span class="dv">20</span>, <span class="at">df=</span><span class="dv">1</span>)</span>
<span id="cb652-4"><a href="chap-lin-reg.html#cb652-4" aria-hidden="true" tabindex="-1"></a>m <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb652-5"><a href="chap-lin-reg.html#cb652-5" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(<span class="fu">residuals</span>(m))</span>
<span id="cb652-6"><a href="chap-lin-reg.html#cb652-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(<span class="fu">residuals</span>(m))</span></code></pre></div>
<p><img src="dataviz_book_files/figure-html/lecture-02-409-1.png" width="480" /></p>
<p><strong>So what?</strong>
If Gaussianity of the residuals is not satisifed, this means that the noise model is wrong. This can have the following practical implications:</p>
<ul>
<li><p>With enough data, the regression lines might not be too severely affected the least squares estimates converge to the expected values. Applying least squares to fit and predict does not depend on the Gaussian assumption!</p></li>
<li><p>Hypothesis testing can be flawed.</p></li>
</ul>
<p><strong>What to do</strong>
We may work with fundamentally non-Gaussian data (like Poisson distributed data, which are count data), or data with long tails and outliers. If one has an idea of what could be a better noise model, consider a generalized linear model with another distribution. Alternatively, use case resampling to estimate confidence intervals.</p>
</div>
</div>
<div id="conclusions-1" class="section level2" number="10.5">
<h2><span class="header-section-number">10.5</span> Conclusions</h2>
<p>Linear models:</p>
<ul>
<li>are a powerful and versatile tool</li>
<li>can be used to
<ul>
<li>predict future data</li>
<li>quantify explained variance</li>
<li>assess linear relations between variables (hypothesis testing)</li>
<li>control for confounding (multiple linear regression)</li>
</ul></li>
<li>assumptions need to be checked (diagnostic plots)</li>
<li>can be generalized for different distributions (GLMs for classification: next lecture)</li>
</ul>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="27">
<li id="fn27"><p>this is a guessed formula for the sake of the explanation. You will build your own model on true data in the exercise and see why the 0.5 coefficients is probably not correct.<a href="chap-lin-reg.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>The popular deep neural networks are able, with enough data, to automatically learn these transformations. Nevertheless, their final operation (so-called last layer), is typically a linear combination as in Equation <a href="chap-lin-reg.html#eq:E-linreg">(10.3)</a><a href="chap-lin-reg.html#fnref28" class="footnote-back">↩︎</a></p></li>
<li id="fn29"><p><a href="https://en.wikipedia.org/wiki/Francis_Galton" class="uri">https://en.wikipedia.org/wiki/Francis_Galton</a><a href="chap-lin-reg.html#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>Galton made important contributions to statistics and genetics, but he was also one of the first proponents of eugenics, a scientifically flawed philosophical movement favored by many biologists of Galton’s time but with horrific historical consequences. You can read more about it here: [<a href="https://pged.org/history-eugenics-and-genetics" class="uri">https://pged.org/history-eugenics-and-genetics</a>].<a href="chap-lin-reg.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p>
<span class="math inline">\(N(x | 0, \sigma^2) = \frac{1}{\sigma\sqrt{2 \pi}} \exp(-\frac{x^2}{2 \sigma^2}).\)</span><a href="chap-lin-reg.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p>See the best seller Thinking fast and slow by Nobel prize-winning psychologist Daniel Kahneman (thank you, Lisa!) or <a href="https://en.wikipedia.org/wiki/Regression_toward_the_mean" class="uri">https://en.wikipedia.org/wiki/Regression_toward_the_mean</a>.<a href="chap-lin-reg.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>A lengthier case study on this dataset is given in R. Irizzarys’ book.<a href="chap-lin-reg.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p><a href="https://en.wikipedia.org/wiki/User:Cburnett" class="uri">https://en.wikipedia.org/wiki/User:Cburnett</a><a href="chap-lin-reg.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p><a href="https://creativecommons.org/licenses/by-sa/3.0/deed.en" class="uri">https://creativecommons.org/licenses/by-sa/3.0/deed.en</a><a href="chap-lin-reg.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p>One can extract data of so-called singles, i.e the number of runs reaching the first base, and find that the effects on runs are similar than for BBs, which is consistent with the intuition that BB directly contributes to runs because it brings the batter to the first base. See R. Irizzary’s book.<a href="chap-lin-reg.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p><a href="https://en.wikipedia.org/wiki/Variance-stabilizing_transformation" class="uri">https://en.wikipedia.org/wiki/Variance-stabilizing_transformation</a><a href="chap-lin-reg.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn38"><p><a href="https://en.wikipedia.org/wiki/Generalized_linear_model" class="uri">https://en.wikipedia.org/wiki/Generalized_linear_model</a><a href="chap-lin-reg.html#fnref38" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="big-data-stat.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chap-log-reg.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": {},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
