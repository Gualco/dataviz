---
title       : Data Analysis and Visualization
subtitle    : Classification
author      : Julien Gagneur, Jan Krumsiek
# job         : Professor Computational Biology
biglogo     : title.jpg
framework   : io2012        # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
hitheme     : tomorrow      # 
widgets     : [mathjax, bootstrap, quiz]            # {mathjax, quiz, bootstrap}
ext_widgets : {rCharts: ["libraries/highcharts", libraries/nvd3"]}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
---
<!-- Center image on slide --> 
<script 
src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script> <script 
type='text/javascript'> $(function() { $("p:has(img)").addClass('centered'); });
</script>

<!-- setwd('./lectures/') -->

```{r global_options, include=FALSE, cache=F}
source("../config.R")
opts_chunk$set(
    echo=TRUE, warning=FALSE, message=FALSE, cache=F, 
    results="show",
    out.width="500px", out.height="400px", fig.height = 3, fig.width = 4, 
    dpi=200
)
# options(width=100)
imgprefix <- 'assets/img/lec13_plotting/'
figprefix <- 'assets/fig/lec13_'

mysize <- 15
mytheme <- theme(
    axis.title = element_text(size=mysize), 
    axis.text = element_text(size=mysize),
    legend.title = element_text(size=mysize),
    legend.text = element_text(size=mysize)
    )

options(knitr.package.unnamed.chunk.label="lecture-13")

library(data.table)
```

<!-- START LECTURE -->

```{r, cache=F, echo=F, message=FALSE, warning=FALSE} 
source("../config.R") 
```

--- 
## Motivation

* One application of linear regression is the prediction of the value of the response $y$ for new values of the predictors $x_1,...,x_p$

* In regression, $y$ is a continuous real value

* In many prediction applications $y$ is a category. Examples:

  * diagnostic (have a disease or not)
  * spam email, not spam email
  * Handwritten digit recognition (0,1,...,9)
  * Speech recognition (words)

* Prediction tasks when the response is a category are called classification tasks

--- 
## Overview

* Notations and definitions

* Logistic regression

* Other classifiers: k-NN, SVM, decision trees, random forest and neural networks

* Assessing classifiers

* Training classifiers robustly

---
## Notation

* We use the machine learning nomenclature:

  * $y$ denotes the _outcome_ (or response) we want to predict 
  * $x_1, \dots, x_p$ denote the _features_ that we will use to predict the outcome.

* Goal: Build an algorithm that takes feature values as input and returns a prediction for the outcome when we don't know the outcome.

* The machine learning approach is to _train_ an algorithm using a dataset for which we do know the outcome, and then apply this algorithm in the future to make a prediction when we don't know the outcome.

---
## Prediction

We have a series of features and an unknown outcome we want to predict:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(dslabs)
n <- 1
tmp <- data.frame(outcome=rep("?",n), 
                  feature_1 = paste0("$x_1$"),
                  feature_2 = paste0("$x_2$"),
                  feature_3 = paste0("$x_3$"),
                  feature_4 = paste0("$x_4$"),
                  feature_5 = paste0("$x_5$"))
tmp %>% knitr::kable(align="c")
```

To _build a model_ that provides a prediction for any set of observed values $x_1, x_2, \dots x_5$, we collect data for which we know the outcome:

```{r, echo=FALSE}
n <- 10
tmp <- data.frame(outcome = paste0("$y_{", 1:n,"}$"), 
                  feature_1 = paste0("$x_{",1:n,",1}$"),
                  feature_2 = paste0("$x_{",1:n,",2}$"),
                  feature_3 = paste0("$x_{",1:n,",3}$"),
                  feature_4 = paste0("$x_{",1:n,",4}$"),
                  feature_5 = paste0("$x_{",1:n,",5}$"))
tmp %>% knitr::kable()
```

---
## Classification

* A _classification_ is a prediction task with a categorical outcome.

* A _binary classification_ is a prediction task with a binary outcome.

* Here we will focus on binary classification.

* We denote $k=0,1$ the two classes.


---
## A univariate example: predicting sex given the height.
The `heights` dataset of the `dslabs` package :
```{r, echo=TRUE, out.width = "500px"}
heights <- as.data.table(heights)
heights[, y:=as.numeric(sex == "Female")]
heights
```

---
## Linear regression is not appropriate for classification
```{r, echo=TRUE, out.width = "500px"}
lm_fit0 <- lm(y~height, data=heights)
ggplot(heights, aes(height, y)) + geom_point() + geom_abline(intercept = lm_fit0$coef[1], slope = lm_fit0$coef[2])
```

* We need to step back and consider a different modeling approach for categorical responses.

---
## Linear regression predicts the expected values
* We denote the density of the normal distribution with mean $\mu$ and variance $\sigma^2$ as:
$$N (x|\mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp(-\frac{(x-\mu)^2}{2 \sigma^2})$$

> * We have introduced linear regression with the following model:
$$
\begin{array}
\\
y_i &= \beta_0 + \sum_{j=1}^p \beta_j x_{ij}  + \epsilon_i \\
p(\epsilon_i) &= N(\epsilon_i| 0, \sigma^2)
\end{array}
$$ 
> * This is equivalent to write:
$$
\begin{array}
\\
p(y_i | x_i) &= N(y_i | \mu_i, \sigma^2) \\
\mu_i &= \beta_0 + \sum_{j=1}^p \beta_j x_{ij} 
\end{array}
$$ 

> * Hence, linear regression models $\mu_i := E(y_i|x_{i1},...x_{ip})$, the _conditional_ _expectation_ of the outcome conditioned on the features, as a linear combination of the features. 

---
## Logistic regression predicts the expected values
* Logistic regression models the conditional expectation of the outcome conditioned on the features.

> * For a binary outcome $y \in \{0,1\}$, the expectation $\mu$ is the probability of class 1.

> * Modeling a probability with a linear function is not ideal because you can make predictions <0 or >1. Here is a linear fit to the proportion of women vs. height in the `heights` dataset of the `dslabs` package :
```{r, echo=FALSE, out.width = "500px"}
props <- heights %>% 
  mutate(height = round(height)) %>%
  group_by(height) %>%
  filter(n() >= 10) %>%
  summarize(prop = mean(sex == "Female"))

lm_fit <- lm(prop ~ height, data=props)

ggplot(props, aes(height, prop)) +
  geom_point() +
  xlab("Height (rounded closest inch)") +
  ylab("Proportion of women") +
  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2])
```

---
## The logistic function maps real numbers to the [0,1] interval

The logistic function $\sigma (t)$ is defined as follows:

$$\sigma (t) = \frac{1}{1+e^{-t}}$$


```{r, echo=FALSE, out.width = "500px"}
library(data.table)
df <- data.table(x=seq(-5,5, length.out=1000))
df[,y:=1/(1+exp(-x))]
ggplot(df, aes(x, y)) +
  geom_line() +
  xlab("x") +
  ylab( expression(sigma(x)) ) 
```

---
## The logistic regression
* We can use the logistic function to map linear combinations of the features to the [0,1] interval. A logistic regression models:
$$
\begin{array}
\\
p(y_i | x_i) &= \mu_i \\
\mu_i &= \sigma(\beta_0 + \sum_{j=1}^p \beta_j x_{ij}) 
\end{array}
$$ 

> * An alternative approach is to use the inverse function of the sigmoid, the _logit_ function:
$$
\operatorname{logit}(x) = \log(\frac{x}{1-x})
$$

> * We then write:
$$
\begin{array}
\\
& p(y_i | x_i) = \mu_i \\
& \operatorname{logit}(\mu_i) = \eta_i = \beta_0 + \sum_{j=1}^p \beta_j x_{ij} 
\end{array}
$$ 

---
## The logistic regression
* Here are the predicted values using logistic regression (red) compared to a linear regression of the proportions (black). It fits better the data and fixes the negative probabilities issue.


```{r, echo =FALSE}
logistic_fit <- glm(I(sex=="Female") ~ height, data=heights, family = "binomial")
props <- as.data.table(props)
props[, logistic := predict(logistic_fit, props, type="response")]
ggplot(props, aes(height, prop)) +
  geom_point() +
  xlab("Height (rounded closest inch)") +
  ylab("Proportion of women") +
  geom_abline(intercept = lm_fit$coef[1], slope = lm_fit$coef[2]) +
  geom_line(aes(height, logistic), col='red')
```

---
## Classification with logistic regression

* The logistic regression predicts a probability. Therefore, it conveys some uncertainty in the prediction.

* Hard classification is usually performed by the following, simple rule:

If $\mu>0.5$ (or equivalently $\eta>0$), predict class 1, else predict class 0.

---
## Logistic regression as a generalized linear model
* Logistic regression is one instance of generalized linear models, which all exploit the same idea:
  * 1. A probability distribution from the exponential family.
  * 2. A linear predictor $\eta = \mathbf{X}\boldsymbol{\beta}$ .
  * 3. A link function $g$ such that $\text{E}(y) = \mu = g^{-1}(\eta)$

* Popular examples are Linear regression, logistic regression, Poisson regression, and Gamma regression

* The inverse of the _link_ function is called the _activation_ function. For logistic regression, the activation function is the sigmoid and the link function is the logit.

---
## Logistic regression with R
Here is the `heights` dataset:

```{r}
heights <- as.data.table(heights)
heights[,y:=as.numeric(sex == "Female")]
heights
```

---
## Logistic regression with R
* In R, you can fit a logistic regression using the ```glm``` function (for generalized linear model).

```{r}
logistic_fit <- glm(y ~ height, data=heights, family = "binomial")
logistic_fit
```

---
## Logistic regression with R

The fitted model can be applied to data (seen or unseen) using ```predict()```. By default it returns the linear predictor $\eta$. Use ```type='response'``` to have the predicted probabilities:

```{r}
heights[, mu_hat := predict(logistic_fit, heights, type="response")]
heights
```

---
## Odds and odds ratios

* The _odds_ for a binary variable $y$ are defined as $\frac{p(y)}{1-p(y)}$. For instance the odds for someone to be a woman is 238 / 812 = 0.3 in the `heights` dataset.

```{r, echo=TRUE}
table(heights$sex)
```

* The _odds_ _ratio_ is the ratio of the odds of $y$ with some condition and the odds of $y$ without the condition. For instance, the odds ratio for someone to be a woman given the height is shorter than 70 inches (1.78 m) is (222/420) / (16/392) = 12.95. 

```{r, echo=TRUE}
table(heights$sex, shorter_70 = heights$height<70)
```

---
## Coefficients of the logistic regression
* The $\beta$ values from logistic regression are log odds ratios, given that all other variables are fixed. Odds ratios can thus be obtained by applying exp().

```{r}
coef(logistic_fit)
OR_height <- exp(coef(logistic_fit)[2])
OR_height
```
* In this case, an increase by one inch is associated with a 0.72 fold-change of the odds.


---
## Effects on probabilities
$$
p(y) = \frac{odds}{1+odds}
$$

* (A) Starting with 1:1 odds, p=50%, ~63.5 inch., 1-inch increase leads to odds of 0.72:1 odds, i.e. p=42%.

* (B) Starting with 1:9 odds, p=10%, ~70 inch., 1-inch increase leads to odds 0.72*1:9, i.e. p=7.4%.

```{r, out.width = "475px", echo=FALSE}
include_graphics("./assets/img/lec13_odds-ratio.png")
```

