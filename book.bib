@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.org/knitr/},
}
@article{Wickham2014,
   author = {Hadley  Wickham},
   title = {Tidy Data},
   journal = {Journal of Statistical Software, Articles},
   volume = {59},
   number = {10},
   year = {2014},
   keywords = {},
   abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.},
   issn = {1548-7660},
   pages = {1--23},
   doi = {10.18637/jss.v059.i10},
   url = {https://www.jstatsoft.org/v059/i10}
}


@book{bishop2007,
  added-at = {2009-06-02T09:46:22.000+0200},
  asin = {0387310738},
  author = {Bishop, Christopher M.},
  biburl = {https://www.bibsonomy.org/bibtex/2d21de30a3a67c0f9f3c96bd6eec3267a/midtiby},
  description = {Amazon.com: Pattern Recognition and Machine Learning (Information Science and Statistics): Christopher M. Bishop: Books},
  dewey = {006.4},
  ean = {9780387310732},
  edition = 1,
  interhash = {f6fec2ccd82dec0dcd63825e301662cf},
  intrahash = {d21de30a3a67c0f9f3c96bd6eec3267a},
  isbn = {0387310738},
  keywords = {algorithms machinelearning patternrecognition statistics},
  publisher = {Springer},
  timestamp = {2009-06-02T15:22:29.000+0200},
  title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  url = {https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/},
  year = 2007
}

@book{Gareth2014,
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
title = {An Introduction to Statistical Learning: With Applications in R},
year = {2014},
isbn = {1461471370},
publisher = {Springer Publishing Company, Incorporated},
abstract = {An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.}
}

@article{gagneur2013,
    author = {Gagneur, Julien and Stegle, Oliver and Zhu, Chenchen and Jakob, Petra and Tekkedil, Manu M. and Aiyar, Raeka S. and Schuon, Ann-Kathrin and Pe'er, Dana and Steinmetz, Lars M.},
    journal = {PLOS Genetics},
    publisher = {Public Library of Science},
    title = {Genotype-Environment Interactions Reveal Causal Pathways That Mediate Genetic Effects on Phenotype},
    year = {2013},
    month = {09},
    volume = {9},
    url = {https://doi.org/10.1371/journal.pgen.1003803},
    pages = {1-10},
    abstract = {Author Summary A long-standing challenge in biology is to unravel the chain of molecular events linking genetic variation to phenotypes like disease. Identifying the genes that act as intermediates between the underlying genetic variation and the disease can offer new ways to intervene in its progression. While large-scale molecular profiles are an important starting point, it is difficult to distinguish causal relationships from correlative associations. In this study, our goal was to develop strategies to identify these causal intermediates. We studied the effects of genetic differences in baker's yeast on fitness in multiple environmental conditions. While genetic effects on fitness depended strongly on the environment, genetic effects on the expression of truly causal intermediate genes tended to persist despite environmental changes. This indicates that causal intermediates can be found among genes whose expression is affected by genetic variation independently of environment. We thus developed a statistical method to predict causal intermediates based on genetics, gene expression, and fitness in multiple environments. Our study has implications for the design and analysis of clinical molecular profiling efforts towards understanding how genetic variation causes disease, suggesting that multiple contexts (e.g., cell types) can be informative even if they are not afflicted by the disease.},
    number = {9},
    doi = {10.1371/journal.pgen.1003803}
}

@book{davison_hinkley_1997,
  place={Cambridge}, 
  series={Cambridge Series in Statistical and Probabilistic Mathematics},
  title={Bootstrap Methods and their Application},
  DOI={10.1017/CBO9780511802843}, 
  publisher={Cambridge University Press},
  author={Davison, A. C. and Hinkley, D. V.},
  year={1997},
  collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@article {Phipson2010,
      author = "Belinda Phipson and Gordon K Smyth",
      title = "Permutation P-values Should Never Be Zero: Calculating Exact P-values When Permutations Are Randomly Drawn",
      journal = "Statistical Applications in Genetics and Molecular Biology",
      year = {2010},
      publisher = "De Gruyter",
      address = "Berlin, Boston",
      volume = "9",
      number = "1",
      doi = "https://doi.org/10.2202/1544-6115.1585",
      url = "https://www.degruyter.com/view/journals/sagmb/9/1/article-sagmb.2010.9.1.1585.xml.xml"
}

@article{Wasserstein2016,
author = {Ronald L. Wasserstein and Nicole A. Lazar},
title = {The ASA Statement on p-Values: Context, Process, and Purpose},
journal = {The American Statistician},
volume = {70},
number = {2},
pages = {129-133},
year  = {2016},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.2016.1154108},
URL = { 
        https://doi.org/10.1080/00031305.2016.1154108
},
eprint = { 
        https://doi.org/10.1080/00031305.2016.1154108
}
}

@book{holmes2019modern,
  title={Modern Statistics for Modern Biology},
  author={Holmes, S. and Huber, W.},
  isbn={9781108705295},
  url={https://www.huber.embl.de/msmb},
  year={2019},
  publisher={Cambridge University Press}
}

@article{Roberts2017,
author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and Guillera-Arroita, Gurutzeta and Hauenstein, Severin and Lahoz-Monfort, José J. and Schröder, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
title = {Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure},
journal = {Ecography},
volume = {40},
number = {8},
pages = {913-929},
doi = {https://doi.org/10.1111/ecog.02881},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ecog.02881},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/ecog.02881},
abstract = {Ecological data often show temporal, spatial, hierarchical (random effects), or phylogenetic structure. Modern statistical approaches are increasingly accounting for such dependencies. However, when performing cross-validation, these structures are regularly ignored, resulting in serious underestimation of predictive error. One cause for the poor performance of uncorrected (random) cross-validation, noted often by modellers, are dependence structures in the data that persist as dependence structures in model residuals, violating the assumption of independence. Even more concerning, because often overlooked, is that structured data also provides ample opportunity for overfitting with non-causal predictors. This problem can persist even if remedies such as autoregressive models, generalized least squares, or mixed models are used. Block cross-validation, where data are split strategically rather than randomly, can address these issues. However, the blocking strategy must be carefully considered. Blocking in space, time, random effects or phylogenetic distance, while accounting for dependencies in the data, may also unwittingly induce extrapolations by restricting the ranges or combinations of predictor variables available for model training, thus overestimating interpolation errors. On the other hand, deliberate blocking in predictor space may also improve error estimates when extrapolation is the modelling goal. Here, we review the ecological literature on non-random and blocked cross-validation approaches. We also provide a series of simulations and case studies, in which we show that, for all instances tested, block cross-validation is nearly universally more appropriate than random cross-validation if the goal is predicting to new data or predictor space, or for selecting causal predictors. We recommend that block cross-validation be used wherever dependence structures exist in a dataset, even if no correlation structure is visible in the fitted model residuals, or if the fitted models account for such correlations.},
year = {2017}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}


